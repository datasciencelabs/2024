[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "BST 260 Introduction to Data Science\nKresge 202A and 202B (HSPH)\nMonday 09:45 AM - 11:15 AM; Wednesday 09:45 AM - 11:15 AM\nLecture notes: https://datasciencelabs.github.io/2024/\nSlack workspace: https://bst260fall2024.slack.com/\nCanvas: https://canvas.harvard.edu/courses/143922",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#general-information",
    "href": "syllabus.html#general-information",
    "title": "Syllabus",
    "section": "",
    "text": "BST 260 Introduction to Data Science\nKresge 202A and 202B (HSPH)\nMonday 09:45 AM - 11:15 AM; Wednesday 09:45 AM - 11:15 AM\nLecture notes: https://datasciencelabs.github.io/2024/\nSlack workspace: https://bst260fall2024.slack.com/\nCanvas: https://canvas.harvard.edu/courses/143922",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe assume students have taken or are taking a probability and statistics course and have basic programming skills.\nStudents not matriculated in an HSPH Biostatistics graduate program (HDS SM60, BIO SM80 / SM60 / SM1, and CBQG SM80) will be required to score at least 90% on a basic math and programming diagnostic test to enroll in the course. If you are in a HSPH Biostatistics graduate program and you score less than 90% we will contact you to offer supplementary resource to help you be prepared for the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nIntroduction to Data Science: Data Wrangling and Visualization with R\nIntroduction to Data Science: Statistics and Prediction Algorithms Through Case Studies",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the following:\n\nUNIX/Linux shell\nReproducible document preparation with RStudio, knitr, and markdown\nVersion control with git and GitHub\nR programming\nData wrangling with dplyr and data.table\nData visualization with ggplot2\n\nWe also demonstrate how the following concepts are applied in data analysis:\n\nProbability theory\nStatistical inference and modeling\nHigh-dimensional data techniques\nMachine learning\n\nWe do not cover the theory and details of these methods as they are covered in other courses.\nThroughout the course, we use motivating case studies and data analysis problem sets based on challenges similar to those you encounter in scientific research.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#weekly-course-structure",
    "href": "syllabus.html#weekly-course-structure",
    "title": "Syllabus",
    "section": "Weekly Course Structure",
    "text": "Weekly Course Structure\n\nMonday lectures: We describe the concerts, methods, and skills needed for problem sets.\nWednesday labs: We work together on problem sets.\nFriday: Problem sets due (see Key Dates and Problem Sets).\n\nPlease ensure that you read the chapters listed in the syllabus before each Monday. The lectures are designed with the assumption that you have completed the readings, enabling us to dive deeper into the nuances of data analysis and coding.\nLectures will not be recorded.\nWe will have a Slack workspace for you to ask questions during and after class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grade-distribution",
    "href": "syllabus.html#grade-distribution",
    "title": "Syllabus",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\n\n\nComponent\nWeight\n\n\n\n\n10 problem sets\n50%\n\n\nMidterm 1\n10%\n\n\nMidterm 2\n20%\n\n\nFinal project\n20%",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#problem-sets",
    "href": "syllabus.html#problem-sets",
    "title": "Syllabus",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets will be due every week or every other week, depending on difficulty. They will be due at 11:59 PM on the day denoted on the Problem Sets page.\nSome problem sets include open ended questions that will be difficult to answer on your own. We will be working on these together during Wednesday labs. We also offer office hours where you can get help with unanswered questions.\nProblem sets must be submitted via GitHub. Students are required to have a GitHub account and create a repository for the course. We will be providing further instructions during the first lab.\n10% of the total points for the problem sets will be deducted for every late day. Students can have a total of 4 late days without penalty during the entire semester. No need to provide a written excuse. Providing an excuse does not give you more days unless an accommodation is requested and approved by the Office of Student Affairs (this includes COVID).\nProblem set submissions need to be completely reproducible Quarto documents. If your Quarto file does not compile it will be considered a late day, and you will be notified and will need to resubmit a Quarto file that does compile. You will be deducted further late days for every day it takes for you to turn in a Quarto file that does knit. You are required to check emails that come through the Canvas system, as this the only way we will communicate problems with your problem sets.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#midterm-policy",
    "href": "syllabus.html#midterm-policy",
    "title": "Syllabus",
    "section": "Midterm Policy",
    "text": "Midterm Policy\nBoth midterms are closed book, no internet, and in-class. You are expected to complete them in 1 hour.\nQuestions will be drawn mostly or entirely from the problem sets.\nPlease make sure you can come to class on the midterm dates provided in the Key Dates table below. If you miss the exam, you will need approval from the Office of Student Affairs to receive a makeup. All make-up exams will be completely different from the in-class ones.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#final-project",
    "href": "syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\nFor your final project we ask that you turn in a 4-6 page report using data to answer a public health related question. You can chose from one of the following:\n\nBased on state-level data, how effective where vaccines against SARS-CoV-2 reported cases and COVID-19 hospitalizations and deaths, and vaccination rates.\nWhat was the excess mortality after Hurricane María in Puerto Rico? Where different age groups affected differently?\n\nOptionally, you can select a question that align with your ongoing research. This way, it can be directly beneficial to your work. This will require prior approval from the instructor by October 25.\nYet another option is to build a interactive webpage with poll-driven predictions for the 2024 US elections. Note this will be more challenging as we will not cover tools for interactive webpages until the last week of class (time permitting).\nNote: You should start working on your project after the first midterm. Do not wait until the last week. Teaching staff will be available during office hours.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#chatgpt-policy",
    "href": "syllabus.html#chatgpt-policy",
    "title": "Syllabus",
    "section": "ChatGPT Policy",
    "text": "ChatGPT Policy\nYou can use ChatGPT however you want. Do remember you won’t be able to use it during the midterms.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#key-dates",
    "href": "syllabus.html#key-dates",
    "title": "Syllabus",
    "section": "Key Dates",
    "text": "Key Dates\n\n\n\n\n\n\n\nDate\nEvent\n\n\n\n\nSep 10\nPset 1 due\n\n\nSep 13\nPset 2 due\n\n\nOct 14\nNo class: Indigenous Peoples’ Day\n\n\nOct 16\nMidterm 1: covers material from Sep 04-Oct 11\n\n\nOct 23\nStart final project. Obtain approval if you want to do a personal project instead.\n\n\nNov 11\nNo class: Veterans’ Day\n\n\nNov 25\nMidterm 2: cover material from Sep 04-Nov 22\n\n\nNov 27\nNo class: Thanksgiving Recess Begins\n\n\nDec 20\nFinal Project due",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/productivity/01-unix.html#naming-convention",
    "href": "slides/productivity/01-unix.html#naming-convention",
    "title": "Unix",
    "section": "Naming convention",
    "text": "Naming convention\n\nIn general you want to name your files in a way that is related to their contents and specifies how they relate to other files.\nThe Smithsonian Data Management Best Practices has “five precepts of file naming and organization”"
  },
  {
    "objectID": "slides/productivity/01-unix.html#five-precepts-of-file-naming-and-organization",
    "href": "slides/productivity/01-unix.html#five-precepts-of-file-naming-and-organization",
    "title": "Unix",
    "section": "Five precepts of file naming and organization",
    "text": "Five precepts of file naming and organization\n\nHave a distinctive, human-readable name that gives an indication of the content.\nFollow a consistent pattern that is machine-friendly.\nOrganize files into directories (when necessary) that follow a consistent pattern.\nAvoid repetition of semantic elements among file and directory names.\nHave a file extension that matches the file format (no changing extensions!)\n\nFor specific recommendations we highly recommend you follow The Tidyverse Style Guide"
  },
  {
    "objectID": "slides/productivity/01-unix.html#the-terminal",
    "href": "slides/productivity/01-unix.html#the-terminal",
    "title": "Unix",
    "section": "The terminal",
    "text": "The terminal\n\nInstead of clicking, dragging, and dropping to organize our files and folders, we will be typing Unix commands into the terminal.\nThe way we do this is similar to how we type commands into the R console, but instead of generating plots and statistical summaries, we will be organizing files on our system."
  },
  {
    "objectID": "slides/productivity/01-unix.html#the-terminal-1",
    "href": "slides/productivity/01-unix.html#the-terminal-1",
    "title": "Unix",
    "section": "The terminal",
    "text": "The terminal\n\nThe terminal is integrated into Mac and Linux systems, but Windows users will have to install an emulator. Once you have a terminal open, you can start typing commands.\nYou should see a blinking cursor at the spot where what you type will show up. This position is called the command line."
  },
  {
    "objectID": "slides/productivity/01-unix.html#the-filesystem",
    "href": "slides/productivity/01-unix.html#the-filesystem",
    "title": "Unix",
    "section": "The filesystem",
    "text": "The filesystem"
  },
  {
    "objectID": "slides/productivity/01-unix.html#the-home-directory",
    "href": "slides/productivity/01-unix.html#the-home-directory",
    "title": "Unix",
    "section": "The home directory",
    "text": "The home directory"
  },
  {
    "objectID": "slides/productivity/01-unix.html#windows",
    "href": "slides/productivity/01-unix.html#windows",
    "title": "Unix",
    "section": "Windows",
    "text": "Windows\nThe structure on Windows looks something like this:"
  },
  {
    "objectID": "slides/productivity/01-unix.html#mac",
    "href": "slides/productivity/01-unix.html#mac",
    "title": "Unix",
    "section": "Mac",
    "text": "Mac\nAnd on MacOS something like this:"
  },
  {
    "objectID": "slides/productivity/01-unix.html#working-directory",
    "href": "slides/productivity/01-unix.html#working-directory",
    "title": "Unix",
    "section": "Working directory",
    "text": "Working directory\n\nThe working directory is the directly you are currently in. Later we will see that we can move to other directories using the command line.\nIt’s similar to clicking on folders.\nYou can see your working directory using the Unix command pwd\n\nIn R we can use getwd()"
  },
  {
    "objectID": "slides/productivity/01-unix.html#paths",
    "href": "slides/productivity/01-unix.html#paths",
    "title": "Unix",
    "section": "Paths",
    "text": "Paths\n\nThis string returned in previous command is full path to working directory.\nThe full path to your home directory is stored in an environment variable.\nYou can see it like this echo $HOME"
  },
  {
    "objectID": "slides/productivity/01-unix.html#paths-1",
    "href": "slides/productivity/01-unix.html#paths-1",
    "title": "Unix",
    "section": "Paths",
    "text": "Paths\n\nIn Unix, we use the shorthand ~ as a nickname for your home directory\nExample: the full path for docs (in image above) can be written like this ~/docs.\nMost terminals will show the path to your working directory right on the command line.\nLet’s open a terminal window and see if the working directory is listed."
  },
  {
    "objectID": "slides/productivity/01-unix.html#unix-commands",
    "href": "slides/productivity/01-unix.html#unix-commands",
    "title": "Unix",
    "section": "Unix commands",
    "text": "Unix commands\n\nls: Listing directory content\nmkdir and rmdir: make and remove a directory\ncd: navigating the filesystem by changing directories\npwd: see your workding directory\nmv: moving files\ncp: copying files\nrm: removing files\nless: looking at a file"
  },
  {
    "objectID": "slides/productivity/01-unix.html#autocomplete",
    "href": "slides/productivity/01-unix.html#autocomplete",
    "title": "Unix",
    "section": "Autocomplete",
    "text": "Autocomplete\n\nIn Unix you can auto-complete by hitting tab.\nThis means that we can type cd d then hit tab.\nUnix will either auto-complete if docs is the only directory/file starting with d or show you the options.\nTry it out! Using Unix without auto-complete will make it unbearable."
  },
  {
    "objectID": "slides/productivity/01-unix.html#text-editors",
    "href": "slides/productivity/01-unix.html#text-editors",
    "title": "Unix",
    "section": "Text editors",
    "text": "Text editors\nCommand-line text editors are essential tools, especially for system administrators, developers, and other users who frequently work in a terminal environment. Here are some of the most popular command-line text editors:\n\nNano\nPico\nVi or Vim\nEmacs"
  },
  {
    "objectID": "slides/productivity/01-unix.html#other-very-useful-commands-you-should-learn",
    "href": "slides/productivity/01-unix.html#other-very-useful-commands-you-should-learn",
    "title": "Unix",
    "section": "Other very useful commands you should learn",
    "text": "Other very useful commands you should learn\n\ncurl - download data from the internet.\ntar - archive files and subdirectories of a directory into one file.\nssh - connect to another computer.\nfind - search for files by filename in your system.\ngrep - search for patterns in a file.\nawk/sed - These are two very powerful commands that permit you to find specific strings in files and change them."
  },
  {
    "objectID": "slides/productivity/01-unix.html#resources",
    "href": "slides/productivity/01-unix.html#resources",
    "title": "Unix",
    "section": "Resources",
    "text": "Resources\nTo get started.\n\nhttps://www.codecademy.com/learn/learn-the-command-line\nhttps://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1\nhttps://www.coursera.org/learn/unix"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#the-header",
    "href": "slides/productivity/03-quarto.html#the-header",
    "title": "Quarto",
    "section": "The header",
    "text": "The header\n\nStart a new empty document.\nAt the top you see:\n\n\n---\ntitle: \"Untitled\"\n---\n\n\nThe things between the --- is the YAML header.\nYou will see it used throughout the Quarto guide."
  },
  {
    "objectID": "slides/productivity/03-quarto.html#text-formating",
    "href": "slides/productivity/03-quarto.html#text-formating",
    "title": "Quarto",
    "section": "Text formating",
    "text": "Text formating\n*italics* or _italics_ = italics\n**bold** = bold\n***bold italics*** = bold italics\n~~strikethrough~~ = strikethrough\n`code` = code"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#text-formating-1",
    "href": "slides/productivity/03-quarto.html#text-formating-1",
    "title": "Quarto",
    "section": "Text formating",
    "text": "Text formating\nThis:\n```\nline 1\nline 2\n```\nshows code chunks:\nline 1\nline 2"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#links",
    "href": "slides/productivity/03-quarto.html#links",
    "title": "Quarto",
    "section": "Links",
    "text": "Links\n\nShow the link and add link: &lt;https://quarto.org/docs/guide/&gt;\nAdd link to text: [Quarto Guide](https://quarto.org/docs/guide/)\n\nLooks like this:\nhttps://quarto.org/docs/guide/\nQuarto Guide"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#images",
    "href": "slides/productivity/03-quarto.html#images",
    "title": "Quarto",
    "section": "Images",
    "text": "Images\n![My caption](https://datasciencedojo.com/wp-content/uploads/11-1.jpg)\nShows the plot and caption:\n\nMy captionThe image can also be a local file."
  },
  {
    "objectID": "slides/productivity/03-quarto.html#lists",
    "href": "slides/productivity/03-quarto.html#lists",
    "title": "Quarto",
    "section": "Lists",
    "text": "Lists\nBullets:\n-   bullet 1\n    -   sub-bullet 1\n    -   sub-bullet 2\n-   bullet 2\nLooks like this:\n\nbullet 1\n\nsub-bullet 1\nsub-bullet 2\n\nbullet 2"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#lists-1",
    "href": "slides/productivity/03-quarto.html#lists-1",
    "title": "Quarto",
    "section": "Lists",
    "text": "Lists\nOrdered list:\n1.  Item 1\n2.  Item 2\nLooks like this:\n\nItem 1\nItem 2"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#equations",
    "href": "slides/productivity/03-quarto.html#equations",
    "title": "Quarto",
    "section": "Equations",
    "text": "Equations\n\n\n\n\n\n\n\nNote\n\n\nIf you are going to write technical report, you definitely want to learn LaTeX.\nOnce you learn LaTeX you will never want to use an equation editor again.\nThere are many online tutorials, like this one.\nChatGPT is great at LaTeX"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#equations-1",
    "href": "slides/productivity/03-quarto.html#equations-1",
    "title": "Quarto",
    "section": "Equations",
    "text": "Equations\nExamples:\n\nInline: $Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ looks like this \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\)\nDisplay math:\n\n$$\n\\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\varepsilon}\n$$\nlooks like this:\n\\[\n\\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\varepsilon}\n\\]"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations",
    "href": "slides/productivity/03-quarto.html#computations",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\n\nThe main reason we use Quarto is because we can include code and execute the code when compiling the document.\nIn R we refer to them as R chunks.\nThis applies to plots as well; the plot will be placed in that position.\n\n\n\n\n\n\n\nNote\n\n\nTo add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows."
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations-1",
    "href": "slides/productivity/03-quarto.html#computations-1",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\nWe can write something like this:\n```{r}\nx &lt;- 1\ny &lt;- 2\nx + y\n```"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations-2",
    "href": "slides/productivity/03-quarto.html#computations-2",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\nIt look slike this:\n\nx &lt;- 1\ny &lt;- 2\nx + y\n\n[1] 3\n\n\nNote that it was evaluated and the result is shown."
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations-3",
    "href": "slides/productivity/03-quarto.html#computations-3",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\n\nBy default, the code and result will show up as well.\nYou can send arguments to control the behavior with |#\nFor example, to avoid showing code in the final document, you can use the argument echo: FALSE.\n\n```{r}\n#| echo: false\nx &lt;- 1\ny &lt;- 2\nx + y\n```"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations-4",
    "href": "slides/productivity/03-quarto.html#computations-4",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\n\nThere are many options (auto-complete shows them).\nFor example, to avoid the the code running you can use eval: FALSE.\nTo avoid showing warnings warning: FALSE, to avoid showing messages message: FALSE.\n\n\n\n\n\n\n\nNote\n\n\nIf you want to apply an option globally, these can be set globally in the header.\nexecute:\n  echo: false"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#computations-5",
    "href": "slides/productivity/03-quarto.html#computations-5",
    "title": "Quarto",
    "section": "Computations",
    "text": "Computations\n\nWe recommend getting into the habit of labeling code chunks:\n\n```{r}\n#| label: one-plus-two\nx &lt;- 1\ny &lt;- 2\nx + y\n```\n\nHelps with debugging\nGives meaningful names to generated images."
  },
  {
    "objectID": "slides/productivity/03-quarto.html#more-on-markdown",
    "href": "slides/productivity/03-quarto.html#more-on-markdown",
    "title": "Quarto",
    "section": "More on markdown",
    "text": "More on markdown\nThere is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including:\n\nRStudio’s tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\nThe knitR book: https://yihui.name/knitr/\nPandoc’s Markdown in-depth documentation\nGuide for academic reports"
  },
  {
    "objectID": "slides/productivity/03-quarto.html#quarto-render",
    "href": "slides/productivity/03-quarto.html#quarto-render",
    "title": "Quarto",
    "section": "quarto render",
    "text": "quarto render\n\nRStudio provides the Render button that makes it easier to compile the document.\nYou can also type quarto render filename.qmd on the command line. This offers many options.\nYou can produce html, pdf, or word documents.\nYou can specify the default in the YAML header using: format: html, format: pdf,format: docx, or format: gfm (gfm stands for GitHub flavored markdown, a convenient way to share your reports)."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference",
    "href": "slides/inference/22-intro-inference.html#statistical-inference",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical Inference is the branch of statistics dedicated to distinguishing patterns arising from signal versus those arising from chance.\nIt is a broad topic and we review the basics using polls as a motivating example.\nWe motivate the concepts with election forecasting as a case study."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-1",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-1",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nThe day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”.\nThey went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%.\nFiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-2",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-2",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nThe predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference.\nTheir performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities.\nFour years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-3",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-3",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nPolitical commentator Joe Scarborough said during his show\n\n\n\nAnybody that thinks that this race is anything but a toss-up right now is such an ideologue they’re jokes."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-4",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-4",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nTo which Nate Silver responded via Twitter:\n\n\n\nIf you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?"
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-5",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-5",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nIn 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning.\nIn contrast, many other forecasters were almost certain she would win.\nShe lost."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-6",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-6",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nBut 71% is still more than 50%, so was Mr. Silver wrong?\nWhat does probability mean in this context anyway?\nWe will demonstrate how the probability concepts covered in the previous part can be applied to develop statistical approaches that render polls effective tools."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-7",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-7",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nForecasting an election is a more complex process that involves combining results from 50 states and DC.\nWe will learn the statistical concepts necessary to define estimates and margins of errors for the popular vote, and show how these are used to construct confidence intervals.\nOnce we grasp these ideas, we will be able to understand statistical power and p-values, concepts that are ubiquitous in the academic literature."
  },
  {
    "objectID": "slides/inference/22-intro-inference.html#statistical-inference-8",
    "href": "slides/inference/22-intro-inference.html#statistical-inference-8",
    "title": "Introduction to Statistical Inference and Models",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nWe will then aggregate data from different pollsters to highlight the shortcomings of the models used by traditional pollsters and present a method for improving these models.\nTo understand probabilistic statements about the chances of a candidate winning, we will introduce Bayesian modeling.\nFinally, we put it all together using hierarchical models to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nConfidence intervals are a very useful concept widely employed by data analysts.\nA version of these that are commonly seen come from the ggplot geometry geom_smooth.\nBelow is an example using a temperature dataset available in R:"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-1",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-1",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-2",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-2",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nIn our competition, you were asked to give an interval.\nIf the interval you submitted includes the \\(p\\), you receive half the money you spent on your “poll” back and proceed to the next stage of the competition."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-3",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-3",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nOne way to pass to the second round is to report a very large interval.\nFor example, the interval \\([0,1]\\) is guaranteed to include \\(p\\).\nHowever, with an interval this big, we have no chance of winning the competition."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-4",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-4",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nSimilarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious.\nEven a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-5",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-5",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nOn the other hand, the smaller the interval we report, the smaller our chances are of winning the prize.\nLikewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster.\nWe might want to be somewhere in between.\nWe can use the statistical theory we have learned to compute the probability of any given interval including \\(p\\)."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-6",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-6",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nTo illustrate this we run the Monte Carlo simulation.\nWe use the same parameters as above:\n\n\np &lt;- 0.45 \nN &lt;- 1000"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-7",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-7",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nAnd notice that the interval here:\n\n\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p)) \nx_hat &lt;- mean(x) \nse_hat &lt;- sqrt(x_hat*(1 - x_hat)/N) \nc(x_hat - 1.96*se_hat, x_hat + 1.96*se_hat) \n\n[1] 0.4032809 0.4647191\n\n\n\nis different from this one:\n\n\nx &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p)) \nx_hat &lt;- mean(x) \nse_hat &lt;- sqrt(x_hat*(1 - x_hat)/N) \nc(x_hat - 1.96*se_hat, x_hat + 1.96*se_hat) \n\n[1] 0.4301041 0.4918959\n\n\n\nKeep sampling and creating intervals, and you will see the random variation."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-8",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-8",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nTo determine the probability that the interval includes \\(p\\), we need to compute the following:\n\n\\[\n\\mbox{Pr}\\left(\\bar{X} - 1.96\\hat{\\mbox{SE}}(\\bar{X}) \\leq p \\leq \\bar{X} + 1.96\\hat{\\mbox{SE}}(\\bar{X})\\right)\n\\]"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-9",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-9",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nBy subtracting and dividing the same quantities in all parts of the equation, we find that the above is equivalent to:\n\n\\[\n\\mbox{Pr}\\left(-1.96 \\leq \\frac{\\bar{X}- p}{\\hat{\\mbox{SE}}(\\bar{X})} \\leq  1.96\\right)\n\\]"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-10",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-10",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nThe term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with \\(Z\\), so we have:\n\n\\[\n\\mbox{Pr}\\left(-1.96 \\leq Z \\leq  1.96\\right)\n\\]\n\nwhich we can quickly compute using :\n\n\npnorm(1.96) - pnorm(-1.96) \n\n[1] 0.9500042\n\n\n\nproving that we have a 95% probability."
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-11",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-11",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nIf we want to have a larger probability, say 99%, we need to multiply by whatever z satisfies the following:\n\n\\[\n\\mbox{Pr}\\left(-z \\leq Z \\leq  z\\right) = 0.99\n\\]\n\nWe use:\n\n\nz &lt;- qnorm(0.995) \nz \n\n[1] 2.575829\n\npnorm(z) - pnorm(-z) \n\n[1] 0.99"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#confidence-intervals-12",
    "href": "slides/inference/24-confidence-intervals.html#confidence-intervals-12",
    "title": "Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nIn statistics textbooks, confidence interval formulas are given for arbitraty probabilities written as \\(1-\\alpha\\).\nWe can obtain the \\(z\\) for the equation above using z = qnorm(1 - alpha / 2) because \\(1 - \\alpha/2 - \\alpha/2 = 1 - \\alpha\\).\nSo, for example, for \\(\\alpha=0.05\\), \\(1 - \\alpha/2 = 0.975\\) and we get the \\(z=1.96\\) we used above:\n\n\nqnorm(0.975) \n\n[1] 1.959964"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#a-monte-carlo-simulation",
    "href": "slides/inference/24-confidence-intervals.html#a-monte-carlo-simulation",
    "title": "Confidence Intervals",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nWe can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time.\n\n\nN &lt;- 1000 \nB &lt;- 10000 \ninside &lt;- replicate(B, { \n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p)) \n  x_hat &lt;- mean(x) \n  se_hat &lt;- sqrt(x_hat*(1 - x_hat)/N) \n  between(p, x_hat - 1.96*se_hat, x_hat + 1.96*se_hat) \n}) \nmean(inside) \n\n[1] 0.9536"
  },
  {
    "objectID": "slides/inference/24-confidence-intervals.html#a-monte-carlo-simulation-1",
    "href": "slides/inference/24-confidence-intervals.html#a-monte-carlo-simulation-1",
    "title": "Confidence Intervals",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nThe following plot shows the first 100 confidence intervals."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#importing-data",
    "href": "slides/wrangling/11-importing-files.html#importing-data",
    "title": "Importing files",
    "section": "Importing data",
    "text": "Importing data\n\nOne of the most common ways of storing and sharing data is through electronic spreadsheets.\nA spreadsheet is a file version of a data frame.\nBut there are many ways to store spreadsheets in files."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#importing-data-1",
    "href": "slides/wrangling/11-importing-files.html#importing-data-1",
    "title": "Importing files",
    "section": "Importing data",
    "text": "Importing data\nTo import data we need to:\n\nIdentify the file’s location.\nKnow what function or parsers to use.\n\nFor the second step it helps to know the file type and encoding."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#file-types",
    "href": "slides/wrangling/11-importing-files.html#file-types",
    "title": "Importing files",
    "section": "File types",
    "text": "File types\n\nFiles can generally be classified into two categories: text and binary.\nWe describe the most widely used format for storing data for both these types and learn how to identify them."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#text-files",
    "href": "slides/wrangling/11-importing-files.html#text-files",
    "title": "Importing files",
    "section": "Text files",
    "text": "Text files\n\nYou have already worked with text files: R scripts and Quarto files, for example.\ndslabs offers examples:\n\n\ndir &lt;- system.file(package = \"dslabs\") \nfile_path &lt;- file.path(dir, \"extdata/murders.csv\") \nfile.copy(file_path, \"murders.csv\") \n\n[1] TRUE"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#text-files-1",
    "href": "slides/wrangling/11-importing-files.html#text-files-1",
    "title": "Importing files",
    "section": "Text files",
    "text": "Text files\n\nAn advantage of text files is that we can easily “look” at them without having to purchase any kind of special software or follow complicated instructions.\nExercise:\n\ncopy murders.csv into your working directory and examine it with less.\nThen try the Open file RStudio tool."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#text-files-2",
    "href": "slides/wrangling/11-importing-files.html#text-files-2",
    "title": "Importing files",
    "section": "Text files",
    "text": "Text files\n\nLine breaks are used to separate rows and a delimiter to separate columns within a row.\nThe most common delimiters are comma (,), semicolon (;), space (), and tab (\\t).\nDifferent parsers are used to read these files, so we need to know what delimiter was used.\nIn some cases, the delimiter can be inferred from file suffix: csv, tsv, for example.\nBut we recommend looking at the file rather than inferring from the suffix."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#text-files-3",
    "href": "slides/wrangling/11-importing-files.html#text-files-3",
    "title": "Importing files",
    "section": "Text files",
    "text": "Text files\n\nIn R, you can look at any number of lines from within R using the readLines function:\n\n\nreadLines(\"murders.csv\", n = 3) \n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\""
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#binary-files",
    "href": "slides/wrangling/11-importing-files.html#binary-files",
    "title": "Importing files",
    "section": "Binary files",
    "text": "Binary files\n\nOpening image files such as jpg or png in a text editor or using readLines in R will not show comprehensible content: these are binary files.\nUnlike text files, which are designed for human readability and have standardized conventions, binary files have many formats specific to their data type.\nWhile R’s readBin function can process any binary file, interpreting the output necessitates a thorough understanding of the file’s structure.\nWe focus on the the most prevalent binary formats for spreadsheets: Microsoft Excel xls and xlsx."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#r-base-parsers",
    "href": "slides/wrangling/11-importing-files.html#r-base-parsers",
    "title": "Importing files",
    "section": "R Base Parsers",
    "text": "R Base Parsers\nHere example of useful R base parsers:\n\nx &lt;- read.table(\"murders.csv\", sep = \"\\t\")\nx &lt;- read.csv(\"murders.csv\")"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#readr-parsers",
    "href": "slides/wrangling/11-importing-files.html#readr-parsers",
    "title": "Importing files",
    "section": "readr Parsers",
    "text": "readr Parsers\nreadr provides alternatives that produce tibbles:\n\nlibrary(readr)\nx &lt;- read_csv(\"murders.csv\")\nx &lt;- read_delim(\"murders.csv\", delim = \"\\t\")\n\nNotice the messages produced."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#readr-parsers-1",
    "href": "slides/wrangling/11-importing-files.html#readr-parsers-1",
    "title": "Importing files",
    "section": "readr Parsers",
    "text": "readr Parsers\n\n\n\n\n\n\n\n\nFunction\nFormat\nSuffix\n\n\n\n\nread_table\nwhite space separated values\ntxt\n\n\nread_csv\ncomma separated values\ncsv\n\n\nread_csv2\nsemicolon separated values\ncsv\n\n\nread_tsv\ntab separated values\ntsv\n\n\nread_delim\nmust define delimiter\ntxt\n\n\nread_lines\nsimilar to readLines\nany file"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#readxl-parsers",
    "href": "slides/wrangling/11-importing-files.html#readxl-parsers",
    "title": "Importing files",
    "section": "readxl Parsers",
    "text": "readxl Parsers\nFor Excel files you can use the readxl package.\n\nlibrary(readxl)\nfn &lt;- file.path(dir, \"extdata/2010_bigfive_regents.xls\") \ny &lt;- read_xls(fn)"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#readxl-parsers-1",
    "href": "slides/wrangling/11-importing-files.html#readxl-parsers-1",
    "title": "Importing files",
    "section": "readxl Parsers",
    "text": "readxl Parsers\nYou can read specific sheets and see them using\n\nexcel_sheets(fn)\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n\nNote that read_xls has a sheet argument."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#readxl-parsers-2",
    "href": "slides/wrangling/11-importing-files.html#readxl-parsers-2",
    "title": "Importing files",
    "section": "readxl Parsers",
    "text": "readxl Parsers\n\n\n\n\n\n\n\n\nFunction\nFormat\nSuffix\n\n\n\n\nread_excel\nauto detect the format\nxls, xlsx\n\n\nread_xls\noriginal format\nxls\n\n\nread_xlsx\nnew format\nxlsx\n\n\nexcel_sheets\ndetects sheets\nxls, xlsx"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#data.table-parsers",
    "href": "slides/wrangling/11-importing-files.html#data.table-parsers",
    "title": "Importing files",
    "section": "data.table Parsers",
    "text": "data.table Parsers\nThe data.table package provide a very fast parser:\n\nlibrary(data.table)\nx &lt;- fread(\"murders.csv\")\n\nNote: It returns a file in data.table format which we have mentioned but not explained."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#scan",
    "href": "slides/wrangling/11-importing-files.html#scan",
    "title": "Importing files",
    "section": "scan",
    "text": "scan\n\nThe scan function is the most general parser.\nIt will read in any text file and return a vector so you are on your own coverting it to a data frame.\nBecause it returns a vector, you need to tell it in advance what data type to expect:\n\n\nscan(\"murders.csv\", what = \"c\", sep = \",\", n = 10)\n\n [1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n [6] \"Alabama\"    \"AL\"         \"South\"      \"4779736\"    \"135\"       \n\n\n\nIt can also be used to read from the console. Try typing scan(). Hit return to stop."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding",
    "href": "slides/wrangling/11-importing-files.html#encoding",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nComputer translates everything into 0s and 1s.\nASCII is an encoding system that assigns specific numbers to characters.\nUsing 7 bits, ASCII can represent \\(2^7 = 128\\) unique symbols, sufficient for all English keyboard characters.\nHowever, many global languages contain characters outside ASCII’s range."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding-1",
    "href": "slides/wrangling/11-importing-files.html#encoding-1",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nFor instance, the é in “México” isn’t in ASCII’s catalog.\nTo address this, broader encodings emerged.\nUnicode offers variations using 8, 16, or 32 bits, known as UTF-8, UTF-16, and UTF-32.\nRStudio typically uses UTF-8 as its default.\nNotably, ASCII is a subset of UTF-8, meaning that if a file is ASCII-encoded, presuming it’s UTF-8 encoded won’t cause issues."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding-2",
    "href": "slides/wrangling/11-importing-files.html#encoding-2",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nHowever, there other encodings, such as ISO-8859-1 (also known as Latin-1) developed for the western European languages, Big5 for Traditional Chinese, and ISO-8859-6 for Arabic.\nTake a look at this file:\n\n\nfn &lt;- \"calificaciones.csv\" \nfile.copy(file.path(system.file(\"extdata\", package = \"dslabs\"), fn), fn) \n\n[1] TRUE\n\nreadLines(fn, n = 1) \n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\""
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding-3",
    "href": "slides/wrangling/11-importing-files.html#encoding-3",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nThe readr parsers permit us to specify an encoding.\nIt also includes a function that tries to guess the encoding:\n\n\nguess_encoding(\"murders.csv\") \n\n# A tibble: 1 × 2\n  encoding confidence\n  &lt;chr&gt;         &lt;dbl&gt;\n1 ASCII             1\n\nguess_encoding(\"calificaciones.csv\") \n\n# A tibble: 3 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding-4",
    "href": "slides/wrangling/11-importing-files.html#encoding-4",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nOnce we know the encoding we can specify it through the locale argument:\n\n\ndat &lt;- read_csv(\"calificaciones.csv\", show_col_types = FALSE, \n                locale = locale(encoding = \"ISO-8859-1\")) \n\n\nWe learn about locales in later."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#encoding-5",
    "href": "slides/wrangling/11-importing-files.html#encoding-5",
    "title": "Importing files",
    "section": "Encoding",
    "text": "Encoding\n\nWe can now see that the characters in the header were read in correctly:\n\n\ndat\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02        875\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05        990\n3 João     10 de junio de 1931      2023-09-21 22:43:28        989\n4 López    24 de julio de 1969      2023-09-22 01:06:59        887\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37        931\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21        887\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02        830"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#downloading-files",
    "href": "slides/wrangling/11-importing-files.html#downloading-files",
    "title": "Importing files",
    "section": "Downloading files",
    "text": "Downloading files\n\nA common place for data to reside is on the internet.\nWe can download these files and then import them.\nWe can also read them directly from the web.\n\n\nurl &lt;- paste0(\"https://raw.githubusercontent.com/\", \n              \"rafalab/dslabs/master/inst/extdata/murders.csv\") \nx &lt;- read.csv(url)"
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#downloading-files-1",
    "href": "slides/wrangling/11-importing-files.html#downloading-files-1",
    "title": "Importing files",
    "section": "Downloading files",
    "text": "Downloading files\n\nIf you want a local copy, you can use `download.file:\n\n\ndownload.file(url, \"murders.csv\") \n\n\nThis will download the file and save it on your system with the name murders.csv.\nNote You can use any name here, not necessarily murders.csv."
  },
  {
    "objectID": "slides/wrangling/11-importing-files.html#downloading-files-2",
    "href": "slides/wrangling/11-importing-files.html#downloading-files-2",
    "title": "Importing files",
    "section": "Downloading files",
    "text": "Downloading files\n\n\n\n\n\n\nWarning\n\n\nThe function download.file overwrites existing files without warning.\n\n\n\n\nTwo functions that are sometimes useful when downloading data from the internet are tempdir and tempfile.\n\n\ntmp_filename &lt;- tempfile() \ndownload.file(url, tmp_filename) \ndat &lt;- read_csv(tmp_filename) \nfile.remove(tmp_filename)"
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#data-apis",
    "href": "slides/wrangling/14-data-apis.html#data-apis",
    "title": "Data APIs",
    "section": "Data APIs",
    "text": "Data APIs\n\nAn Application Programming Interface (API) is a set of rules and protocols that allows different software entities to communicate with each other.\nIt defines methods and data formats that software components should use when requesting and exchanging information.\nAPIs play a crucial role in enabling the integration that make today’s software so interconnected and versatile."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#types-and-concepts",
    "href": "slides/wrangling/14-data-apis.html#types-and-concepts",
    "title": "Data APIs",
    "section": "Types and concepts",
    "text": "Types and concepts\nThe main APIs related to retrieving data are:\n\nWeb Services - Often built using protocols like HTTP/HTTPS.\nDatabase APIs - Enable communication between an application and a database, SQL-based calls for example.\n\nHere we focus on Web Services since it more common among public resources such CDC and the US Census."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#key-concepts",
    "href": "slides/wrangling/14-data-apis.html#key-concepts",
    "title": "Data APIs",
    "section": "Key concepts",
    "text": "Key concepts\n\nEndpoints: Usually a URL where API can be accessed.\nMethods: Actions that can be performed, for example HTTP methods like GET, POST, PUT, or DELETE.\nRequest: Asking the API to perform a function.\nResponse: The data it returns.\nRate Limits: Restrictions on calls to API.\nAuthentication and Authorization: Methods include API keys, OAuth, or Jason Web Tokens (JWT).\nData Formats: Many web APIs exchange data in a specific format, often JSON or CSV."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#json",
    "href": "slides/wrangling/14-data-apis.html#json",
    "title": "Data APIs",
    "section": "JSON",
    "text": "JSON\n\nSharing data on the internet has become more and more common.\nUnfortunately, providers use different formats, which makes wrangling harder.\nYet there are some standards that are also becoming more common.\nA format that is widely being adopted is the JavaScript Object Notation or JSON.\nBecause this format is very general, it is nothing like a spreadsheet."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#json-1",
    "href": "slides/wrangling/14-data-apis.html#json-1",
    "title": "Data APIs",
    "section": "JSON",
    "text": "JSON\n\nJSON files look like code you use to define a list:\n\n\n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\n\nThe file above actually represents a data frame."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#json-2",
    "href": "slides/wrangling/14-data-apis.html#json-2",
    "title": "Data APIs",
    "section": "JSON",
    "text": "JSON\n\nWe can use the function fromJSON from the jsonlite package to read files.\nHere is an example providing information Nobel prize winners:\n\n\nlibrary(jsonlite) \nnobel &lt;- fromJSON(\"http://api.nobelprize.org/v1/prize.json\") \n\n\nThis downloads a JSON file and reads into a list:\n\n\nclass(nobel)\n\n[1] \"list\""
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#json-3",
    "href": "slides/wrangling/14-data-apis.html#json-3",
    "title": "Data APIs",
    "section": "JSON",
    "text": "JSON\nThe JSON parsers have arguments that make the list components into vectors and lists into data frames when possible:\n\nsimplifyVector\nsimplifyDataFrame\nsimplifyMatrix\nflatten"
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#json-4",
    "href": "slides/wrangling/14-data-apis.html#json-4",
    "title": "Data APIs",
    "section": "JSON",
    "text": "JSON\n\nThe object is rather complicated. The prizes component includes a list of data frames with information about Nobel Laureates:\n\n\nnobel$prizes |&gt; \n  filter(category == \"literature\" & year == \"1971\") |&gt;  \n  pull(laureates) |&gt; \n  first() |&gt; \n  select(id, firstname, surname) \n\n   id firstname surname\n1 645     Pablo  Neruda"
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nHTTPS is the most widely used protocol for data sharing through the internet.\nThe httr2 package provides functions to work with HTTPS requests.\nOne of the core functions in this package is request, which is used to form request to send to web services.\nThe req_perform function sends the request.\nThis request function forms an HTTP GET request to the specified URL."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-1",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-1",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nTypically, HTTP GET requests are used to retrieve information from a server based on the provided URL.\nThe function returns an object of class response.\nThis object contains all the details of the server’s response, including status code, headers, and content.\nYou can then use other httr2 functions to extract or interpret information from this response.\nLet’s say you want to retrieve COVID-19 deaths by state from the CDC."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-2",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-2",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nBy visiting their data catalog https://data.cdc.gov you can search for datasets and find that the data is provided through this API:\n\n\nurl &lt;- \"https://data.cdc.gov/resource/muzy-jte6.csv\" \n\n\nWe can then make create and perform a request like this:\n\n\nlibrary(httr2) \nresponse &lt;- request(url) |&gt; req_perform()"
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-3",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-3",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nWe can see the results of the request by looking at the returned object.\n\n\nresponse\n\n&lt;httr2_response&gt;\n\n\nGET https://data.cdc.gov/resource/muzy-jte6.csv\n\n\nStatus: 200 OK\n\n\nContent-Type: text/csv\n\n\nBody: In memory (210808 bytes)"
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-4",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-4",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nTo extract the body, which is where the data are, we can use resp_body_string and send the result, a comma delimited string, to read_csv.\n\n\nlibrary(readr) \ntab &lt;- response |&gt; resp_body_string() |&gt; read_csv() \n\n\nWe note that the returned object is only 1000 entries.\nAPI often limit how much you can download."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-5",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-5",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nThe documentation for this API explains that we can change this limit through the.\n$limit parameters.\nWe can use the req_url_path_append to add this to our request:\n\n\nresponse &lt;- request(url) |&gt;  \n  req_url_path_append(\"?$limit=100000\") |&gt;  \n  req_perform()  \n\n\nThe CDC service returns data in csv format but a more common format used by web services is JSON."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-6",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-6",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nThe CDC also provides data in json format through:\n\n\nurl &lt;- \"https://data.cdc.gov/resource/muzy-jte6.json\" \n\n\nTo extract the data table we use the fromJSON function from the jsonlite package.\n\n\ntab &lt;- request(url) |&gt;  \n   req_perform() |&gt;  \n   resp_body_json(simplifyDataFrame = TRUE) \n\n\nWhen working with APIs, it’s essential to check the API’s documentation for rate limits, required headers, or authentication methods."
  },
  {
    "objectID": "slides/wrangling/14-data-apis.html#the-httr2-package-7",
    "href": "slides/wrangling/14-data-apis.html#the-httr2-package-7",
    "title": "Data APIs",
    "section": "The httr2 package",
    "text": "The httr2 package\n\nThe httr2 package provides tools to handle these requirements, such as setting headers or authentication parameters."
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling",
    "href": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling",
    "title": "Introduction to Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe two datasets used in the problem sets are already tidy data frames.\nHowever, very rarely in a data science project is data already available in this form.\nMuch more typical is for the data to be in a file, a database, or extracted from a document, including web pages, tweets, or PDFs.\nAs a result, data might be unstructured in complex ways."
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-1",
    "href": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-1",
    "title": "Introduction to Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling is what we call the process of structuring data from it’s original state into a form that permits us to focus on analysis.\nTidy data is an example of a form that permits us to focus on analysis.\nWe focus on tidy data as a target, but we can have other forms as targets, such as matrices."
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-2",
    "href": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-2",
    "title": "Introduction to Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nData wrangling can involve several complicated steps such as:\n\nextracting data from a file,\nconverting nested key-value pairs into a data frame,\nintegrating data from different source, and\nconstructing requests for data bases."
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-3",
    "href": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-3",
    "title": "Introduction to Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nToday we briefly discuss five concepts/tools considered essential for data wrangling:\n\nImporting data from files\nRESTful APIs\nJoining tables\nhtml parsing\nworking with dates and times\nlocales"
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-4",
    "href": "slides/wrangling/10-intro-to-wrangling.html#data-wrangling-4",
    "title": "Introduction to Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nWe barely scratch the surface on these topics.\nRarely are all these relevant for a single analysis.\nYou will likely face them all at some point.\nLecture goal is to make you aware of challenges, tools to tackle them, and help you learn how to learn more."
  },
  {
    "objectID": "slides/wrangling/10-intro-to-wrangling.html#further-learning",
    "href": "slides/wrangling/10-intro-to-wrangling.html#further-learning",
    "title": "Introduction to Wrangling",
    "section": "Further learning",
    "text": "Further learning\n\nSQL widely used in data-intensive industries to manage and manipulate large databases.\nIn R, dplyr functions like filter, select, and the joins we will learn this week, mirror SQL operations.\nBy learning dplyr, you’ve already covered many key SQL concepts and makes the transition to SQL easier.\nRecommended SQL Resources: W3Schools, Codecademy, Khan Academy, and SQLZoo"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#ggplot2",
    "href": "slides/R/08-ggplot2.html#ggplot2",
    "title": "ggplot2",
    "section": "ggplot2",
    "text": "ggplot2\nThe code in this lecture assumes these three libraries are loaded:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dslabs)"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#the-components-of-a-graph",
    "href": "slides/R/08-ggplot2.html#the-components-of-a-graph",
    "title": "ggplot2",
    "section": "The components of a graph",
    "text": "The components of a graph\nIn today’s lecture we recreate this:"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#the-components-of-a-graph-1",
    "href": "slides/R/08-ggplot2.html#the-components-of-a-graph-1",
    "title": "ggplot2",
    "section": "The components of a graph",
    "text": "The components of a graph\n\ngg stands for grammar of graphics.\nAnalogy: we learn verbs and nouns to construct sentences.\nThe first step in learning ggplot2 is breaking a graph apart into components.\nLet’s break down the plot we want to recreate while introducing some ggplot2 terminology."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#the-components-of-a-graph-2",
    "href": "slides/R/08-ggplot2.html#the-components-of-a-graph-2",
    "title": "ggplot2",
    "section": "The components of a graph",
    "text": "The components of a graph\n\nThe main three components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\n\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component.\nAesthetic mapping: How we map visual cues to information provided by the dataset."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#aesthetic-mapping",
    "href": "slides/R/08-ggplot2.html#aesthetic-mapping",
    "title": "ggplot2",
    "section": "Aesthetic mapping",
    "text": "Aesthetic mapping\n\nThe two most important cues in the plot we are recreating are the point positions on the x-axis and y-axis.\nEach point represents a different observation, and we map data about these observations to visual cues like x- and y-scale.\nColor is another visual cue that we map to region.\nHow we define the mapping depends on what geometry we are using."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#the-components-of-a-graph-3",
    "href": "slides/R/08-ggplot2.html#the-components-of-a-graph-3",
    "title": "ggplot2",
    "section": "The components of a graph",
    "text": "The components of a graph\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data.\nAxes are in the log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nWe will now construct the plot, piece by piece."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#ggplot-objects",
    "href": "slides/R/08-ggplot2.html#ggplot-objects",
    "title": "ggplot2",
    "section": "ggplot objects",
    "text": "ggplot objects\n\nStart by defining the dataset:\n\n\nggplot(data = murders)\n\n\nWe can also use the pipe:\n\n\nmurders |&gt; ggplot()\n\n\nWe call also assign the output to a variable\n\n\np &lt;- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\""
  },
  {
    "objectID": "slides/R/08-ggplot2.html#ggplot-objects-1",
    "href": "slides/R/08-ggplot2.html#ggplot-objects-1",
    "title": "ggplot2",
    "section": "ggplot objects",
    "text": "ggplot objects\nTo see the plot we can print it:\n\nprint(p)"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#geometries",
    "href": "slides/R/08-ggplot2.html#geometries",
    "title": "ggplot2",
    "section": "Geometries",
    "text": "Geometries\n\nWe create graphs by adding layers.\nLayers define geometries, compute summary statistics, define what scales to use, or even change styles.\nTo add layers, we use the symbol +.\nIn general, a line of code will look like this:\n\n\nDATA |&gt; ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\n\nUsually, the first added layer defines the geometry."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#geometries-1",
    "href": "slides/R/08-ggplot2.html#geometries-1",
    "title": "ggplot2",
    "section": "Geometries",
    "text": "Geometries\n\nSo if we want to make a scatterplot, what geometry do we use?\nLet’s look at the cheat sheet: https://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#aesthetic-mappings",
    "href": "slides/R/08-ggplot2.html#aesthetic-mappings",
    "title": "ggplot2",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nTo make a scatter plot we use geom_points.\nThe help file tells us this is how we use it:\n\n\nmurders |&gt; ggplot() + geom_point(aes(x = population/10^6, y = total))"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#aesthetic-mappings-1",
    "href": "slides/R/08-ggplot2.html#aesthetic-mappings-1",
    "title": "ggplot2",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nSince we defined p earlier, we can add a layer like this:\n\n\np + geom_point(aes(population/10^6, total))\n\n\n\nNote we are no longer using x= and y =."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#layers",
    "href": "slides/R/08-ggplot2.html#layers",
    "title": "ggplot2",
    "section": "Layers",
    "text": "Layers\n\nTo add text we use geom_text:\n\n\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#layers-1",
    "href": "slides/R/08-ggplot2.html#layers-1",
    "title": "ggplot2",
    "section": "Layers",
    "text": "Layers\n\nAs an example of the unique behavior of aes note that this call:\n\n\np_test &lt;- p + geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) \n\nwill give you an error since abb is not found because it is outside of the aes function.\n\nThe layer geom_text does not know where to find abb: it’s a column name and not a global variable."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#tinkering-with-arguments",
    "href": "slides/R/08-ggplot2.html#tinkering-with-arguments",
    "title": "ggplot2",
    "section": "Tinkering with arguments",
    "text": "Tinkering with arguments\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\nsize can be an aesthetic mapping, but here it is not, so all points get bigger."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#tinkering-with-arguments-1",
    "href": "slides/R/08-ggplot2.html#tinkering-with-arguments-1",
    "title": "ggplot2",
    "section": "Tinkering with arguments",
    "text": "Tinkering with arguments\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5)\n\n\n\nnudge_x is not an aesthetic mapping."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#global-versus-local-mappings",
    "href": "slides/R/08-ggplot2.html#global-versus-local-mappings",
    "title": "ggplot2",
    "section": "Global versus local mappings",
    "text": "Global versus local mappings\n\nNote that in we can define a global aes in the ggplot function:\n\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\n\nWe refer to this as the global mapping."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#global-versus-local-mappings-1",
    "href": "slides/R/08-ggplot2.html#global-versus-local-mappings-1",
    "title": "ggplot2",
    "section": "Global versus local mappings",
    "text": "Global versus local mappings\n\nAll the layers will assume the global mapping unless we explicitly define another one.\n\n\np &lt;- murders |&gt; ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) + geom_text(nudge_x = 1.5)\n\n\n\nThe two layers use the global mapping."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#global-versus-local-mappings-2",
    "href": "slides/R/08-ggplot2.html#global-versus-local-mappings-2",
    "title": "ggplot2",
    "section": "Global versus local mappings",
    "text": "Global versus local mappings\n\nWe can override the global aes by defining one in the geometry functions:\n\n\np + geom_point(size = 3) +  \n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#scales",
    "href": "slides/R/08-ggplot2.html#scales",
    "title": "ggplot2",
    "section": "Scales",
    "text": "Scales\n\nLayers can define transformations:\n\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#scales-1",
    "href": "slides/R/08-ggplot2.html#scales-1",
    "title": "ggplot2",
    "section": "Scales",
    "text": "Scales\n\nThis particular transformation is so common that ggplot2 provides the specialized functions:\n\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#labels-and-titles",
    "href": "slides/R/08-ggplot2.html#labels-and-titles",
    "title": "ggplot2",
    "section": "Labels and titles",
    "text": "Labels and titles\n\nThere are layers for adding labels and titles:\n\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#labels-and-titles-1",
    "href": "slides/R/08-ggplot2.html#labels-and-titles-1",
    "title": "ggplot2",
    "section": "Labels and titles",
    "text": "Labels and titles\n\nWe can also use the labs function:\n\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\")\n\n\nThis produces the same graph as in the previous slide."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#almost-there",
    "href": "slides/R/08-ggplot2.html#almost-there",
    "title": "ggplot2",
    "section": "Almost there",
    "text": "Almost there\n\nTargetCurrent draftCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#adding-color",
    "href": "slides/R/08-ggplot2.html#adding-color",
    "title": "ggplot2",
    "section": "Adding color",
    "text": "Adding color\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\") +\n  geom_point(size = 3, color = \"blue\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#a-mapped-color",
    "href": "slides/R/08-ggplot2.html#a-mapped-color",
    "title": "ggplot2",
    "section": "A mapped color",
    "text": "A mapped color\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\") +\n  geom_point(aes(col = region), size = 3)\n\n\nA legend is added automatically!"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#change-legend-name",
    "href": "slides/R/08-ggplot2.html#change-legend-name",
    "title": "ggplot2",
    "section": "Change legend name",
    "text": "Change legend name\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\",\n       color = \"Region\") +\n  geom_point(aes(col = region), size = 3)"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#other-adjustments",
    "href": "slides/R/08-ggplot2.html#other-adjustments",
    "title": "ggplot2",
    "section": "Other adjustments",
    "text": "Other adjustments\n\nWe want to add a line with intercept the US rate.\nLets compute that\n\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt; \n  pull(rate)"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-a-line",
    "href": "slides/R/08-ggplot2.html#add-a-line",
    "title": "ggplot2",
    "section": "Add a line",
    "text": "Add a line\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\",\n       color = \"Region\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#we-are-close",
    "href": "slides/R/08-ggplot2.html#we-are-close",
    "title": "ggplot2",
    "section": "We are close!",
    "text": "We are close!"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#other-adjustments-1",
    "href": "slides/R/08-ggplot2.html#other-adjustments-1",
    "title": "ggplot2",
    "section": "Other adjustments",
    "text": "Other adjustments\n\nTo make the final adjustments we will save our current draft in p and add layers.\n\n\np &lt;- murders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\",\n       color = \"Region\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\")"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages",
    "href": "slides/R/08-ggplot2.html#add-on-packages",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\n\nThe dslabs package can define the look used in the textbook:\n\n\nds_theme_set()\n\n\nMany other themes are added by the package ggthemes."
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-1",
    "href": "slides/R/08-ggplot2.html#add-on-packages-1",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\nggthemes provides pre-designed themes.\n\nlibrary(ggthemes)\np + theme_economist()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-2",
    "href": "slides/R/08-ggplot2.html#add-on-packages-2",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\nHere is the FiveThirtyEight theme:\n\np + theme_fivethirtyeight()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-3",
    "href": "slides/R/08-ggplot2.html#add-on-packages-3",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\nIf you want to ruin the plot use the excel theme:\n\np + theme_excel()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-4",
    "href": "slides/R/08-ggplot2.html#add-on-packages-4",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\nThemePark provides fun themes:\n\nlibrary(ThemePark)\np + theme_starwars()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-5",
    "href": "slides/R/08-ggplot2.html#add-on-packages-5",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\nThis is a fan favorite:\n\np + theme_barbie()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#add-on-packages-6",
    "href": "slides/R/08-ggplot2.html#add-on-packages-6",
    "title": "ggplot2",
    "section": "Add-on packages",
    "text": "Add-on packages\n\nTo avoid the state abbreviations being on top of each other we can use the ggrepel package.\nWe change the layer geom_text(nudge_x = 0.05) to geom_text_repel()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#putting-it-all-together",
    "href": "slides/R/08-ggplot2.html#putting-it-all-together",
    "title": "ggplot2",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nCodePlot\n\n\n\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt;\n  pull(rate)\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_text_repel() + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\",\n       color = \"Region\") +\n  theme_economist()"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#grids-of-plots",
    "href": "slides/R/08-ggplot2.html#grids-of-plots",
    "title": "ggplot2",
    "section": "Grids of plots",
    "text": "Grids of plots\n\nWe often want to put plots next to each other.\nThe gridExtra package permits us to do that:\n\n\nCodePlot\n\n\n\nlibrary(gridExtra)\np1 &lt;- murders |&gt; \n  ggplot(aes(log10(population))) + \n  geom_histogram()\np2 &lt;- murders |&gt; \n  gplot(aes(log10(population), log10(total))) + \n  geom_point()\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "slides/R/08-ggplot2.html#grids-of-plots-1",
    "href": "slides/R/08-ggplot2.html#grids-of-plots-1",
    "title": "ggplot2",
    "section": "Grids of plots",
    "text": "Grids of plots\n\nThere are several additional packages for combining ggplot2 plots into visually appealing layouts:**\ncowplot: A versatile package designed for publication-quality plots, offering seamless integration with ggplot2.\nggpubr: Provides user-friendly functions for combining and annotating ggplot2 plots with minimal effort.\nNew packages frequently emerge. Explore beyond these options and stay curious—there might be new tools that suit your needs even better!"
  },
  {
    "objectID": "slides/R/06-vectorization.html#vectorization",
    "href": "slides/R/06-vectorization.html#vectorization",
    "title": "Vectorization",
    "section": "Vectorization",
    "text": "Vectorization\n\nWe will be using the murders dataset in the dslabs package.\nIncludes data on 2010 gun murders for the US 50 states and DC.\nWe will use it to answer questions such as “What is the state with lowest crime rate in the Western part of the US?”"
  },
  {
    "objectID": "slides/R/06-vectorization.html#vectorization-1",
    "href": "slides/R/06-vectorization.html#vectorization-1",
    "title": "Vectorization",
    "section": "Vectorization",
    "text": "Vectorization\n\nFirst, some simple examples of vectorization.\nLet’s convert the following heights in inches to meters:\n\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\n\nRather than loop we use vectorization:\n\n\nheights*2.54/100\n\n [1] 1.7526 1.5748 1.6764 1.7780 1.7780 1.8542 1.7018 1.8542 1.7018 1.7780"
  },
  {
    "objectID": "slides/R/06-vectorization.html#vectorization-2",
    "href": "slides/R/06-vectorization.html#vectorization-2",
    "title": "Vectorization",
    "section": "Vectorization",
    "text": "Vectorization\n\nWe can subtract a constant from each element of a vector.\nThis is convenient for computing residuals or deviations from an average:\n\n\navg &lt;- mean(heights)\nheights - avg \n\n [1]  0.3 -6.7 -2.7  1.3  1.3  4.3 -1.7  4.3 -1.7  1.3"
  },
  {
    "objectID": "slides/R/06-vectorization.html#vectorization-3",
    "href": "slides/R/06-vectorization.html#vectorization-3",
    "title": "Vectorization",
    "section": "Vectorization",
    "text": "Vectorization\n\nThis means we can compute standard units like this:\n\n\ns &lt;- sd(heights)\n(heights - avg)/s\n\n [1]  0.08995503 -2.00899575 -0.80959530  0.38980515  0.38980515  1.28935548\n [7] -0.50974519  1.28935548 -0.50974519  0.38980515\n\n\n\nThere is actually a function, scale, that does this. We describe it soon."
  },
  {
    "objectID": "slides/R/06-vectorization.html#vectorization-4",
    "href": "slides/R/06-vectorization.html#vectorization-4",
    "title": "Vectorization",
    "section": "Vectorization",
    "text": "Vectorization\n\nIf we operate on two vectors, vectorization is componentwise.\nHere is an example:\n\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\nerror &lt;- rnorm(length(heights), 0, 0.1)\nheights + error\n\n [1] 69.03952 61.84743 66.01834 69.88322 69.69173 73.23231 66.96888 72.94392\n [9] 66.91477 69.96632"
  },
  {
    "objectID": "slides/R/06-vectorization.html#exercise",
    "href": "slides/R/06-vectorization.html#exercise",
    "title": "Vectorization",
    "section": "Exercise",
    "text": "Exercise\n\nAdd a column to the murders dataset with the murder rate.\nUse murders per 100,000 persons as the unit."
  },
  {
    "objectID": "slides/R/06-vectorization.html#functions-that-vectorize",
    "href": "slides/R/06-vectorization.html#functions-that-vectorize",
    "title": "Vectorization",
    "section": "Functions that vectorize",
    "text": "Functions that vectorize\n\nMost arithmetic functions work on vectors.\n\n\nx &lt;- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\n2^x\n\n [1]    2    4    8   16   32   64  128  256  512 1024"
  },
  {
    "objectID": "slides/R/06-vectorization.html#functions-that-vectorize-1",
    "href": "slides/R/06-vectorization.html#functions-that-vectorize-1",
    "title": "Vectorization",
    "section": "Functions that vectorize",
    "text": "Functions that vectorize\n\nscale(heights)\n\n             [,1]\n [1,]  0.08995503\n [2,] -2.00899575\n [3,] -0.80959530\n [4,]  0.38980515\n [5,]  0.38980515\n [6,]  1.28935548\n [7,] -0.50974519\n [8,]  1.28935548\n [9,] -0.50974519\n[10,]  0.38980515\nattr(,\"scaled:center\")\n[1] 68.7\nattr(,\"scaled:scale\")\n[1] 3.335\n\n\nprovides the same results,\n\n(heights - mean(heights))/sd(heights)\n\n [1]  0.08995503 -2.00899575 -0.80959530  0.38980515  0.38980515  1.28935548\n [7] -0.50974519  1.28935548 -0.50974519  0.38980515"
  },
  {
    "objectID": "slides/R/06-vectorization.html#functions-that-vectorize-2",
    "href": "slides/R/06-vectorization.html#functions-that-vectorize-2",
    "title": "Vectorization",
    "section": "Functions that vectorize",
    "text": "Functions that vectorize\n\nBut scale coerces to a column matrix:\n\n\nclass(scale(heights))\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "slides/R/06-vectorization.html#functions-that-vectorize-3",
    "href": "slides/R/06-vectorization.html#functions-that-vectorize-3",
    "title": "Vectorization",
    "section": "Functions that vectorize",
    "text": "Functions that vectorize\n\nThe conditional function if-else does not vectorize.\nFunctions such as any and all, covert vectors to logicals of lenght one needed for if-else.\nA particularly useful function is a vectorized version ifelse.\nHere is an example:\n\n\na &lt;- c(0, 1, 2, -4, 5)\nifelse(a &gt; 0, 1/a, NA)\n\n[1]  NA 1.0 0.5  NA 0.2"
  },
  {
    "objectID": "slides/R/06-vectorization.html#indexing",
    "href": "slides/R/06-vectorization.html#indexing",
    "title": "Vectorization",
    "section": "Indexing",
    "text": "Indexing\n\nVectorization also works for logical relationships:\n\n\nlibrary(dslabs)\nind &lt;- murders$population &lt; 10^6\n\n\nA convenient aspect of this is that you can subset a vector using this logical vector for indexing:\n\n\nmurders$state[ind]\n\n[1] \"Alaska\"               \"Delaware\"             \"District of Columbia\"\n[4] \"Montana\"              \"North Dakota\"         \"South Dakota\"        \n[7] \"Vermont\"              \"Wyoming\""
  },
  {
    "objectID": "slides/R/06-vectorization.html#indexing-1",
    "href": "slides/R/06-vectorization.html#indexing-1",
    "title": "Vectorization",
    "section": "Indexing",
    "text": "Indexing\n\nYou can also use vectorization to apply logical operators:\n\n\nind &lt;- murders$population &lt; 10^6 & murders$region == \"West\"\nmurders$state[ind]\n\n[1] \"Alaska\"  \"Montana\" \"Wyoming\""
  },
  {
    "objectID": "slides/R/06-vectorization.html#split",
    "href": "slides/R/06-vectorization.html#split",
    "title": "Vectorization",
    "section": "split",
    "text": "split\n\nsplit is a useful function to get indexes using a factor:\n\n\ninds &lt;- with(murders, split(seq_along(region), region))\nmurders$state[inds$West]\n\n [1] \"Alaska\"     \"Arizona\"    \"California\" \"Colorado\"   \"Hawaii\"    \n [6] \"Idaho\"      \"Montana\"    \"Nevada\"     \"New Mexico\" \"Oregon\"    \n[11] \"Utah\"       \"Washington\" \"Wyoming\""
  },
  {
    "objectID": "slides/R/06-vectorization.html#functions-for-subsetting",
    "href": "slides/R/06-vectorization.html#functions-for-subsetting",
    "title": "Vectorization",
    "section": "Functions for subsetting",
    "text": "Functions for subsetting\n\nThe functions which, match and the operator %in% are useful for sub-setting\nTo understand how they work it’s best to use examples."
  },
  {
    "objectID": "slides/R/06-vectorization.html#which",
    "href": "slides/R/06-vectorization.html#which",
    "title": "Vectorization",
    "section": "which",
    "text": "which\n\nind &lt;- which(murders$state == \"California\")\nind\n\n[1] 5\n\nmurders[ind,]\n\n       state abb region population total\n5 California  CA   West   37253956  1257"
  },
  {
    "objectID": "slides/R/06-vectorization.html#match",
    "href": "slides/R/06-vectorization.html#match",
    "title": "Vectorization",
    "section": "match",
    "text": "match\n\nind &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nind\n\n[1] 33 10 44\n\nmurders[ind,]\n\n      state abb    region population total\n33 New York  NY Northeast   19378102   517\n10  Florida  FL     South   19687653   669\n44    Texas  TX     South   25145561   805"
  },
  {
    "objectID": "slides/R/06-vectorization.html#in",
    "href": "slides/R/06-vectorization.html#in",
    "title": "Vectorization",
    "section": "%in%",
    "text": "%in%\n\nind &lt;- which(murders$state %in% c(\"New York\", \"Florida\", \"Texas\"))\nind\n\n[1] 10 33 44\n\nmurders[ind,]\n\n      state abb    region population total\n10  Florida  FL     South   19687653   669\n33 New York  NY Northeast   19378102   517\n44    Texas  TX     South   25145561   805\n\n\n\nNote this is similar to using match.\nBut note the order is different."
  },
  {
    "objectID": "slides/R/06-vectorization.html#match-versus-in",
    "href": "slides/R/06-vectorization.html#match-versus-in",
    "title": "Vectorization",
    "section": "match versus %in%",
    "text": "match versus %in%\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE\n\n\n\nmatch(c(\"Boston\", \"Dakota\", \"Washington\"), murders$state)\n\n[1] NA NA 48\n\n\n\nmatch(murders$state, c(\"Boston\", \"Dakota\", \"Washington\"))\n\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA  3 NA NA\n[51] NA"
  },
  {
    "objectID": "slides/R/06-vectorization.html#the-apply-functions",
    "href": "slides/R/06-vectorization.html#the-apply-functions",
    "title": "Vectorization",
    "section": "The apply functions",
    "text": "The apply functions\n\nThe apply functions let use the concept of vectorization with functions that don’t vectorize.\nHere is an example of a function that won’t vectorize in a convenient way:\n\n\ns &lt;- function(n){\n   return(sum(1:n))\n}\n\n\nTry it on a vector:\n\n\nns &lt;- c(25, 100, 1000)\ns(ns)\n\n[1] 325"
  },
  {
    "objectID": "slides/R/06-vectorization.html#the-apply-functions-1",
    "href": "slides/R/06-vectorization.html#the-apply-functions-1",
    "title": "Vectorization",
    "section": "The apply functions",
    "text": "The apply functions\n\nWe can use sapply, one of the apply functions:\n\n\nsapply(ns, s)\n\n[1]    325   5050 500500\n\n\n\nsapply will work on any vector, including lists."
  },
  {
    "objectID": "slides/R/06-vectorization.html#the-apply-functions-2",
    "href": "slides/R/06-vectorization.html#the-apply-functions-2",
    "title": "Vectorization",
    "section": "The apply functions",
    "text": "The apply functions\n\nThere are other apply functions:\n\nlapply - returns a list. Convenient when the function returns something other than a number.\ntapply - can apply to subsets defined by second variable.\nmapply - multivariate version of sapply.\napply - applies function to rows or columns o matrix.\n\nWe will learn some of these as we go."
  },
  {
    "objectID": "slides/R/05-r-basics.html#packages",
    "href": "slides/R/05-r-basics.html#packages",
    "title": "R Basics",
    "section": "Packages",
    "text": "Packages\n\nUse install.packages to install the dslabs package.\nTryout the following functions: sessionInfo, installed.packages"
  },
  {
    "objectID": "slides/R/05-r-basics.html#prebuilt-functions",
    "href": "slides/R/05-r-basics.html#prebuilt-functions",
    "title": "R Basics",
    "section": "Prebuilt functions",
    "text": "Prebuilt functions\n\nMuch of what we do in R is based on prebuilt functions.\nMany are included in automatically loaded packages: stats, graphics, grDevices, utils, datasets, methods.\nThis subset of the R universe is refereed to as R base.\nVery popular packages not included in R base: ggplot2, dplyr, tidyr, and data.table."
  },
  {
    "objectID": "slides/R/05-r-basics.html#prebuilt-functions-1",
    "href": "slides/R/05-r-basics.html#prebuilt-functions-1",
    "title": "R Basics",
    "section": "Prebuilt functions",
    "text": "Prebuilt functions\n\nExample of prebuilt functions that we will use today: ls, rm, library, search, factor, list, exists, str, typeof, and class.\nYou can see the raw code for a function by typing it without the parenthesis: type ls on your console to see an example."
  },
  {
    "objectID": "slides/R/05-r-basics.html#help-system",
    "href": "slides/R/05-r-basics.html#help-system",
    "title": "R Basics",
    "section": "Help system",
    "text": "Help system\n\nIn R you can use ? or help to learn more about functions.\nYou can learn about function using\n\nhelp(\"ls\")\nor\n?ls"
  },
  {
    "objectID": "slides/R/05-r-basics.html#the-workspace",
    "href": "slides/R/05-r-basics.html#the-workspace",
    "title": "R Basics",
    "section": "The workspace",
    "text": "The workspace\n\nDefine a variable.\n\n\na &lt;- 2\n\n\nUse ls to see if it’s there. Also take a look at the Environment tab in RStudio.\n\n\nls()\n\n[1] \"a\"\n\n\n\nUse rm to remove the variable you defined.\n\n\nrm(a)"
  },
  {
    "objectID": "slides/R/05-r-basics.html#variable-name-convention",
    "href": "slides/R/05-r-basics.html#variable-name-convention",
    "title": "R Basics",
    "section": "Variable name convention",
    "text": "Variable name convention\n\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces.\nFor more we recommend this guide."
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-types",
    "href": "slides/R/05-r-basics.html#data-types",
    "title": "R Basics",
    "section": "Data types",
    "text": "Data types\nThe main data types in R are:\n\nOne dimensional vectors: numeric, integer, logical, complex, characters.\nFactors\nLists: this includes data frames.\nArrays: Matrices are the most widely used.\nDate and time\ntibble\nS4 objects"
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-types-1",
    "href": "slides/R/05-r-basics.html#data-types-1",
    "title": "R Basics",
    "section": "Data types",
    "text": "Data types\n\nMany errors in R come from confusing data types.\nstr stands for structure, gives us information about an object.\ntypeof gives you the basic data type of the object. It reveals the lower-level, more fundamental type of an object in R’s memory.\nclass This function returns the class attribute of an object. The class of an object is essentially type_of at a higher, often user-facing level."
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-types-2",
    "href": "slides/R/05-r-basics.html#data-types-2",
    "title": "R Basics",
    "section": "Data types",
    "text": "Data types\nLet’s see some example:\n\nlibrary(dslabs)\ntypeof(murders)\n\n[1] \"list\"\n\nclass(murders)\n\n[1] \"data.frame\"\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ..."
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-frames",
    "href": "slides/R/05-r-basics.html#data-frames",
    "title": "R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nDate frames are the most common class used in data analysis. It is like a spreadsheet.\nUsually, rows represents observations and columns variables.\nEach variable can be a different data type.\nYou can see part of the content like this\n\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65"
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-frames-1",
    "href": "slides/R/05-r-basics.html#data-frames-1",
    "title": "R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nand all of the content like this:\n\n\nView(murders)\n\n\nType the above in RStudio."
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-frames-2",
    "href": "slides/R/05-r-basics.html#data-frames-2",
    "title": "R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nA very common operation is adding columns like this:\n\n\nmurders$pop_rank &lt;- rank(murders$population)\nhead(murders)\n\n       state abb region population total pop_rank\n1    Alabama  AL  South    4779736   135       29\n2     Alaska  AK   West     710231    19        5\n3    Arizona  AZ   West    6392017   232       36\n4   Arkansas  AR  South    2915918    93       20\n5 California  CA   West   37253956  1257       51\n6   Colorado  CO   West    5029196    65       30"
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-frames-3",
    "href": "slides/R/05-r-basics.html#data-frames-3",
    "title": "R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nNote that we used $.\nThis is called the accessor because it lets us access columns.\n\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\n\nMore generally: used to access components of a list."
  },
  {
    "objectID": "slides/R/05-r-basics.html#data-frames-4",
    "href": "slides/R/05-r-basics.html#data-frames-4",
    "title": "R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nOne way R confuses beginners is by having multiple ways of doing the same thing.\nFor example you can access the 4th column in the following five different ways:\n\n\nmurders$population\nmurders[, \"population\"]\nmurders[[\"population\"]]\nmurders[, 4]\nmurders[[4]]\n\n\nIn general, we recommend using the name rather than the number as it is less likely to change."
  },
  {
    "objectID": "slides/R/05-r-basics.html#with",
    "href": "slides/R/05-r-basics.html#with",
    "title": "R Basics",
    "section": "with",
    "text": "with\n\nwith let’s us use the column names as objects.\nThis is convenient to avoid typing the data frame name over and over:\n\n\nrate &lt;- with(murders, total/population)"
  },
  {
    "objectID": "slides/R/05-r-basics.html#with-1",
    "href": "slides/R/05-r-basics.html#with-1",
    "title": "R Basics",
    "section": "with",
    "text": "with\n\nNote you can write entire code chunks by enclosing it in curly brackets:\n\n\nwith(murders, {\n   rate &lt;- total/population\n   rate &lt;- round(rate*10^5)\n   print(rate[1:5])\n})\n\n[1] 3 3 4 3 3"
  },
  {
    "objectID": "slides/R/05-r-basics.html#vectors",
    "href": "slides/R/05-r-basics.html#vectors",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\n\nThe columns of data frames are an example of one dimensional (atomic) vectors.\n\n\nlength(murders$population)\n\n[1] 51"
  },
  {
    "objectID": "slides/R/05-r-basics.html#vectors-1",
    "href": "slides/R/05-r-basics.html#vectors-1",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\n\nOften we have to create vectors.\nThe concatenate function c is the most basic way used to create vectors:\n\n\nx &lt;- c(\"b\", \"s\", \"t\", \" \", \"2\", \"6\", \"0\")"
  },
  {
    "objectID": "slides/R/05-r-basics.html#sequences",
    "href": "slides/R/05-r-basics.html#sequences",
    "title": "R Basics",
    "section": "Sequences",
    "text": "Sequences\n\nSequences are a the common example of vectors we generate.\n\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1, 9, 2)\n\n[1] 1 3 5 7 9\n\n\n\nWhen increasing by 1 you can use :\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "slides/R/05-r-basics.html#sequences-1",
    "href": "slides/R/05-r-basics.html#sequences-1",
    "title": "R Basics",
    "section": "Sequences",
    "text": "Sequences\n\nA useful function to quickly generate the sequence 1:length(x) is seq_along:\n\n\nx &lt;- c(\"b\", \"s\", \"t\", \" \", \"2\", \"6\", \"0\")\nseq_along(x)\n\n[1] 1 2 3 4 5 6 7\n\n\n\nA reason to use this is to loop through entries:\n\n\nfor (i in seq_along(x)) {\n  cat(toupper(x[i]))\n}\n\nBST 260"
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors",
    "href": "slides/R/05-r-basics.html#factors",
    "title": "R Basics",
    "section": "Factors",
    "text": "Factors\n\nOne key distinction between data types you need to understad is the difference between factors and characters.\nThe murder dataset has examples of both.\n\n\nclass(murders$state)\n\n[1] \"character\"\n\nclass(murders$region)\n\n[1] \"factor\"\n\n\n\nWhy do you think this is?"
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors-1",
    "href": "slides/R/05-r-basics.html#factors-1",
    "title": "R Basics",
    "section": "Factors",
    "text": "Factors\n\nFactors store levels and the label of each level.\nThis is useful for categorical data.\n\n\nx &lt;- murders$region\nlevels(x)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\""
  },
  {
    "objectID": "slides/R/05-r-basics.html#categories-based-on-strata",
    "href": "slides/R/05-r-basics.html#categories-based-on-strata",
    "title": "R Basics",
    "section": "Categories based on strata",
    "text": "Categories based on strata\n\nIn data analysis we often have to stratify continuous variables into categories.\nThe function cut helps us do this:\n\n\nage &lt;- c(5, 93, 18, 102, 14, 22, 45, 65, 67, 25, 30, 16, 21)\ncut(age, c(0, 11, 27, 43, 59, 78, 96, Inf))\n\n [1] (0,11]   (78,96]  (11,27]  (96,Inf] (11,27]  (11,27]  (43,59]  (59,78] \n [9] (59,78]  (11,27]  (27,43]  (11,27]  (11,27] \nLevels: (0,11] (11,27] (27,43] (43,59] (59,78] (78,96] (96,Inf]"
  },
  {
    "objectID": "slides/R/05-r-basics.html#categories-based-on-strata-1",
    "href": "slides/R/05-r-basics.html#categories-based-on-strata-1",
    "title": "R Basics",
    "section": "Categories based on strata",
    "text": "Categories based on strata\n\nWe can assign it more meaningful level names:\n\n\nage &lt;- c(5, 93, 18, 102, 14, 22, 45, 65, 67, 25, 30, 16, 21)\ncut(age, c(0, 11, 27, 43, 59, 78, 96, Inf), \n    labels = c(\"Alpha\", \"Zoomer\", \"Millennial\", \"X\", \"Boomer\", \"Silent\", \"Greatest\"))\n\n [1] Alpha      Silent     Zoomer     Greatest   Zoomer     Zoomer    \n [7] X          Boomer     Boomer     Zoomer     Millennial Zoomer    \n[13] Zoomer    \nLevels: Alpha Zoomer Millennial X Boomer Silent Greatest"
  },
  {
    "objectID": "slides/R/05-r-basics.html#changing-levels",
    "href": "slides/R/05-r-basics.html#changing-levels",
    "title": "R Basics",
    "section": "Changing levels",
    "text": "Changing levels\n\nThis is often needed for ordinal data because R defaults to alphabetical order:\n\n\ngen &lt;- factor(c(\"Alpha\", \"Zoomer\", \"Millennial\"))\nlevels(gen)\n\n[1] \"Alpha\"      \"Millennial\" \"Zoomer\"    \n\n\n\nYou can change this with the levels argument:\n\n\ngen &lt;- factor(gen, levels = c(\"Alpha\", \"Zoomer\", \"Millennial\", \"X\", \"Boomer\", \"Silent\", \"Greatest\"))\nlevels(gen)\n\n[1] \"Alpha\"      \"Zoomer\"     \"Millennial\" \"X\"          \"Boomer\"    \n[6] \"Silent\"     \"Greatest\""
  },
  {
    "objectID": "slides/R/05-r-basics.html#changing-levels-1",
    "href": "slides/R/05-r-basics.html#changing-levels-1",
    "title": "R Basics",
    "section": "Changing levels",
    "text": "Changing levels\n\nA common reason we need to change levels is to assure R is aware which is the reference strata.\nThis is important for linear models because the first level is assumed to be the reference.\n\n\nx &lt;- factor(c(\"no drug\", \"drug 1\", \"drug 2\"))\nlevels(x)\n\n[1] \"drug 1\"  \"drug 2\"  \"no drug\"\n\nx &lt;- relevel(x, ref = \"no drug\")\nlevels(x)          \n\n[1] \"no drug\" \"drug 1\"  \"drug 2\""
  },
  {
    "objectID": "slides/R/05-r-basics.html#changing-levels-2",
    "href": "slides/R/05-r-basics.html#changing-levels-2",
    "title": "R Basics",
    "section": "Changing levels",
    "text": "Changing levels\n\nWe often want to order strata based on a summary statistic.\nThis is common in data visualization.\nWe can use reorder for this:\n\n\nx &lt;- reorder(murders$region, murders$population, sum)"
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors-2",
    "href": "slides/R/05-r-basics.html#factors-2",
    "title": "R Basics",
    "section": "Factors",
    "text": "Factors\n\nAnother reason we used factors is because they more efficient:\n\n\nx &lt;- sample(murders$state[c(5,33,44)], 10^7, replace = TRUE)\ny &lt;- factor(x)\nobject.size(x)\n\n80000232 bytes\n\nobject.size(y)\n\n40000648 bytes\n\n\n\nAn integer is easier to store than a character string."
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors-3",
    "href": "slides/R/05-r-basics.html#factors-3",
    "title": "R Basics",
    "section": "Factors",
    "text": "Factors\nExercise: How can we make this go much faster?\n\nsystem.time({levels(y) &lt;- tolower(levels(y))})\n\n   user  system elapsed \n  0.018   0.000   0.019"
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors-can-be-confusing",
    "href": "slides/R/05-r-basics.html#factors-can-be-confusing",
    "title": "R Basics",
    "section": "Factors can be confusing",
    "text": "Factors can be confusing\n\nTry to make sense of this:\n\n\nx &lt;- factor(c(\"3\",\"2\",\"1\"), levels = c(\"3\",\"2\",\"1\"))\nas.numeric(x)\n\n[1] 1 2 3\n\nx[1]\n\n[1] 3\nLevels: 3 2 1\n\nlevels(x[1])\n\n[1] \"3\" \"2\" \"1\"\n\ntable(x[1])\n\n\n3 2 1 \n1 0 0"
  },
  {
    "objectID": "slides/R/05-r-basics.html#factors-can-be-confusing-1",
    "href": "slides/R/05-r-basics.html#factors-can-be-confusing-1",
    "title": "R Basics",
    "section": "Factors can be confusing",
    "text": "Factors can be confusing\n\nAvoid keeping extra levels with droplevels:\n\n\nz &lt;- x[1]\nz &lt;- droplevels(z)\n\n\nBut note what happens if we change to another level:\n\n\nz[1] &lt;- \"1\"\nz\n\n[1] &lt;NA&gt;\nLevels: 3"
  },
  {
    "objectID": "slides/R/05-r-basics.html#nas",
    "href": "slides/R/05-r-basics.html#nas",
    "title": "R Basics",
    "section": "NAs",
    "text": "NAs\n\nNA stands for not available.\nData analysts have to deal with NAs often."
  },
  {
    "objectID": "slides/R/05-r-basics.html#nas-1",
    "href": "slides/R/05-r-basics.html#nas-1",
    "title": "R Basics",
    "section": "NAs",
    "text": "NAs\n\ndslabs includes an example dataset with NAs\n\n\nlibrary(dslabs)\nna_example[1:20]\n\n [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2\n\n\n\nThe is.na function is key for dealing with NAs\n\n\nis.na(na_example[1])\n\n[1] FALSE\n\nis.na(na_example[17])\n\n[1] TRUE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(\"NA\")\n\n[1] FALSE"
  },
  {
    "objectID": "slides/R/05-r-basics.html#nas-2",
    "href": "slides/R/05-r-basics.html#nas-2",
    "title": "R Basics",
    "section": "NAs",
    "text": "NAs\n\nTechnically NA is a logical\n\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\nWhen used with ands and ors, NAs behaves like FALSE\n\n\nTRUE & NA\n\n[1] NA\n\nTRUE | NA\n\n[1] TRUE\n\n\n\nBut NA is not FALSE. Try this:\n\n\nif (NA) print(1) else print(0)"
  },
  {
    "objectID": "slides/R/05-r-basics.html#nans",
    "href": "slides/R/05-r-basics.html#nans",
    "title": "R Basics",
    "section": "NaNs",
    "text": "NaNs\n\nA related constant is NaN.\nUnlike NA, which is a logical, NaN is a number.\nIt is a numeric that is Not a Number.\nHere are some examples:\n\n\n0/0\n\n[1] NaN\n\nclass(0/0)\n\n[1] \"numeric\"\n\nsqrt(-1)\n\n[1] NaN\n\nlog(-1)\n\n[1] NaN"
  },
  {
    "objectID": "slides/R/05-r-basics.html#coercing",
    "href": "slides/R/05-r-basics.html#coercing",
    "title": "R Basics",
    "section": "Coercing",
    "text": "Coercing\n\nWhen you do something inconsistent with data types, R tries to figure out what you mean and change it accordingly.\nWe call this coercing.\nR does not return an error and in some cases does not return a warning either.\nThis can cause confusion and unnoticed errors.\nSo it’s important to understand how and when it happens."
  },
  {
    "objectID": "slides/R/05-r-basics.html#coercing-1",
    "href": "slides/R/05-r-basics.html#coercing-1",
    "title": "R Basics",
    "section": "Coercing",
    "text": "Coercing\n\nHere are some examples:\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1 + 1L)\n\n[1] \"double\"\n\nc(\"a\", 1, 2)\n\n[1] \"a\" \"1\" \"2\"\n\nTRUE + FALSE\n\n[1] 1\n\nfactor(\"a\") == \"a\"\n\n[1] TRUE\n\nidentical(factor(\"a\"), \"a\")\n\n[1] FALSE"
  },
  {
    "objectID": "slides/R/05-r-basics.html#coercing-2",
    "href": "slides/R/05-r-basics.html#coercing-2",
    "title": "R Basics",
    "section": "Coercing",
    "text": "Coercing\n\nWhen R can’t figure out how to coerce, rather an error it returns an NA:\n\n\nas.numeric(\"a\")\n\n[1] NA\n\n\n\nNote that including NAs in arithmetical operations usually returns an NA.\n\n\n1 + 2 + NA\n\n[1] NA"
  },
  {
    "objectID": "slides/R/05-r-basics.html#coercing-3",
    "href": "slides/R/05-r-basics.html#coercing-3",
    "title": "R Basics",
    "section": "Coercing",
    "text": "Coercing\n\nYou want to avoid automatic coercion and instead explicitly do it.\nMost coercion functions start with as.\nHere is an example.\n\n\nx &lt;- factor(c(\"a\",\"b\",\"b\",\"c\"))\nas.character(x)\n\n[1] \"a\" \"b\" \"b\" \"c\"\n\nas.numeric(x)\n\n[1] 1 2 2 3"
  },
  {
    "objectID": "slides/R/05-r-basics.html#coercing-4",
    "href": "slides/R/05-r-basics.html#coercing-4",
    "title": "R Basics",
    "section": "Coercing",
    "text": "Coercing\n\nMore examples:\n\n\nx &lt;- c(\"12323\", \"12,323\")\nas.numeric(x)\n\n[1] 12323    NA\n\nlibrary(readr)\nparse_guess(x)\n\n[1] 12323 12323"
  },
  {
    "objectID": "slides/R/05-r-basics.html#lists",
    "href": "slides/R/05-r-basics.html#lists",
    "title": "R Basics",
    "section": "Lists",
    "text": "Lists\n\nData frames are a type of list.\nLists permit components of different types and, unlike data frames, different lengths:\n\n\nx &lt;- list(name = \"John\", id = 112, grades = c(95, 87, 92))\n\n\nThe JSON format is best represented as list in R."
  },
  {
    "objectID": "slides/R/05-r-basics.html#lists-1",
    "href": "slides/R/05-r-basics.html#lists-1",
    "title": "R Basics",
    "section": "Lists",
    "text": "Lists\n\nYou can access components in different ways:\n\n\nx$name\n\n[1] \"John\"\n\nx[[1]]\n\n[1] \"John\"\n\nx[[\"name\"]]\n\n[1] \"John\""
  },
  {
    "objectID": "slides/R/05-r-basics.html#matrics",
    "href": "slides/R/05-r-basics.html#matrics",
    "title": "R Basics",
    "section": "Matrics",
    "text": "Matrics\n\nMatrices are another widely used data type.\nThey are similar to data frames except all entries need to be of the same type.\nWe will learn more about matrices in the High Dimensional data Analysis part of the class."
  },
  {
    "objectID": "slides/R/05-r-basics.html#functions",
    "href": "slides/R/05-r-basics.html#functions",
    "title": "R Basics",
    "section": "Functions",
    "text": "Functions\n\nYou can define your own function. The form is like this:\n\n\nf &lt;- function(x, y, z = 0){\n  ### do calculations with x, y, z to compute object\n  ## return(object)\n}"
  },
  {
    "objectID": "slides/R/05-r-basics.html#functions-1",
    "href": "slides/R/05-r-basics.html#functions-1",
    "title": "R Basics",
    "section": "Functions",
    "text": "Functions\n\nHere is an example of a function that sums \\(1,2,\\dots,n\\)\n\n\ns &lt;- function(n){\n   return(sum(1:n))\n}"
  },
  {
    "objectID": "slides/R/05-r-basics.html#lexical-scope",
    "href": "slides/R/05-r-basics.html#lexical-scope",
    "title": "R Basics",
    "section": "Lexical scope",
    "text": "Lexical scope\n\nStudy what happens here:\n\n\nf &lt;- function(x){\n  cat(\"y is\", y,\"\\n\")\n  y &lt;- x\n  cat(\"y is\", y,\"\\n\")\n  return(y)\n}\ny &lt;- 2\nf(3)\n\ny is 2 \ny is 3 \n\n\n[1] 3\n\ny &lt;- f(3)\n\ny is 2 \ny is 3 \n\ny\n\n[1] 3"
  },
  {
    "objectID": "slides/R/05-r-basics.html#namespaces",
    "href": "slides/R/05-r-basics.html#namespaces",
    "title": "R Basics",
    "section": "Namespaces",
    "text": "Namespaces\n\nLook at how this function changes by typing the following:\n\n\nfilter\nlibrary(dplyr)\nfilter"
  },
  {
    "objectID": "slides/R/05-r-basics.html#namespaces-1",
    "href": "slides/R/05-r-basics.html#namespaces-1",
    "title": "R Basics",
    "section": "Namespaces",
    "text": "Namespaces\n\nNote what R searches the Global Environment first.\nUse search to see other environments R searches.\nNote many prebuilt functions are in stats."
  },
  {
    "objectID": "slides/R/05-r-basics.html#namespaces-2",
    "href": "slides/R/05-r-basics.html#namespaces-2",
    "title": "R Basics",
    "section": "Namespaces",
    "text": "Namespaces\n\nYou can explicitly say which filter you want using namespaces:\n\n\nstats::filter\ndplyr::filter"
  },
  {
    "objectID": "slides/R/05-r-basics.html#namespaces-3",
    "href": "slides/R/05-r-basics.html#namespaces-3",
    "title": "R Basics",
    "section": "Namespaces",
    "text": "Namespaces\n\nRestart yoru R Consuole and study this example:\n\n\nlibrary(dslabs)\nexists(\"murders\")\n\n[1] TRUE\n\nmurders &lt;- murders\nmurders2 &lt;- murders\nrm(murders)\nexists(\"murders\")\n\n[1] TRUE\n\ndetach(\"package:dslabs\")\nexists(\"murders\")\n\n[1] FALSE\n\nexists(\"murders2\")\n\n[1] TRUE"
  },
  {
    "objectID": "slides/R/05-r-basics.html#object-oriented-programming",
    "href": "slides/R/05-r-basics.html#object-oriented-programming",
    "title": "R Basics",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\n\nR uses object oriented programming (OOP).\nIt uses two approaches referred to as S3 and S4, respectively.\nS3, the original approach, is more common.\nThe S4 approach is more similar to the conventions used by modern OOP languages."
  },
  {
    "objectID": "slides/R/05-r-basics.html#object-oriented-programming-1",
    "href": "slides/R/05-r-basics.html#object-oriented-programming-1",
    "title": "R Basics",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\n\nTime seriesNumeric\n\n\n\nplot(co2)\n\n\n\n\n\n\n\n\n\n\n\nplot(as.numeric(co2))"
  },
  {
    "objectID": "slides/R/05-r-basics.html#object-oriented-programming-2",
    "href": "slides/R/05-r-basics.html#object-oriented-programming-2",
    "title": "R Basics",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\n\nNote co2 is not numeric:\n\n\nclass(co2)\n\n[1] \"ts\"\n\n\n\nThe plots are different because plot behaves different with different classes."
  },
  {
    "objectID": "slides/R/05-r-basics.html#object-oriented-programming-3",
    "href": "slides/R/05-r-basics.html#object-oriented-programming-3",
    "title": "R Basics",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\n\nThe first plot actually calls the function\n\n\nplot.ts\n\n\nNotice all the plot functions that start with plot by typing plot. and then tab.\nThe function plot will call different functions depending on the class of the arguments."
  },
  {
    "objectID": "slides/R/05-r-basics.html#plots",
    "href": "slides/R/05-r-basics.html#plots",
    "title": "R Basics",
    "section": "Plots",
    "text": "Plots\n\nSoon we will learn how to use the ggplot2 package to make plots.\nR base does have functions for plotting though\nSome you should know about are:\n\nplot - mainly for making scatterplots.\nlines - add lines/curves to an existing plot.\nhist - to make a histogram.\nboxplot - makes boxplots.\nimage - uses color to represent entries in a matrix."
  },
  {
    "objectID": "slides/R/05-r-basics.html#plots-1",
    "href": "slides/R/05-r-basics.html#plots-1",
    "title": "R Basics",
    "section": "Plots",
    "text": "Plots\n\nAlthough, in general, we recommend using ggplot2, R base plots are often better for quick exploratory plots.\nFor example, to make a histogram of values in x simply type:\n\n\nhist(x)\n\n\nTo make a scatter plot of y versus x and then interpolate we type:\n\n\nplot(x,y)\nlines(x,y)"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models",
    "href": "slides/prob/21-inference-foundations.html#sampling-models",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nMany data generation procedures can be effectively modeled as draws from an urn.\nWe can model the process of polling likely voters as drawing 0s for one party and 1s for the other.\nEpidemiologist assume subjects in their studies are a random sample from the population of interest."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-1",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-1",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nIn general, the data related to a specific outcome can be modeled as a random sample from an urn containing the outcomes for the entire population of interest."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-2",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-2",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nSimilarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population.\nRandomized experiments can be modeled by draws from an urn, reflecting the way individuals are assigned into group; when getting assigned, individuals draw their group at random."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-3",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-3",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nSampling models are therefore ubiquitous in data science.\nCasino games offer a plethora of real-world cases in which sampling models are used to answer specific questions.\nWe will therefore start with these examples."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-4",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-4",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels.\nWe will assume that 1,000 people will play, and that the only game available on the roulette wheel is to bet on red or black.\nThe casino wants you to predict how much money they will make or lose.\nThey want a range of values and, in particular, they want to know what’s the chance of losing money."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-5",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-5",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nIf this probability is too high, they will decide against installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-6",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-6",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\nThis is a roullette:"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-7",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-7",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nLet’s start by constructing the urn.\nA roulette wheel has 18 red pockets, 18 black pockets and 2 green ones.\nSo playing a color in one game of roulette is equivalent to drawing from this urn:\n\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-8",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-8",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nThe 1,000 outcomes from 1,000 people playing are independent draws from this urn.\nIf red comes up, the gambler wins, and the casino loses a dollar, resulting random variable being -$1.\nOtherwise, the casino wins a dollar, and the random variable is $1."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-9",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-9",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nTo construct our random variable \\(S\\), we can use this code:\n\n\nn &lt;- 1000 \nx &lt;- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE) \nx[1:10] \n#&gt;  [1]  1 -1  1 -1  1  1 -1  1  1  1"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-10",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-10",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nBecause we know the proportions of 1s and -1s, we can generate the draws without defining color.\n\n\nx &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#sampling-models-11",
    "href": "slides/prob/21-inference-foundations.html#sampling-models-11",
    "title": "Foundations of Statistical Inference",
    "section": "Sampling models",
    "text": "Sampling models\n\nThis is a sampling model, as it models the random behavior through the sampling of draws from an urn.\nThe total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\n\n\nx &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19)) \ns &lt;- sum(x) \ns \n#&gt; [1] 44\n\n\nIf you rerun the code above, you see that \\(S\\) changes every time.\n\\(S\\) is a random variable."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distributions",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distributions",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distributions",
    "text": "The probability distributions\n\nThe probability distribution of a random variable informs us about the probability of the observed value falling in any given interval.\nFor example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\((-\\infty,0)\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nIf we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), we can answer any question about the probability of events defined by \\(S\\).\nWe call this \\(F\\) the random variable’s distribution function.\nProbability and Statistics classes dedicate much time to calculating or approximating these.\nWe can also estimate the distribution function for \\(S\\) using a Monte Carlo simulation."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-1",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-1",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nWith this code, we run the experiment of having 1,000 people repeatedly play roulette, specifically \\(B = 10,000\\) times:\n\n\nn &lt;- 1000 \nB &lt;- 10000 \nroulette_winnings &lt;- function(n){ \n  x &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19)) \n  sum(x) \n} \ns &lt;- replicate(B, roulette_winnings(n))"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-2",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-2",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nNow, we can ask the following: in our simulation, how often did we get sums less than or equal to a?\n\n\nmean(s &lt;= a)"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-3",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-3",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nThis will be a very good approximation of \\(F(a)\\).\nThis allows us to easily answer the casino’s question: How likely is it that we will lose money?\nIt is quite low:\n\n\nmean(s &lt; 0) \n#&gt; [1] 0.045"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-4",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-4",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-5",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-5",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nWe see that the distribution appears to be approximately normal.\nA QQ-plot will confirm that the normal approximation is close to a perfect approximation."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-6",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-6",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nRemeber, if the distribution is normal, all we need to define it are the average and the standard deviation (SD).\nSince we have the original values from which the distribution is created, we can easily compute these with mean(s) and sd(s).\nThe blue curve added to the histogram is a normal density with this average and standard deviation."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-probability-distribution-7",
    "href": "slides/prob/21-inference-foundations.html#the-probability-distribution-7",
    "title": "Foundations of Statistical Inference",
    "section": "The probability distribution",
    "text": "The probability distribution\n\nThis average and this standard deviation have special names; they are referred to as the expected value and standard error (SE) of the random variable \\(S\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nBefore we continue, let’s establish an important distinction and connection between the distribution of a list of numbers and a probability distribution.\nAny list of numbers \\(x_1,\\dots,x_n\\) has a distribution.\nIt does not have a probability distribution because they are not random."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-1",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-1",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nWe define \\(F(a)\\) as the function that indicates what proportion of the list is less than or equal to \\(a\\).\nGiven their usefulness as summaries when the distribution is approximately normal, we also define the average and standard deviation."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-2",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-2",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nThese are determined with a straightforward operation involving the vector containing the list of numbers, denoted as x:\n\n\nm &lt;- sum(x)/length(x) \ns &lt;- sqrt(sum((x - m)^2)/length(x))"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-3",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-3",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nA random variable \\(X\\) has a distribution function.\nTo define this, we do not need a list of numbers; it is a theoretical concept.\nWe define the distribution as the \\(F(a)\\) that answers the question: What is the probability that \\(X\\) is less than or equal to \\(a\\)?"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-4",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-4",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nHowever, if \\(X\\) is defined by drawing from an urn containing numbers, then there is a list: the list of numbers inside the urn.\nIn this case, the distribution of that list is the probability distribution of \\(X\\), and the average and standard deviation of that list are the expected value and standard error of the random variable."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-5",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-5",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nAnother way to think about it without involving an urn is by running a Monte Carlo simulation and generating a very large list of outcomes of \\(X\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-6",
    "href": "slides/prob/21-inference-foundations.html#distributions-versus-probability-distributions-6",
    "title": "Foundations of Statistical Inference",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\n\nThese outcomes form a list of numbers, and the distribution of this list will be a very good approximation of the probability distribution of \\(X\\).\nThe longer the list, the better the approximation.\nThe average and standard deviation of this list will approximate the expected value and standard error of the random variable."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#notation-for-random-variables",
    "href": "slides/prob/21-inference-foundations.html#notation-for-random-variables",
    "title": "Foundations of Statistical Inference",
    "section": "Notation for random variables",
    "text": "Notation for random variables\n\nIn statistical textbooks, upper case letters denote random variables, and we will adhere to this convention.\nLower case letters are used for observed values.\nYou will see some notation that include both.\nFor example, you will see events defined as \\(X \\leq x\\).\nHere \\(X\\) is a random variable and \\(x\\) is an arbitrary value and not random."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#notation-for-random-variables-1",
    "href": "slides/prob/21-inference-foundations.html#notation-for-random-variables-1",
    "title": "Foundations of Statistical Inference",
    "section": "Notation for random variables",
    "text": "Notation for random variables\n\nSo, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see: 1, 2, 3, 4, 5, or 6.\nIn this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\).\nWe can discuss what we expect \\(X\\) to be, what values are probable, but we can’t discuss what value \\(X\\) is."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#notation-for-random-variables-2",
    "href": "slides/prob/21-inference-foundations.html#notation-for-random-variables-2",
    "title": "Foundations of Statistical Inference",
    "section": "Notation for random variables",
    "text": "Notation for random variables\n\nOnce we have data, we do see a realization of \\(X\\).\nTherefore, data analysts often speak of what could have been after observing what actually happened."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nWe will now review the mathematical theory that allows us to approximate the probability distributions for the sum of draws.\nThe same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion, which we will need to understand how polls work.\nThe first important concept to learn is the expected value."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-1",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-1",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nIn statistics books, it is common to represent the expected value of the random variable \\(X\\) with the letter \\(\\mbox{E}\\) like this:\n\n\\[\\mbox{E}[X]\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-2",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-2",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nA random variable will vary around its expected value in a manner that if you take the average of many, many draws, the average will approximate the expected value.\nThis approximation improves as you take more draws, making the expected value a useful quantity to compute."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-3",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-3",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\), the expected value is defined as:\n\n\\[\n\\mbox{E}[X] = \\sum_{i=1}^n x_i \\,\\mbox{Pr}(X = x_i)\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-4",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-4",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nNote that in the case that we are picking values from an urn, and each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected, the above equation is simply the average of the \\(x_i\\)s.\n\n\\[\n\\mbox{E}[X] = \\frac{1}{n}\\sum_{i=1}^n x_i  \n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-5",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-5",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nIf \\(X\\) is a continuous random variable with a range of values \\(a\\) to \\(b\\) and a probability density function \\(f(x)\\), this sum transforms into an integral:\n\n\\[\n\\mbox{E}[X] = \\int_a^b x f(x)\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-6",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-6",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nIn the urn used to model betting on red in roulette, we have 20 one-dollar bills and 18 negative one-dollar bills, so the expected value is:\n\n\\[\n\\mbox{E}[X] = (20 + -18)/38\n\\]\n\nwhich is about 5 cents."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-7",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-7",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nYou might consider it a bit counterintuitive to say that \\(X\\) varies around 0.05 when it only takes the values 1 and -1.\nTo make sense of the expected value in this context is by realizing that, if we play the game over and over, the casino wins, on average, 5 cents per game."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-8",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-8",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nA Monte Carlo simulation confirms this:\n\n\nB &lt;- 10^6 \nx &lt;- sample(c(-1, 1), B, replace = TRUE, prob = c(9/19, 10/19)) \nmean(x) \n#&gt; [1] 0.0522"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-9",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-9",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\n\\[\\mbox{E}[X] = ap + b(1-p)\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-10",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-10",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nTo confirm this, observe that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s, and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-11",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-11",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nThe reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum.\nThis, in turn, is useful for describing the distribution of averages and proportions."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-12",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-12",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nThe first useful fact is that the expected value of the sum of the draws is the number of draws \\(\\times\\) the average of the numbers in the urn.\nTherefore, if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-13",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-13",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nHowever, this is an expected value.\nHow different can one observation be from the expected value? The casino really needs to know this.\nWhat is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels.\nStatistical theory once again answers this question."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-14",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-14",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nThe standard error (SE) gives us an idea of the size of the variation around the expected value.\nIn statistics books, it’s common to use:\n\n\\[\\mbox{SE}[X] = \\sqrt{\\mbox{Var}[X]}\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-15",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-15",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nto denote the standard error of a random variable.\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\), the standard error is defined as:\n\n\\[\n\\mbox{SE}[X] = \\sqrt{\\sum_{i=1}^n \\left(x_i - E[X]\\right)^2 \\,\\mbox{Pr}(X = x_i)},\n\\]\n\nwhich you can think of as the expected average distance of \\(X\\) from the expected value."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-16",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-16",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nNote that in the case that we are picking values from an un urn where each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected, the above equation is simply the standard deviation of of the \\(x_i\\)s.\n\n\\[\n\\mbox{SE}[X] = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2} \\mbox{ with } \\bar{x} =  \\frac{1}{n}\\sum_{i=1}^n x_i  \n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-17",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-17",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nIf \\(X\\) is a continuous random variable, with range of values \\(a\\) to \\(b\\) and probability density function \\(f(x)\\), this sum turns into an integral:\n\n\\[\n\\mbox{SE}[X] = \\sqrt{\\int_a^b \\left(x-\\mbox{E}[X]\\right)^2 f(x)\\,\\mathrm{d}x}\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-18",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-18",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nUsing the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\n\\[| b - a |\\sqrt{p(1-p)}.\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-19",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-19",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nSo in our roulette example, the standard deviation of the values inside the urn is:\n\n\\[\n| 1 - (-1) | \\sqrt{10/19 \\times 9/19}\n\\]\nor:\n\n2*sqrt(90)/19 \n#&gt; [1] 0.999"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-20",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-20",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nSince one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a SE of about 1.\nThis makes sense since we obtain either 1 or -1, with 1 slightly favored over -1."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-21",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-21",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nA widely used mathematical result is that if our draws are independent, then the standard error of the sum is given by the equation:\n\n\\[\n\\sqrt{\\mbox{number of draws}} \\times \\mbox{ SD of the numbers in the urn}\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-22",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-22",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nUsing this formula, the sum of 1,000 people playing has standard error of about $32:\n\n\nn &lt;- 1000 \nsqrt(n)*2*sqrt(90)/19 \n#&gt; [1] 31.6"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-23",
    "href": "slides/prob/21-inference-foundations.html#the-expected-value-and-se-23",
    "title": "Foundations of Statistical Inference",
    "section": "The expected value and SE",
    "text": "The expected value and SE\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32.\nIt therefore seems like a safe bet to install more roulette wheels.\nBut we still haven’t answered the question: How likely is the casino to lose money? The CLT will help in this regard."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal.\nGiven that sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-1",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-1",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation.\nWe also know that the same applies to probability distributions."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-2",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-2",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nIf a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-3",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-3",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nWe previously ran this Monte Carlo simulation:\n\n\nn &lt;- 1000 \nB &lt;- 10000 \nroulette_winnings &lt;- function(n){ \n  x &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19)) \n  sum(x) \n} \ns &lt;- replicate(B, roulette_winnings(n))"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-4",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-4",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution.\nUsing the formulas above, we know that the expected value and standard error are:\n\n\nn * (20 - 18)/38  \n#&gt; [1] 52.6\nsqrt(n)*2*sqrt(90)/19  \n#&gt; [1] 31.6"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-5",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-5",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\n\nmean(s) \n#&gt; [1] 52.6\nsd(s) \n#&gt; [1] 31.5"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#central-limit-theorem-6",
    "href": "slides/prob/21-inference-foundations.html#central-limit-theorem-6",
    "title": "Foundations of Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\n\nmu &lt;- n*(20 - 18)/38 \nse &lt;- sqrt(n)*2*sqrt(90)/19  \npnorm(0, mu, se) \n#&gt; [1] 0.0478\n\n\nwhich is also in very good agreement with our Monte Carlo result:\n\n\nmean(s &lt; 0) \n#&gt; [1] 0.0442"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt",
    "href": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt",
    "title": "Foundations of Statistical Inference",
    "section": "How large is large in the CLT?",
    "text": "How large is large in the CLT?\n\nThe CLT works when the number of draws is large, but “large” is a relative term.\nIn many circumstances, as few as 30 draws is enough to make the CLT useful.\nIn some specific instances, as few as 10 is enough.\nHowever, these should not be considered general rules.\nNote that when the probability of success is very small, much larger sample sizes are needed."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-1",
    "href": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-1",
    "title": "Foundations of Statistical Inference",
    "section": "How large is large in the CLT?",
    "text": "How large is large in the CLT?\n\nBy way of illustration, let’s consider the lottery.\nIn the lottery, the chances of winning are less than 1 in a million.\nThousands of people play so the number of draws is very large.\nYet the number of winners, the sum of the draws, range between 0 and 4.\nThis sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-2",
    "href": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-2",
    "title": "Foundations of Statistical Inference",
    "section": "How large is large in the CLT?",
    "text": "How large is large in the CLT?\n\nThis is generally true when the probability of a success is very low.\nIn these cases, the Poisson distribution is more appropriate.\nYou can explore the properties of the Poisson distribution using dpois and ppois.\nYou can generate random variables following this distribution with rpois.\nHowever, we won’t cover the theory here."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-3",
    "href": "slides/prob/21-inference-foundations.html#how-large-is-large-in-the-clt-3",
    "title": "Foundations of Statistical Inference",
    "section": "How large is large in the CLT?",
    "text": "How large is large in the CLT?\n\nYou can learn about the Poisson distribution in any probability textbook and even Wikipedia"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#statistical-properties-of-averages",
    "href": "slides/prob/21-inference-foundations.html#statistical-properties-of-averages",
    "title": "Foundations of Statistical Inference",
    "section": "Statistical properties of averages",
    "text": "Statistical properties of averages\n\nThere are several useful mathematical results that we used above and often employ when working with data.\nWe list them below."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-1",
    "href": "slides/prob/21-inference-foundations.html#property-1",
    "title": "Foundations of Statistical Inference",
    "section": "Property 1",
    "text": "Property 1\n\nThe expected value of the sum of random variables is the sum of each random variable’s expected value.\nWe can write it like this:\n\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n] =  \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n]\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-1-1",
    "href": "slides/prob/21-inference-foundations.html#property-1-1",
    "title": "Foundations of Statistical Inference",
    "section": "Property 1",
    "text": "Property 1\n\nIf \\(X\\) represents independent draws from the urn, then they all have the same expected value.\nLet’s denote the expected value with \\(\\mu\\) and rewrite the equation as:\n\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n]=  n\\mu\n\\]\n\nwhich is another way of writing the result we show above for the sum of draws."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-2",
    "href": "slides/prob/21-inference-foundations.html#property-2",
    "title": "Foundations of Statistical Inference",
    "section": "Property 2",
    "text": "Property 2\n\nThe expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable.\nThis is easier to explain with symbols:\n\n\\[\n\\mbox{E}[aX] =  a\\times\\mbox{E}[X]\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-2-1",
    "href": "slides/prob/21-inference-foundations.html#property-2-1",
    "title": "Foundations of Statistical Inference",
    "section": "Property 2",
    "text": "Property 2\n\nTo understand why this is intuitive, consider changing units.\nIf we change the units of a random variable, such as from dollars to cents, the expectation should change in the same way."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-2-2",
    "href": "slides/prob/21-inference-foundations.html#property-2-2",
    "title": "Foundations of Statistical Inference",
    "section": "Property 2",
    "text": "Property 2\n\nA consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, denoted as \\(\\mu\\) again:\n\n\\[\n\\mbox{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu  \n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-3",
    "href": "slides/prob/21-inference-foundations.html#property-3",
    "title": "Foundations of Statistical Inference",
    "section": "Property 3",
    "text": "Property 3\n\nThe square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable.\nThis one is easier to understand in math form:\n\n\\[\n\\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2  }\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-3-1",
    "href": "slides/prob/21-inference-foundations.html#property-3-1",
    "title": "Foundations of Statistical Inference",
    "section": "Property 3",
    "text": "Property 3\n\nThe square of the standard error is referred to as the variance in statistical textbooks.\nNote that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-4",
    "href": "slides/prob/21-inference-foundations.html#property-4",
    "title": "Foundations of Statistical Inference",
    "section": "Property 4",
    "text": "Property 4\n\nThe standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error.\nAs with the expectation:\n\n\\[\n\\mbox{SE}[aX] =  a \\times \\mbox{SE}[X]\n\\]\n\nTo see why this is intuitive, again think of units."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-4-1",
    "href": "slides/prob/21-inference-foundations.html#property-4-1",
    "title": "Foundations of Statistical Inference",
    "section": "Property 4",
    "text": "Property 4\n\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\n\\[\n\\begin{aligned}\n\\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\\n&= \\sqrt{n\\sigma^2}/n\\\\\n&= \\sigma / \\sqrt{n}     \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#property-5",
    "href": "slides/prob/21-inference-foundations.html#property-5",
    "title": "Foundations of Statistical Inference",
    "section": "Property 5",
    "text": "Property 5\n\nIf \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable.\nAll we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#notation",
    "href": "slides/prob/21-inference-foundations.html#notation",
    "title": "Foundations of Statistical Inference",
    "section": "Notation",
    "text": "Notation\n\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively.\nThis is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value.\nSimilarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important",
    "href": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important",
    "title": "Foundations of Statistical Inference",
    "section": "The assumption of independence is important",
    "text": "The assumption of independence is important\n\nThe given equation reveals crucial insights for practical scenarios.\nSpecifically, it suggests that the standard error can be minimized by increasing the sample size, \\(n\\), and we can quantify this reduction.\nHowever, this principle holds true only when the variables \\(X_1, X_2, \\dots, X_n\\) are independent.\nIf they are not, the estimated standard error can be significantly off."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important-1",
    "href": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important-1",
    "title": "Foundations of Statistical Inference",
    "section": "The assumption of independence is important",
    "text": "The assumption of independence is important\n\nWe later introduce the concept of correlation, which quantifies the degree to which variables are interdependent.\nIf the correlation coefficient among the \\(X\\) variables is \\(\\rho\\), the standard error of their average is:\n\n\\[\n\\mbox{SE}\\left(\\bar{X}\\right) = \\sigma \\sqrt{\\frac{1 + (n-1) \\rho}{n}}\n\\]"
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important-2",
    "href": "slides/prob/21-inference-foundations.html#the-assumption-of-independence-is-important-2",
    "title": "Foundations of Statistical Inference",
    "section": "The assumption of independence is important",
    "text": "The assumption of independence is important\n\nThe key observation here is that as \\(\\rho\\) approaches its upper limit of 1, the standard error increases.\nNotably, in the situation where \\(\\rho = 1\\), the standard error, \\(\\mbox{SE}(\\bar{X})\\), equals \\(\\sigma\\), and it becomes unaffected by the sample size \\(n\\)."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#law-of-large-numbers",
    "href": "slides/prob/21-inference-foundations.html#law-of-large-numbers",
    "title": "Foundations of Statistical Inference",
    "section": "Law of large numbers",
    "text": "Law of large numbers\n\nAn important implication of result 4 above is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger.\nWhen \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn.\nThis is known in statistical textbooks as the law of large numbers or the law of averages."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages",
    "href": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages",
    "title": "Foundations of Statistical Inference",
    "section": "Misinterpretation of the law of averages",
    "text": "Misinterpretation of the law of averages\n\nThe law of averages is sometimes misinterpreted.\nFor example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-1",
    "href": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-1",
    "title": "Foundations of Statistical Inference",
    "section": "Misinterpretation of the law of averages",
    "text": "Misinterpretation of the law of averages\n\nA similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row.\nYet these events are independent so the chance of a coin landing heads is 50%, regardless of the previous 5."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-2",
    "href": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-2",
    "title": "Foundations of Statistical Inference",
    "section": "Misinterpretation of the law of averages",
    "text": "Misinterpretation of the law of averages\n\nThe same principle applies to the roulette outcome.\nThe law of averages applies only when the number of draws is very large and not in small samples."
  },
  {
    "objectID": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-3",
    "href": "slides/prob/21-inference-foundations.html#misinterpretation-of-the-law-of-averages-3",
    "title": "Foundations of Statistical Inference",
    "section": "Misinterpretation of the law of averages",
    "text": "Misinterpretation of the law of averages\n\nAfter a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.\nAnother funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation\n\nThe next few slides are examples of infogrpahics that don’t follow data visualization principles.\nWe use them as motivation.\nSeveral of the examples come from here: https://venngage.com/blog/bad-infographics/"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-1",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-1",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-2",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-2",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-3",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-3",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-4",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-4",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-5",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-5",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-6",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-6",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#some-motivation-7",
    "href": "slides/dataviz/17-dataviz-principles.html#some-motivation-7",
    "title": "Data Visualization Principles",
    "section": "Some motivation",
    "text": "Some motivation"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles",
    "href": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles",
    "title": "Data Visualization Principles",
    "section": "Data visualization principles",
    "text": "Data visualization principles\n\nWe provide some general principles we can use as a guide for effective data visualization.\nMuch of this section is based on a talk by Karl Broman titled Creating Effective Figures and Tables and includes some of the figures which were made with code that Karl makes available on his GitHub repository, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-1",
    "href": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-1",
    "title": "Data Visualization Principles",
    "section": "Data visualization principles",
    "text": "Data visualization principles\n\nFollowing Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles.\nWe compare and contrast plots that follow these principles to those that don’t."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-2",
    "href": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-2",
    "title": "Data Visualization Principles",
    "section": "Data visualization principles",
    "text": "Data visualization principles\n\nThe principles are mostly based on research related to how humans detect patterns and make visual comparisons.\nThe preferred approaches are those that best fit the way our brains process visual information.\nWhen deciding on a visualization approach, it is also important to keep our goal in mind."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-3",
    "href": "slides/dataviz/17-dataviz-principles.html#data-visualization-principles-3",
    "title": "Data Visualization Principles",
    "section": "Data visualization principles",
    "text": "Data visualization principles\nWe may be comparing a\n\nViewable number of quantities.\nDescribing distributions for categories or numeric values.\nComparing the data from two groups.\nDescribing the relationship between two variables."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues",
    "title": "Data Visualization Principles",
    "section": "Encoding data using visual cues",
    "text": "Encoding data using visual cues\nWe start by describing some principles for visualy encoding numerical values:\n\naligned lengths\nposition\nangles\narea\nbrightness\ncolor hue"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-1",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-1",
    "title": "Data Visualization Principles",
    "section": "Encoding data using visual cues",
    "text": "Encoding data using visual cues\nExample:\n\nSuppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015.\nFor each year, we are simply comparing five quantities – the five percentages for Opera, Safari, Firefox,IE, and Chrome."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-2",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-2",
    "title": "Data Visualization Principles",
    "section": "Encoding data using visual cues",
    "text": "Encoding data using visual cues\nA widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-3",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-3",
    "title": "Data Visualization Principles",
    "section": "Encoding data using visual cues",
    "text": "Encoding data using visual cues\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents.\nThis turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-4",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-data-using-visual-cues-4",
    "title": "Data Visualization Principles",
    "section": "Encoding data using visual cues",
    "text": "Encoding data using visual cues\nThe donut chart uses only area:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#compare-2000-to-2015",
    "href": "slides/dataviz/17-dataviz-principles.html#compare-2000-to-2015",
    "title": "Data Visualization Principles",
    "section": "Compare 2000 to 2015",
    "text": "Compare 2000 to 2015\n\nCan you determine the actual percentages and rank the browsers’ popularity?\nCan you see how they changed from 2000 to 2015?"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#show-the-numbers",
    "href": "slides/dataviz/17-dataviz-principles.html#show-the-numbers",
    "title": "Data Visualization Principles",
    "section": "Show the numbers",
    "text": "Show the numbers\nA better approach is to simply show the numbers. It is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n\nBrowser\n2000\n2015\n\n\n\n\nOpera\n3\n2\n\n\nSafari\n21\n22\n\n\nFirefox\n23\n21\n\n\nChrome\n26\n29\n\n\nIE\n28\n27"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#barplots",
    "href": "slides/dataviz/17-dataviz-principles.html#barplots",
    "title": "Data Visualization Principles",
    "section": "Barplots",
    "text": "Barplots\nLength is the best visual cue:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#if-foreced-to-make-a-pie-chart",
    "href": "slides/dataviz/17-dataviz-principles.html#if-foreced-to-make-a-pie-chart",
    "title": "Data Visualization Principles",
    "section": "If foreced to make a pie chart",
    "text": "If foreced to make a pie chart\nLabel each pie slice with its respective percentage so viewers do not have to infer them:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\n\nWhen using barplots, it is misinformative not to start the bars at 0.\nThis is because, by using a barplot, we are implying the length is proportional to the quantities being displayed.\nBy avoiding 0, relatively small differences can be made to look much bigger than they actually are.\nThis approach is often used by politicians or media organizations trying to exaggerate a difference."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-1",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-1",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nBelow is an illustrative example used by Peter Aldhous in this lecture.\n\n\n\n\n\n(Source: Fox News, via Media Matters.)"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-2",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-2",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nHere is the correct plot:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-3",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-3",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nAnother examples:\n\n\n\n\n\n(Source: Fox News, via Flowing Data.)"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-4",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-4",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nAnd here is the correct plot:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-5",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-5",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nOne more example:\n\n\n\n\n\n(Source: Venezolana de Televisión via Pakistan Today and Diego Mariano.)"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-6",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-6",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\nHere is the appropriate plot:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-7",
    "href": "slides/dataviz/17-dataviz-principles.html#know-when-to-include-0-7",
    "title": "Data Visualization Principles",
    "section": "Know when to include 0",
    "text": "Know when to include 0\n\nWhen using position rather than length, it is not necessary to include 0.\nIn particularly when comparing between to within groups variability."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities",
    "href": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities",
    "title": "Data Visualization Principles",
    "section": "Do not distort quantities",
    "text": "Do not distort quantities\nPresident Obama used the following chart to compare the US GDP to the GDP of four competing nations:\n\n\n\n\n\n(Source: The 2011 State of the Union Address)"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities-1",
    "href": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities-1",
    "title": "Data Visualization Principles",
    "section": "Do not distort quantities",
    "text": "Do not distort quantities\nHere is comparison of using radius versus area:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities-2",
    "href": "slides/dataviz/17-dataviz-principles.html#do-not-distort-quantities-2",
    "title": "Data Visualization Principles",
    "section": "Do not distort quantities",
    "text": "Do not distort quantities\n\nggplot2 defaults to using area rather than radius.\nOf course, in this case, we really should be using length:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "href": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "title": "Data Visualization Principles",
    "section": "Order categories by a meaningful value",
    "text": "Order categories by a meaningful value\n\nWhen one of the axes is used to show categories the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings.\nIf they are defined by factors, they are ordered by the factor levels.\nWe rarely want to use alphabetical order.\nInstead, we should order by a meaningful quantity."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value-1",
    "href": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value-1",
    "title": "Data Visualization Principles",
    "section": "Order categories by a meaningful value",
    "text": "Order categories by a meaningful value\nNote that the plot on the right is more informative:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value-2",
    "href": "slides/dataviz/17-dataviz-principles.html#order-categories-by-a-meaningful-value-2",
    "title": "Data Visualization Principles",
    "section": "Order categories by a meaningful value",
    "text": "Order categories by a meaningful value\nHere is another example:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#show-the-data",
    "href": "slides/dataviz/17-dataviz-principles.html#show-the-data",
    "title": "Data Visualization Principles",
    "section": "Show the data",
    "text": "Show the data\n\nWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nSuppose we want to describe height data to an extra-terrestrial."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#show-the-data-1",
    "href": "slides/dataviz/17-dataviz-principles.html#show-the-data-1",
    "title": "Data Visualization Principles",
    "section": "Show the data",
    "text": "Show the data\nA commonly used plot, popularized by Microsoft Excel, is a barplot like this:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#show-the-data-2",
    "href": "slides/dataviz/17-dataviz-principles.html#show-the-data-2",
    "title": "Data Visualization Principles",
    "section": "Show the data",
    "text": "Show the data"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#show-the-data-3",
    "href": "slides/dataviz/17-dataviz-principles.html#show-the-data-3",
    "title": "Data Visualization Principles",
    "section": "Show the data",
    "text": "Show the data\nUse jitter to avoid over-plotting"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#histograms",
    "href": "slides/dataviz/17-dataviz-principles.html#histograms",
    "title": "Data Visualization Principles",
    "section": "Histograms",
    "text": "Histograms\nSince there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#ease-comparisons",
    "href": "slides/dataviz/17-dataviz-principles.html#ease-comparisons",
    "title": "Data Visualization Principles",
    "section": "Ease comparisons",
    "text": "Ease comparisons\n\nUse common axes\nIf horizontal comparison, stack graphs vertically\nIf vertical comparison, stack graphs horizontally"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#stack-vertically",
    "href": "slides/dataviz/17-dataviz-principles.html#stack-vertically",
    "title": "Data Visualization Principles",
    "section": "Stack vertically",
    "text": "Stack vertically"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#same-axis",
    "href": "slides/dataviz/17-dataviz-principles.html#same-axis",
    "title": "Data Visualization Principles",
    "section": "Same axis",
    "text": "Same axis"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#boxplot-is-a-vertical",
    "href": "slides/dataviz/17-dataviz-principles.html#boxplot-is-a-vertical",
    "title": "Data Visualization Principles",
    "section": "Boxplot is a vertical",
    "text": "Boxplot is a vertical\nStack horizontally"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#contrast-and-compare",
    "href": "slides/dataviz/17-dataviz-principles.html#contrast-and-compare",
    "title": "Data Visualization Principles",
    "section": "Contrast and compare",
    "text": "Contrast and compare"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#consider-transformations",
    "href": "slides/dataviz/17-dataviz-principles.html#consider-transformations",
    "title": "Data Visualization Principles",
    "section": "Consider transformations",
    "text": "Consider transformations\nHere is a terrible plot comparing population across continents"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#two-countries-drive-average",
    "href": "slides/dataviz/17-dataviz-principles.html#two-countries-drive-average",
    "title": "Data Visualization Principles",
    "section": "Two countries drive average",
    "text": "Two countries drive average"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#transformations",
    "href": "slides/dataviz/17-dataviz-principles.html#transformations",
    "title": "Data Visualization Principles",
    "section": "Transformations",
    "text": "Transformations\nHere a log transformation provides a much more informative plot:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent",
    "href": "slides/dataviz/17-dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent",
    "title": "Data Visualization Principles",
    "section": "Visual cues to be compared should be adjacent",
    "text": "Visual cues to be compared should be adjacent\nNote that it is hard to compare 1970 to 2020 by country:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent-1",
    "href": "slides/dataviz/17-dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent-1",
    "title": "Data Visualization Principles",
    "section": "Visual cues to be compared should be adjacent",
    "text": "Visual cues to be compared should be adjacent\nMuch easier if they are adjacent"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#use-color",
    "href": "slides/dataviz/17-dataviz-principles.html#use-color",
    "title": "Data Visualization Principles",
    "section": "Use color",
    "text": "Use color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#think-of-the-color-blind",
    "href": "slides/dataviz/17-dataviz-principles.html#think-of-the-color-blind",
    "title": "Data Visualization Principles",
    "section": "Think of the color blind",
    "text": "Think of the color blind\n\nApproximately 1 in 12 men (8%) and 1 in 200 women (0.5%) worldwide are color blind.\nThe most common type of color blindness is red-green color blindness, which affects around 99% of all color blind individuals.\nThe prevalence of blue-yellow color blindness and total color blindness (achromatopsia) is much lower.\nAn example of how we can use a color blind friendly palette is described here."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#think-of-the-color-blind-1",
    "href": "slides/dataviz/17-dataviz-principles.html#think-of-the-color-blind-1",
    "title": "Data Visualization Principles",
    "section": "Think of the color blind",
    "text": "Think of the color blind\n\nExample of color-blind-friendly color palette:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#plots-for-two-variables",
    "href": "slides/dataviz/17-dataviz-principles.html#plots-for-two-variables",
    "title": "Data Visualization Principles",
    "section": "Plots for two variables",
    "text": "Plots for two variables\nIn general, you should use scatterplots to visualize the relationship between two variables.\nHowever, there are some exceptions.\n\nWe describe two alternative plots here:\n\nslope chart\nBland-Altman plot"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#slope-charts",
    "href": "slides/dataviz/17-dataviz-principles.html#slope-charts",
    "title": "Data Visualization Principles",
    "section": "Slope charts",
    "text": "Slope charts\nSlope charts adds angle as a visual cue, useful when comparing two groups and each element across two variables, such as years."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#scatterplot-version",
    "href": "slides/dataviz/17-dataviz-principles.html#scatterplot-version",
    "title": "Data Visualization Principles",
    "section": "Scatterplot version",
    "text": "Scatterplot version"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#bland-altman-plot",
    "href": "slides/dataviz/17-dataviz-principles.html#bland-altman-plot",
    "title": "Data Visualization Principles",
    "section": "Bland-Altman plot",
    "text": "Bland-Altman plot\nShows difference in the y-axis and average on the x-axis."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-a-third-variable",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-a-third-variable",
    "title": "Data Visualization Principles",
    "section": "Encoding a third variable",
    "text": "Encoding a third variable\nWe can use\n\ndifferent colors or shapes for categoris\nareas, brightness or hue for continuous values"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#encoding-a-third-variable-1",
    "href": "slides/dataviz/17-dataviz-principles.html#encoding-a-third-variable-1",
    "title": "Data Visualization Principles",
    "section": "Encoding a third variable",
    "text": "Encoding a third variable\nWe encode OPEC membership, region, and population."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#point-shapes-available-in-r",
    "href": "slides/dataviz/17-dataviz-principles.html#point-shapes-available-in-r",
    "title": "Data Visualization Principles",
    "section": "Point shapes available in R",
    "text": "Point shapes available in R"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#using-intensity-or-hue",
    "href": "slides/dataviz/17-dataviz-principles.html#using-intensity-or-hue",
    "title": "Data Visualization Principles",
    "section": "Using intensity or hue",
    "text": "Using intensity or hue\nWhen selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#sequential-colors",
    "href": "slides/dataviz/17-dataviz-principles.html#sequential-colors",
    "title": "Data Visualization Principles",
    "section": "Sequential colors",
    "text": "Sequential colors\nSequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#diverging-colors",
    "href": "slides/dataviz/17-dataviz-principles.html#diverging-colors",
    "title": "Data Visualization Principles",
    "section": "Diverging colors",
    "text": "Diverging colors\nDiverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\nThe figure below, taken from the scientific literature, shows three variables: dose, drug type and survival:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-1",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-1",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\n\nHumans are not good at seeing in three dimensions and our limitation is even worse with regard to pseudo-three-dimensions."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-2",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-2",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\n\nWhen does the purple ribbon intersects the red?"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-3",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-3",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\nColor is enough to represent the categorical variable:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-4",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-4",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. We show two examples:"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-5",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-pseudo-3d-plots-5",
    "title": "Data Visualization Principles",
    "section": "Avoid pseudo-3D plots",
    "text": "Avoid pseudo-3D plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Images courtesy of Karl Broman)"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits",
    "title": "Data Visualization Principles",
    "section": "Avoid too many significant digits",
    "text": "Avoid too many significant digits\n\nBy default, statistical software like R returns many significant digits.\nThe default behavior in R is to show 7 significant digits.\nThat many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-1",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-1",
    "title": "Data Visualization Principles",
    "section": "Avoid too many significant digits",
    "text": "Avoid too many significant digits\nAs an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-2",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-2",
    "title": "Data Visualization Principles",
    "section": "Avoid too many significant digits",
    "text": "Avoid too many significant digits\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates.\n\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-3",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-3",
    "title": "Data Visualization Principles",
    "section": "Avoid too many significant digits",
    "text": "Avoid too many significant digits\n\nIn this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-4",
    "href": "slides/dataviz/17-dataviz-principles.html#avoid-too-many-significant-digits-4",
    "title": "Data Visualization Principles",
    "section": "Avoid too many significant digits",
    "text": "Avoid too many significant digits\nUseful ways to change the number of significant digits or to round numbers are\n\nsignif\nround\n\nYou can define the number of significant digits globally by setting options like this: options(digits = 3)."
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#values-compared-in-columns",
    "href": "slides/dataviz/17-dataviz-principles.html#values-compared-in-columns",
    "title": "Data Visualization Principles",
    "section": "Values compared in columns",
    "text": "Values compared in columns\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Compare these two presentations:\n\n\n\n\n\nstate\ndisease\n1940\n1950\n1960\n1970\n1980\n\n\n\n\nCalifornia\nMeasles\n37.9\n13.9\n14.1\n1\n0.4\n\n\nCalifornia\nPertussis\n18.3\n4.7\nNA\nNA\n0.1\n\n\nCalifornia\nPolio\n0.8\n2.0\n0.3\nNA\nNA"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#values-compared-in-columns-1",
    "href": "slides/dataviz/17-dataviz-principles.html#values-compared-in-columns-1",
    "title": "Data Visualization Principles",
    "section": "Values compared in columns",
    "text": "Values compared in columns\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Compare these two presentations:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA"
  },
  {
    "objectID": "slides/dataviz/17-dataviz-principles.html#know-your-audience",
    "href": "slides/dataviz/17-dataviz-principles.html#know-your-audience",
    "title": "Data Visualization Principles",
    "section": "Know your audience",
    "text": "Know your audience\nGraphs can be used for\n\nour own exploratory data analysis,\nto convey a message to experts, or\nto help tell a story to a general audience.\n\nMake sure that the intended audience understands each element of the plot."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#data-visualization-in-practice",
    "href": "slides/dataviz/19-dataviz-in-practice.html#data-visualization-in-practice",
    "title": "Dataviz In Practice",
    "section": "Data visualization in practice",
    "text": "Data visualization in practice\n\nIn this chapter, we will demonstrate how relatively simple ggplot2 code can create insightful and aesthetically pleasing plots.\nAs motivation we will create plots that help us better understand trends in world health and economics.\nAs we go through our case study, we will describe relevant general data visualization principles and learn concepts such as faceting, time series plots, transformations, and ridge plots."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nHans Rosling was the co-founder of the Gapminder Foundation, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world.\nThe organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-1",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nAs stated in the Gapminder Foundation’s website:\n\n\nJournalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-2",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nThis talk is based on two talks by Hans Rosling, one on education and The Best Stats You’ve Ever Seen\nWe use data to attempt to answer the following two questions:\n\nIs it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?\nHas income inequality across countries worsened during the last 40 years?"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-3",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nTo answer these questions, we will be using the gapminder dataset provided in dslabs.\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \nds_theme_set()\noptions(digits = 3)\ngapminder |&gt; as_tibble() |&gt; head()\n\n# A tibble: 6 × 9\n  country    year infant_mortality life_expectancy fertility population      gdp\n  &lt;fct&gt;     &lt;int&gt;            &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Albania    1960            115.             62.9      6.19    1636054 NA      \n2 Algeria    1960            148.             47.5      7.65   11124892  1.38e10\n3 Angola     1960            208              36.0      7.32    5270844 NA      \n4 Antigua …  1960             NA              63.0      4.43      54681 NA      \n5 Argentina  1960             59.9            65.4      3.11   20619075  1.08e11\n6 Armenia    1960             NA              66.9      4.55    1867396 NA      \n# ℹ 2 more variables: continent &lt;fct&gt;, region &lt;fct&gt;"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-4",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-4",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nAs done in the New Insights on Poverty video, we start by testing our knowledge.\nWhich country do you think had the highest child mortality rates in 2015?\n\nSri Lanka or Turkey.\nPoland or South Korea.\nMalaysia or Russia.\nPakistan or Vietnam.\nThailand or South Africa."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-5",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-5",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\nHere is the data:\n\n\n\n\n\ncountry\ninf_mort\ncountry\ninf_mort\n\n\n\n\nSri Lanka\n8.4\nTurkey\n11.6\n\n\nPoland\n4.5\nSouth Korea\n2.9\n\n\nMalaysia\n6.0\nRussia\n8.2\n\n\nPakistan\n65.8\nVietnam\n17.3\n\n\nThailand\n10.5\nSouth Africa\n33.6"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#scatterplots",
    "href": "slides/dataviz/19-dataviz-in-practice.html#scatterplots",
    "title": "Dataviz In Practice",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nWe start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds.\n\n\nfilter(gapminder, year == 1962) |&gt; \n  ggplot(aes(fertility, life_expectancy)) + \n  geom_point()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-1",
    "title": "Dataviz In Practice",
    "section": "Scatterplots",
    "text": "Scatterplots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-2",
    "title": "Dataviz In Practice",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nTo confirm that indeed these countries are from the regions we expect, we can use color to represent continent.\n\n\nfilter(gapminder, year == 1962) |&gt; \n  ggplot( aes(fertility, life_expectancy, color = continent)) + \n  geom_point()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#scatterplots-3",
    "title": "Dataviz In Practice",
    "section": "Scatterplots",
    "text": "Scatterplots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting\n\nTo make comparisons, however, side by side plots are preferable.\n\n\nfilter(gapminder, year %in% c(1962, 2012)) |&gt; \n  ggplot(aes(fertility, life_expectancy, col = continent)) + \n  geom_point() + \n  facet_grid(year~continent)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting-1",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting-2",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting\n\nIn this case, there is just one variable and we use . to let facet know that we are not using a second variable:\n\n\nfilter(gapminder, year %in% c(1962, 2012)) |&gt; \n  ggplot(aes(fertility, life_expectancy, col = continent)) + \n  geom_point() + \n  facet_grid(. ~ year)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting-3",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting\nThis plot clearly shows that the majority of countries have moved from the developing world cluster to the western world one."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#facet_wrap",
    "href": "slides/dataviz/19-dataviz-in-practice.html#facet_wrap",
    "title": "Dataviz In Practice",
    "section": "facet_wrap",
    "text": "facet_wrap\nThe function facet_wrap permits automatically wrap the series of plots so that each display has viewable dimensions:\n\nyears &lt;- c(1962, 1980, 1990, 2000, 2012) \ncontinents &lt;- c(\"Europe\", \"Asia\") \ngapminder |&gt;  \n  filter(year %in% years & continent %in% continents) |&gt; \n  ggplot( aes(fertility, life_expectancy, col = continent)) + \n  geom_point() + \n  facet_wrap(~year)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#facet_wrap-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#facet_wrap-1",
    "title": "Dataviz In Practice",
    "section": "facet_wrap",
    "text": "facet_wrap"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting-4",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting-4",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting\n\nBy default axes ranges are the same across panes.\nWe can make the axes adapt using scales = \"free\".\n\n\nyears &lt;- c(1962, 1980, 1990, 2000, 2012) \ncontinents &lt;- c(\"Europe\", \"Asia\") \ngapminder |&gt;  \n  filter(year %in% years & continent %in% continents) |&gt; \n  ggplot( aes(fertility, life_expectancy, col = continent)) + \n  geom_point() + \n  facet_wrap(~year, scales = \"free\")  \n\n\nYou will notice that it is harder to see the pattern."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#faceting-5",
    "href": "slides/dataviz/19-dataviz-in-practice.html#faceting-5",
    "title": "Dataviz In Practice",
    "section": "Faceting",
    "text": "Faceting"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#gganimate",
    "href": "slides/dataviz/19-dataviz-in-practice.html#gganimate",
    "title": "Dataviz In Practice",
    "section": "gganimate",
    "text": "gganimate\n\nCodeWrangleAnimation\n\n\n\nlibrary(gganimate)\nyears &lt;- c(1960:2016)\np &lt;- filter(gapminder, year %in% years & !is.na(group) & \n              !is.na(fertility) & !is.na(life_expectancy)) |&gt;\n  mutate(population_in_millions = population/10^6) |&gt;\n  ggplot(aes(fertility, y = life_expectancy, col = group, size = population_in_millions)) +\n  geom_point(alpha = 0.8) +\n  guides(size = \"none\") +\n  theme(legend.title = element_blank()) + \n  coord_cartesian(ylim = c(30, 85)) + \n  labs(title = 'Year: {frame_time}', \n       x = 'Fertility rate (births per woman)', y = 'Life expectancy') +\n  transition_time(year) +\n  ease_aes('linear')\n\nanimate(p, end_pause = 15)\n\n\n\n\nwest &lt;- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ngapminder &lt;- gapminder |&gt; \n  mutate(group = case_when(\n    region %in% west ~ \"The West\",\n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    TRUE ~ \"Others\"))\ngapminder &lt;- gapminder |&gt; \n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"The West\"))))"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\nHere is a trend plot of United States fertility rates:\n\ngapminder |&gt;  \n  filter(country == \"United States\") |&gt;  \n  ggplot(aes(year, fertility)) + \n  geom_point()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-1",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-2",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\n\nWhen the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single series, here a country:\n\n\ngapminder |&gt;  \n  filter(country == \"United States\") |&gt;  \n  ggplot(aes(year, fertility)) + \n  geom_line()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-3",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-4",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-4",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\n\nThis is particularly helpful when we look at two countries.\n\n\ncountries &lt;- c(\"South Korea\", \"Germany\") \ngapminder |&gt; filter(country %in% countries) |&gt;  \n  ggplot(aes(year,fertility)) + \n  geom_line()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-5",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-5",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-6",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-6",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\n\nThis is not the plot that we want.\nTo let ggplot know that there are two curves that need to be made separately, we assign each point to a group, one for each country:\n\n\ncountries &lt;- c(\"South Korea\",\"Germany\") \ngapminder |&gt; filter(country %in% countries & !is.na(fertility)) |&gt;  \n  ggplot(aes(year, fertility, group = country)) + \n  geom_line()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-7",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-7",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-8",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-8",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\n\nBut which line goes with which country? We can assign colors to make this distinction.\n\n\ncountries &lt;- c(\"South Korea\",\"Germany\") \ngapminder |&gt; filter(country %in% countries & !is.na(fertility)) |&gt;  \n  ggplot(aes(year,fertility, col = country)) + \n  geom_line()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-9",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-9",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-10",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-10",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots\n\nWe define a data table with the label locations and then use a second mapping just for these labels:\n\n\nlibrary(geomtextpath) \ngapminder |&gt;  \n  filter(country %in% countries) |&gt;  \n  ggplot(aes(year, life_expectancy, col = country, label = country)) + \n  geom_textpath() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-11",
    "href": "slides/dataviz/19-dataviz-in-practice.html#time-series-plots-11",
    "title": "Dataviz In Practice",
    "section": "Time series plots",
    "text": "Time series plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#data-transformations",
    "href": "slides/dataviz/19-dataviz-in-practice.html#data-transformations",
    "title": "Dataviz In Practice",
    "section": "Data transformations",
    "text": "Data transformations\n\nWe now shift our attention to the the commonly held notion that wealth distribution across the world has become worse during the last decades.\nWhen general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes.\nBy using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case.\nFirst we learn how transformations can sometimes help provide more informative summaries and plots."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#data-transformations-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#data-transformations-1",
    "title": "Dataviz In Practice",
    "section": "Data transformations",
    "text": "Data transformations\n\nGDP measures the market value of goods and services produced by a country in a year.\nThe GDP per person is often used as a rough summary of a country’s wealth.\nGapmider adjusts GDP values for inflation and represent current US dollars.\nWe divide this quantity by 365 to obtain the more interpretable measure dollars per day.\n\n\ngapminder &lt;- gapminder |&gt;   \n  mutate(dollars_per_day = gdp/population/365)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#log-transformation",
    "href": "slides/dataviz/19-dataviz-in-practice.html#log-transformation",
    "title": "Dataviz In Practice",
    "section": "Log transformation",
    "text": "Log transformation\n\nHere is a histogram of per day incomes from 1970:\n\n\npast_year &lt;- 1970 \ngapminder |&gt;  \n  filter(year == past_year & !is.na(gdp)) |&gt; \n  ggplot(aes(dollars_per_day)) +  \n  geom_histogram(binwidth = 1, color = \"black\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-1",
    "title": "Dataviz In Practice",
    "section": "Log transformation",
    "text": "Log transformation"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-2",
    "title": "Dataviz In Practice",
    "section": "Log transformation",
    "text": "Log transformation\n\nIt might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day.\nHere is the distribution if we apply a log base 2 transform:\n\n\ngapminder |&gt;  \n  filter(year == past_year & !is.na(gdp)) |&gt; \n  ggplot(aes(log2(dollars_per_day))) +  \n  geom_histogram(binwidth = 1, color = \"black\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#log-transformation-3",
    "title": "Dataviz In Practice",
    "section": "Log transformation",
    "text": "Log transformation"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#which-base",
    "href": "slides/dataviz/19-dataviz-in-practice.html#which-base",
    "title": "Dataviz In Practice",
    "section": "Which base?",
    "text": "Which base?\n\nIn the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret.\nThe range of the untransformed values is 0.327, 48.885.\nIn base 10, this turns into a range that includes very few integers: just 0 and 1.\nWith base 2, our range includes -2, -1, 0, 1, 2, 3, 4, and 5."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#which-base-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#which-base-1",
    "title": "Dataviz In Practice",
    "section": "Which base?",
    "text": "Which base?\n\nIt is easier to compute \\(2^x\\) and \\(10^x\\) when \\(x\\) is an integer and between -10 and 10, so we prefer to have smaller integers in the scale.\nAnother consequence of a limited range is that choosing the binwidth is more challenging.\nWith log base 2, we know that a binwidth of 1 will translate to a bin with range \\(x\\) to \\(2x\\)."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#which-base-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#which-base-2",
    "title": "Dataviz In Practice",
    "section": "Which base?",
    "text": "Which base?\n\nFor an example in which base 10 makes more sense, consider population sizes:\n\n\ngapminder |&gt;  \n  filter(year == past_year) |&gt; \n  ggplot(aes(log10(population))) + \n  geom_histogram(binwidth = 0.5, color = \"black\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#which-base-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#which-base-3",
    "title": "Dataviz In Practice",
    "section": "Which base?",
    "text": "Which base?"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale",
    "href": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale",
    "title": "Dataviz In Practice",
    "section": "Transform values or scale?",
    "text": "Transform values or scale?\nSuppose we define \\(z = \\log10(x)\\) and want to know what is the value of \\(z\\) if we see it at the mark ^:\n----1----^----2--------3----.\nWe know that the \\(z\\) is about 1.5.\nBut what If the scales are logged, what value of \\(x\\) marked by ^?\n----10---^---100------1000---\nWe need to compute \\(10^{1.5}\\), which is a bit of extra work."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-1",
    "title": "Dataviz In Practice",
    "section": "Transform values or scale?",
    "text": "Transform values or scale?\n\nHowever, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret.\nFor example, we would see “32 dollars a day” instead of “5 log base 2 dollars a day”."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-2",
    "title": "Dataviz In Practice",
    "section": "Transform values or scale?",
    "text": "Transform values or scale?\n\nTo show logged sclaes, instead of logging the values first, we apply this layer:\n\n\ngapminder |&gt;  \n  filter(year == past_year & !is.na(gdp)) |&gt; \n  ggplot(aes(dollars_per_day)) +  \n  geom_histogram(binwidth = 1, color = \"black\") + \n  scale_x_continuous(trans = \"log2\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#transform-values-or-scale-3",
    "title": "Dataviz In Practice",
    "section": "Transform values or scale?",
    "text": "Transform values or scale?"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions",
    "href": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions",
    "title": "Dataviz In Practice",
    "section": "Comparing distributions",
    "text": "Comparing distributions\n\nWe reorder the regions by the median value and use a log scale.\n\n\ngapminder |&gt;  \n  filter(year == past_year & !is.na(gdp)) |&gt; \n  mutate(region = reorder(region, dollars_per_day, FUN = median)) |&gt; \n  ggplot(aes(dollars_per_day, region)) + \n  geom_point() + \n  scale_x_continuous(trans = \"log2\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions-1",
    "title": "Dataviz In Practice",
    "section": "Comparing distributions",
    "text": "Comparing distributions"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#comparing-distributions-2",
    "title": "Dataviz In Practice",
    "section": "Comparing distributions",
    "text": "Comparing distributions\n\ngapminder &lt;- gapminder |&gt;  \n  mutate(group = case_when( \n    region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \n                    \"Northern America\",  \n                  \"Australia and New Zealand\") ~ \"West\", \n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\", \n    region %in% c(\"Caribbean\", \"Central America\",  \n                  \"South America\") ~ \"Latin America\", \n    continent == \"Africa\" &  \n      region != \"Northern Africa\" ~ \"Sub-Saharan\", \n    TRUE ~ \"Others\")) |&gt;  \n  mutate(group = factor(group, levels = c(\"Others\", \"Latin America\",  \n                                          \"East Asia\", \"Sub-Saharan\", \n                                          \"West\")))"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#boxplots",
    "href": "slides/dataviz/19-dataviz-in-practice.html#boxplots",
    "title": "Dataviz In Practice",
    "section": "Boxplots",
    "text": "Boxplots\n\np &lt;- gapminder |&gt;  \n  filter(year == past_year & !is.na(gdp)) |&gt; \n  mutate(group = reorder(group, dollars_per_day, FUN = median)) |&gt; \n  ggplot(aes(group, dollars_per_day)) + \n  geom_boxplot() + \n  scale_y_continuous(trans = \"log2\") + \n  xlab(\"\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))  \np"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#boxplots-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#boxplots-1",
    "title": "Dataviz In Practice",
    "section": "Boxplots",
    "text": "Boxplots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#boxplots-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#boxplots-2",
    "title": "Dataviz In Practice",
    "section": "Boxplots",
    "text": "Boxplots\n\nBoxplots have the limitation that by summarizing the data into five numbers, we might miss important characteristics of the data.\nOne way to avoid this is by showing the data.\n\n\np + geom_point(alpha = 0.5)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#boxplots-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#boxplots-3",
    "title": "Dataviz In Practice",
    "section": "Boxplots",
    "text": "Boxplots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots\n\nShowing each individual point does not always reveal important characteristics of the distribution.\nBoxplots help with this by providing a five-number summary, but this has limitations too.\nFor example, boxplots will not permit us to discover bimodal distributions."
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-1",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots\nHere is an simulated example:"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-2",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots\n\nHere is the income data shown above with boxplots but with a ridge plot.\n\n\nlibrary(ggridges) \np &lt;- gapminder |&gt;  \n  filter(year == past_year & !is.na(dollars_per_day)) |&gt; \n  mutate(group = reorder(group, dollars_per_day, FUN = median)) |&gt; \n  ggplot(aes(dollars_per_day, group)) +  \n  scale_x_continuous(trans = \"log2\")  \np  + geom_density_ridges()"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-3",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-3",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-4",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-4",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots\n\nIf the number of data points is small enough, we can add them to the ridge plot using the following code:\n\n\np + geom_density_ridges(jittered_points = TRUE)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-5",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-5",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-6",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-6",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots\n\nTo show data points, but without using jitter we can use the following code to add what is referred to as a rug representation of the data.\n\n\np + geom_density_ridges(jittered_points = TRUE,  \n                        position = position_points_jitter(height = 0), \n                        point_shape = '|', point_size = 3,  \n                        point_alpha = 1, alpha = 0.7)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-7",
    "href": "slides/dataviz/19-dataviz-in-practice.html#ridge-plots-7",
    "title": "Dataviz In Practice",
    "section": "Ridge plots",
    "text": "Ridge plots"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-6",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-6",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\nWe keep countries that exist both in 1970 and 2010:\n\npast_year &lt;- 1970 \npresent_year &lt;- 2010 \nyears &lt;- c(past_year, present_year) \ncountry_list &lt;- gapminder |&gt;  \n  filter(year %in% c(present_year, past_year)) |&gt; \n  group_by(country) |&gt; \n  summarize(n = sum(!is.na(dollars_per_day)), .groups = \"drop\") |&gt; \n  filter(n == 2) |&gt; \n  pull(country)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-7",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-7",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\n\nWe can compare the distributions using this code:\n\n\ngapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  mutate(west = ifelse(group == \"West\", \"West\", \"Developing\")) |&gt; \n  ggplot(aes(dollars_per_day)) + \n  geom_histogram(binwidth = 1, color = \"black\") + \n  scale_x_continuous(trans = \"log2\") +  \n  facet_grid(year ~ west)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-8",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-8",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-9",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-9",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\nTo see which specific regions improved the most, we can remake the boxplots we made above, but now adding the year 2010 and then using facet to compare the two years.\n\ngapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  mutate(group = reorder(group, dollars_per_day, FUN = median)) |&gt; \n  ggplot(aes(group, dollars_per_day)) + \n  geom_boxplot() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  scale_y_continuous(trans = \"log2\") + \n  xlab(\"\") + \n  facet_grid(. ~ year)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-10",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-10",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-11",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-11",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\nTo add color:\n\ngapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  mutate(group = reorder(group, dollars_per_day, FUN = median),\n         year = factor(year)) |&gt; \n  ggplot(aes(group, dollars_per_day, fill = year)) + \n  geom_boxplot() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  scale_y_continuous(trans = \"log2\") + \n  xlab(\"\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-12",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-12",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-13",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-13",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1\nLet’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing:\n\ngapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  ggplot(aes(dollars_per_day)) + \n  geom_density(fill = \"grey\") +  \n  scale_x_continuous(trans = \"log2\") +  \n  facet_grid(. ~ year)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-14",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-1-14",
    "title": "Dataviz In Practice",
    "section": "Case study 1",
    "text": "Case study 1"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#accessing-computed-variables",
    "href": "slides/dataviz/19-dataviz-in-practice.html#accessing-computed-variables",
    "title": "Dataviz In Practice",
    "section": "Accessing computed variables",
    "text": "Accessing computed variables\nUse after_stat to access density.\n\np &lt;- gapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  mutate(group = ifelse(group == \"West\", \"West\", \"Developing\")) |&gt; \n  ggplot(aes(dollars_per_day, y = after_stat(count), fill = group)) + \n  scale_x_continuous(trans = \"log2\", limits = c(0.125, 300)) \np + geom_density(alpha = 0.2) + facet_grid(year ~ .)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#accessing-computed-variables-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#accessing-computed-variables-1",
    "title": "Dataviz In Practice",
    "section": "Accessing computed variables",
    "text": "Accessing computed variables"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#changing-smoothness",
    "href": "slides/dataviz/19-dataviz-in-practice.html#changing-smoothness",
    "title": "Dataviz In Practice",
    "section": "Changing smoothness",
    "text": "Changing smoothness\nWe can change the smoothness. We selected 0.75 after trying out several values.\n\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#multiple-ridge-plot",
    "href": "slides/dataviz/19-dataviz-in-practice.html#multiple-ridge-plot",
    "title": "Dataviz In Practice",
    "section": "Multiple ridge plot",
    "text": "Multiple ridge plot\nTo visualize if any of the groups defined above are driving this we can quickly make a ridge plot:\n\ngapminder |&gt;  \n  filter(year %in% years & !is.na(dollars_per_day)) |&gt; \n  mutate(group = reorder(group, dollars_per_day, FUN = median)) |&gt;\n  ggplot(aes(dollars_per_day, group)) +  \n  scale_x_continuous(trans = \"log2\") +  \n  geom_density_ridges(bandwidth = 1.5) + \n  facet_grid(. ~ year)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#multiple-ridge-plot-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#multiple-ridge-plot-1",
    "title": "Dataviz In Practice",
    "section": "Multiple ridge plot",
    "text": "Multiple ridge plot"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#stacking-densities",
    "href": "slides/dataviz/19-dataviz-in-practice.html#stacking-densities",
    "title": "Dataviz In Practice",
    "section": "Stacking densities",
    "text": "Stacking densities\n\nAnother way to achieve this is by stacking the densities on top of each other:\n\n\ngapminder |&gt;  \n    filter(year %in% years & country %in% country_list) |&gt; \n  group_by(year) |&gt; \n  mutate(weight = population/sum(population)*2) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(dollars_per_day, fill = group)) + \n  scale_x_continuous(trans = \"log2\", limits = c(0.125, 300)) +  \n  geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") +  \n  facet_grid(year ~ .)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#stacking-densities-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#stacking-densities-1",
    "title": "Dataviz In Practice",
    "section": "Stacking densities",
    "text": "Stacking densities"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#weighted-densities",
    "href": "slides/dataviz/19-dataviz-in-practice.html#weighted-densities",
    "title": "Dataviz In Practice",
    "section": "Weighted densities",
    "text": "Weighted densities\nHere we weigh the countries by size:\n\ngapminder |&gt;  \n  filter(year %in% years & country %in% country_list) |&gt; \n  group_by(year) |&gt; \n  mutate(weight = population/sum(population)*2) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(dollars_per_day, fill = group, weight = weight)) + \n  scale_x_continuous(trans = \"log2\", limits = c(0.125, 300)) +  \n  geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") + \n  facet_grid(year ~ .)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#weighted-densities-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#weighted-densities-1",
    "title": "Dataviz In Practice",
    "section": "Weighted densities",
    "text": "Weighted densities"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#the-ecological-fallacy",
    "href": "slides/dataviz/19-dataviz-in-practice.html#the-ecological-fallacy",
    "title": "Dataviz In Practice",
    "section": "The ecological fallacy",
    "text": "The ecological fallacy"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#show-the-data",
    "href": "slides/dataviz/19-dataviz-in-practice.html#show-the-data",
    "title": "Dataviz In Practice",
    "section": "Show the data",
    "text": "Show the data"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#case-study-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#case-study-2",
    "title": "Dataviz In Practice",
    "section": "Case study 2",
    "text": "Case study 2\n\nthe_disease &lt;- \"Measles\" \ndat &lt;- us_contagious_diseases |&gt; \n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == the_disease) |&gt; \n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) |&gt;  \n  mutate(state = reorder(state, ifelse(year &lt;= 1963, rate, NA),  \n                         median, na.rm = TRUE))"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#heatmaps",
    "href": "slides/dataviz/19-dataviz-in-practice.html#heatmaps",
    "title": "Dataviz In Practice",
    "section": "Heatmaps",
    "text": "Heatmaps\n\nlibrary(RColorBrewer)\ndat |&gt; ggplot(aes(year, state, fill = rate)) + \n  geom_tile(color = \"grey50\") + \n  scale_x_continuous(expand = c(0,0)) + \n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") + \n  geom_vline(xintercept = 1963, col = \"blue\") + \n  theme_minimal() +   \n  theme(panel.grid = element_blank(),  \n        legend.position = \"bottom\",  \n        text = element_text(size = 8)) + \n  labs(title = the_disease, x = \"\", y = \"\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#heatmaps-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#heatmaps-1",
    "title": "Dataviz In Practice",
    "section": "Heatmaps",
    "text": "Heatmaps"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#using-position",
    "href": "slides/dataviz/19-dataviz-in-practice.html#using-position",
    "title": "Dataviz In Practice",
    "section": "Using position",
    "text": "Using position\nWe will show each state and the avearge:\n\navg &lt;- us_contagious_diseases |&gt; \n  filter(disease == the_disease) |&gt; group_by(year) |&gt; \n  summarize(us_rate = sum(count, na.rm = TRUE) /  \n              sum(population, na.rm = TRUE) * 10000)"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#using-position-1",
    "href": "slides/dataviz/19-dataviz-in-practice.html#using-position-1",
    "title": "Dataviz In Practice",
    "section": "Using position",
    "text": "Using position\n\ndat |&gt;  \n  filter(!is.na(rate)) |&gt; \n    ggplot() + \n  geom_line(aes(year, rate, group = state),  color = \"grey50\",  \n            show.legend = FALSE, alpha = 0.2, linewidth = 1) + \n  geom_line(mapping = aes(year, us_rate),  data = avg, linewidth = 1) + \n  scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) +  \n  ggtitle(\"Cases per 10,000 by state\") +  \n  xlab(\"\") + ylab(\"\") + \n  geom_text(data = data.frame(x = 1955, y = 50),  \n            mapping = aes(x, y, label = \"US average\"),  \n            color = \"black\") +  \n  geom_vline(xintercept = 1963, col = \"blue\")"
  },
  {
    "objectID": "slides/dataviz/19-dataviz-in-practice.html#using-position-2",
    "href": "slides/dataviz/19-dataviz-in-practice.html#using-position-2",
    "title": "Dataviz In Practice",
    "section": "Using position",
    "text": "Using position"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BST 260 Introduction to Data Science",
    "section": "",
    "text": "Instructor: Rafael A.Irizarry\nTeaching fellows: Corri Sept, Nikhil Vytla, and Yuan Wang\nLocation: Kresge 202A and 202B, Harvard School of Public Health\nDate and time: Mon & Wed 9.45 - 11:15am\nTextbooks: https://github.com/rafalab/dsbook-part-1, https://github.com/rafalab/dsbook-part-2\nSlack: https://bst260fall2024.slack.com/\nCanvas: https://canvas.harvard.edu/courses/143922/pages/Course%20Information\nGitHub repo: https://github.com/datasciencelabs/2024\nRemember to read the syllabus, listen to SD.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "BST 260 Introduction to Data Science",
    "section": "",
    "text": "Instructor: Rafael A.Irizarry\nTeaching fellows: Corri Sept, Nikhil Vytla, and Yuan Wang\nLocation: Kresge 202A and 202B, Harvard School of Public Health\nDate and time: Mon & Wed 9.45 - 11:15am\nTextbooks: https://github.com/rafalab/dsbook-part-1, https://github.com/rafalab/dsbook-part-2\nSlack: https://bst260fall2024.slack.com/\nCanvas: https://canvas.harvard.edu/courses/143922/pages/Course%20Information\nGitHub repo: https://github.com/datasciencelabs/2024\nRemember to read the syllabus, listen to SD.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "BST 260 Introduction to Data Science",
    "section": "Lectures",
    "text": "Lectures\nLecture slides, class notes, and problem sets are linked below. New material is added approximately on a weekly basis.\n\n\n\nDates\nTopic\nSlides\nReading\n\n\n\n\nSep 04\nProductivity Tools\nIntro, Unix\nInstalling R and RStudio on Windows or Mac, Getting Started, Unix\n\n\nSep 09, Sep 11\nProductivity Tools\nRStudio, Quarto, Git and GitHub\nRStudio Projects, Quarto, Git\n\n\nSep 16, Sep 19\nR\nR basics, Vectorization\nR Basics, Vectorization\n\n\nSep 23\nR\nTidyverse, ggplot2\ndplyr, ggplot2\n\n\nSep 25\nR\nTyding data\nReshaping Data\n\n\nSep 30, Oct 02\nWrangling\nIntro, Data Importing, Dates and Times, Locales, Data APIs, Web scraping, Joining tables\nImporting data, dates and times, Locales, Joining Tables, Extracting data from the web\n\n\nOct 07, Oct 09\nData visualization\nData Viz Principles, Distributions, Dataviz in practice\nDistributions, Dataviz Principles\n\n\nOct 16\nMidterm 1\n\nCovers material from Sep 04-Oct 11\n\n\nOct 21\nProbability\nIntro, Foundations for Inference\nMonte Carlo, Random Variables & CLT\n\n\nOct 23\nInference\nIntro, Parameter and estimates, Confidence Intervals\nParameters & Estimates, Confidence Intervals\n\n\nOct 28, Oct 30\nStatistical Models\nModels, Bayes, Hierarchical Models\nData-driven Models, Bayesian Statistics, Hierarchical Models\n\n\nNov 04, Nov 06\nLinear models\nIntro, Regression\nRegression, Multivariate Regression\n\n\nNov 13, Nov 18\nLinear models\nMultivariate regression, Treatment effect models\nMeasurement Error Models, Treatment Effect Models, Association Tests, Association Not Causation\n\n\nNov 20\nHigh dimensional data\nIntro to Linear Algebra, Matrices in R\nMatrices in R, Applied Linear Algebra,\n\n\nNov 25\nMidterm 2\n\nMidterm 2: cover material from Sep 04-Nov 22\n\n\nDec 02\nHigh dimensional data\nDistance, Dimension reduction\nDimension Reduction\n\n\nDec 04\nMachine Learning\nIntro, Metrics, Conditionals, Smoothing\nNotation and terminology, Evaluation Metrics, conditional probabilities, smoothing\n\n\nDec 09, Dec 11\nMachine Learning\nkNN, Resampling methods, caret package, Algorithms, ML in practice\nResampling methods, ML algorithms, ML in practice\n\n\nDec 16, Dec 18\nOther topics\nAssociation is not causation, Shiny, Shiny example code\nAssociation is not causation",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#problem-sets",
    "href": "index.html#problem-sets",
    "title": "BST 260 Introduction to Data Science",
    "section": "Problem sets",
    "text": "Problem sets\n\n\n\nProblem set\nTopic\nDue Date\nDifficulty\n\n\n\n\n01\nUnix, Quarto\nSep 11\neasy\n\n\n02\nR\nSep 19\neasy\n\n\n03\nTidyverse\nSep 27\nmedium\n\n\n04\nWrangling\nOct 4\nmedium\n\n\n05\nCovid 19 data visualization\nOct 11\nmedium\n\n\n06\nProbability\nOct 25\neasy\n\n\n07\nPredict the election\nNov 04\nhard\n\n\n08\nExcess mortality after Hurricane María\nNov 15\nmedium\n\n\n09\nMatrices\nNov 22\neasy\n\n\n10\nDigit reading\nDec 16\nhard\n\n\nFinal Project\nYour choice\nDec 20\nhard",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#office-hour-times",
    "href": "index.html#office-hour-times",
    "title": "BST 260 Introduction to Data Science",
    "section": "Office hour times",
    "text": "Office hour times\n\n\n\nMeeting\nTime\nLocation\n\n\n\n\nRafael Irizarry\nMon 8:45-9:45AM\nKresge 203\n\n\nCorri Sept\nTue 3:00-4:00PM\nKresge 201\n\n\nNikhil Vytla\nWed 2:00-3:00PM\nKresge 201\n\n\nYuan Wang\nFri 1:00-2:00PM\nZoom",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "BST 260 Introduction to Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe thank Maria Tackett and Mine Çetinkaya-Rundel for sharing their web page template, which we used in creating this website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "psets/pset-04-wrangling.html",
    "href": "psets/pset-04-wrangling.html",
    "title": "Problem set 4",
    "section": "",
    "text": "In the next problem set, we plan to explore the relationship between COVID-19 death rates and vaccination rates across US states by visually examining their correlation. This analysis will involve gathering COVID-19 related data from the CDC’s API and then extensively processing it to merge the various datasets. Since the population sizes of states vary significantly, we will focus on comparing rates rather than absolute numbers. To facilitate this, we will also source population data from the US Census to accurately calculate these rates.\nIn this problem set we will learn how to extract and wrangle data from the data US Census and CDC APIs.\n\nGet an API key from the US Census at https://api.census.gov/data/key_signup.html. You can’t share this public key. But your code has to run on a TFs computer. Assume the TF will have a file in their working directory named census-key.R with the following one line of code:\n\ncensus_key &lt;- \"A_CENSUS_KEY_THAT_WORKS\"\nWrite a first line of code for your problem set that defines census_key by running the code in the file census-key.R.\n\n## Your code here\n\n\nThe US Census API User Guide provides details on how to leverage this valuable resource. We are interested in vintage population estimates for years 2021 and 2022. From the documentation we find that the endpoint is:\n\n\nurl &lt;- \"https://api.census.gov/data/2021/pep/population\"\n\nUse the httr2 package to construct the following GET request.\nhttps://api.census.gov/data/2021/pep/population?get=POP_2020,POP_2021,NAME&for=state:*&key=YOURKEYHERE\nCreate an object called request of class httr2_request with this URL as an endpoint. Hint: Print out request to check that the URL matches what we want.\n\nlibrary(httr2)\n#request &lt;- \n\n\nMake a request to the US Census API using the request object. Save the response to and object named response. Check the response status of your request and make sure it was successful. You can learn about status codes here.\n\n\n#response &lt;- \n\n\nUse a function from the httr2 package to determine the content type of your response.\n\n\n# Your code here\n\n\nUse just one line of code and one function to extract the data into a matrix. Hints: 1) Use the resp_body_json function. 2) The first row of the matrix will be the variable names and this OK as we will fix in the next exercise.\n\n\n#population &lt;- \n\n\nExamine the population matrix you just created. Notice that 1) it is not tidy, 2) the column types are not what we want, and 3) the first row is a header. Convert population to a tidy dataset. Remove the state ID column and change the name of the column with state names to state_name. Add a column with state abbreviations called state. Make sure you assign the abbreviations for DC and PR correctly. Hint: Use the janitor package to make the first row the header.\n\n\nlibrary(tidyverse)\nlibrary(janitor)\n#population &lt;- population |&gt; ## Use janitor row to names function\n  # convert to tibble\n  # remove stat column\n  # rename state column to state_name\n  # use pivot_longer to tidy\n  # remove POP_ from year\n  # parese all relevant colunns to numeric\n  # add state abbreviations using state.abb variable\n  # use case_when to add abbreviations for DC and PR\n\n\nAs a check, make a barplot of states’ 2021 and 2022 populations. Show the state names in the y-axis ordered by population size. Hint: You will need to use reorder and use facet_wrap.\n\n\n# population |&gt; \n  # reorder state\n  # assign aesthetic mapping\n  # use geom_col to plot barplot\n  # flip coordinates\n  # facet by year\n\n\nThe following URL:\n\n\nurl &lt;- \"https://github.com/datasciencelabs/2024/raw/refs/heads/main/data/regions.json\"\n\npoints to a JSON file that lists the states in the 10 Public Health Service (PHS) defined by CDC. We want to add these regions to the population dataset. To facilitate this create a data frame called regions that has two columns state_name, region, region_name. One of the regions has a long name. Change it to something shorter.\n\nlibrary(jsonlite)\nlibrary(purrr)\nurl &lt;- \"https://github.com/datasciencelabs/2024/raw/refs/heads/main/data/regions.json\"\n# regions &lt;- use jsonlit JSON parser \n# regions &lt;- convert list to data frame. You can use map_df in purrr package \n\n\nAdd a region and region name columns to the population data frame.\n\n\n# population &lt;- \n\n\nFrom reading https://data.cdc.gov/ we learn the endpoint https://data.cdc.gov/resource/pwn4-m3yp.json provides state level data from SARS-COV2 cases. Use the httr2 tools you have learned to download this into a data frame. Is all the data there? If not, comment on why.\n\n\napi &lt;- \"https://data.cdc.gov/resource/pwn4-m3yp.json\"\n# cases_raw &lt;- \n\nWe see exactly 1,000 rows. We should be seeing over \\(52 \\times 3\\) rows per state.\n\nThe reason you see exactly 1,000 rows is because CDC has a default limit. You can change this limit by adding $limit=10000000000 to the request. Rewrite the previous request to ensure that you receive all the data. Then wrangle the resulting data frame to produce a data frame with columns state, date (should be the end date) and cases. Make sure the cases are numeric and the dates are in Date ISO-8601 format.\n\n\napi &lt;- \"https://data.cdc.gov/resource/pwn4-m3yp.json\"\n# cases_raw &lt;- \n\n\nFor 2020 and 2021, make a time series plot of cases per 100,000 versus time for each state. Stratify the plot by region name. Make sure to label you graph appropriately.\n\n\n#cases |&gt;"
  },
  {
    "objectID": "psets/pset-02-r-vectorization.html",
    "href": "psets/pset-02-r-vectorization.html",
    "title": "Problem set 2",
    "section": "",
    "text": "For these exercises, do not load any packages other than dslabs.\nMake sure to use vectorization whenever possible.\n\nWhat is the sum of the first 100 positive integers? Use the functions seq and sum to compute the sum with R for any n.\n\n\n# Your code here\n\n\nLoad the US murders dataset from the dslabs package. Use the function str to examine the structure of the murders object. What are the column names used by the data frame for these five variables? Show the subset of murders showing states with less than 1 per 100,000 deaths. Show all variables.\n\n\nlibrary(dslabs)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\n\n\n# Your code here\n\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths and in the West of the US. Don’t show the region variable.\n\n\n# Your code here\n\n\nShow the largest state with a rate less than 1 per 100,000.\n\n\n# Your code here\n\n\nShow the state with a population of more than 10 million with the lowest rate.\n\n\n# Your code here\n\n\nCompute the rate for each region of the US.\n\n\n# Your code here\n\n\nCreate a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\n\n\n# Your code here\n\n\nMake this data frame:\n\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\", \n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nConvert the temperatures to Celsius.\n\n# Your code here\n\n\nWrite a function euler that compute the following sum for any \\(n\\):\n\n\\[\nS_n = 1+1/2^2 + 1/3^2 + \\dots 1/n^2\n\\]\n\n# Your code here\n\n\nShow that as \\(n\\) gets bigger we get closer \\(\\pi^2/6\\) by plotting \\(S_n\\) versus \\(n\\) with a horizontal dashed line at \\(\\pi^2/6\\).\n\n\n# Your code here\n\n\nUse the %in% operator and the predefined object state.abb to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n\n\n# Your code here\n\n\nExtend the code you used in the previous exercise to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n\n\n# Your code here\n\n\nIn the murders dataset, use %in% to show all variables for New York, California, and Texas, in that order.\n\n\n# Your code here\n\n\nWrite a function called vandermonde_helper that for any \\(x\\) and \\(n\\), returns the vector \\((1, x, x^2, x^3, \\dots, x^n)\\). Show the results for \\(x=3\\) and \\(n=5\\).\n\n\n# Your code here\n\n\nCreate a vector using:\n\n\nn &lt;- 10000\np &lt;- 0.5\nset.seed(2024-9-6)\nx &lt;- sample(c(0,1), n, prob = c(1 - p, p), replace = TRUE)\n\nCompute the length of each stretch of 1s and then plot the distribution of these values. Check to see if the distribution follows a geometric distribution as the theory predicts. Do not use a loop!\n\n# Your code here"
  },
  {
    "objectID": "psets/pset-07-election.html",
    "href": "psets/pset-07-election.html",
    "title": "Problem set 7",
    "section": "",
    "text": "For this problem set we want you to predict the election. You will enter you predictions to this form. You you will report a prediction of the number of electoral votes for Harris and an interval. You will do the same for the popular vote. We will give prizes for those that report the shortest interval but with the true result inside the interval.\n\nRead in the data provided here:\n\n\nurl &lt;- \"https://projects.fivethirtyeight.com/polls/data/president_polls.csv\"\n\nExamine the data frame paying particular attention to the poll_id question_id, population, and candidate. Note that some polls have more than one question based on different population types.\n\nlibrary(tidyverse)\nlibrary(rvest)\nraw_dat &lt;- ### Your code here\n\n\nPolls are based on either likely voters (lv), registered voters (rv), all voters (a), or voters (v). Polls based on ‘voters’ are exit polls. We want to remove these because exit polls are too old or might be biased due to differences in the likelihood of early voter by party. We prefer likely voter (lv) polls because they are more predictive. Registered voter polls are more predictive than all voter (a) polls. Remove the exit poll (v) polls and then redefine population to be a factor ordered from best to worse predictive power: (lv, rv, a). You should also remove hypothetical polls and make the date columns into date objects. Name the resulting data frame dat.\n\n\ndat &lt;- raw_dat |&gt; \n  ## Your code here\n\n\nSome polls asked more than one questions. So if you filter to one poll ID in our dataset, you might see more than one question ID associated with the same poll. The most common reason for this is that they asked a head-to-head question (Harris versus Trump) and, in the same poll, a question about all candidates. We want to prioritize the head-to-head questions.\n\nAdd a column that tells us, for each question, how many candidates where mentioned in that question.\nAdd a new column n to dat that provides the number of candidates mentioned for each question. For example the relevant column of your final table will looks something like this:\n\n\n\npoll_id\nquestion_id\ncandidate\nn\n\n\n\n\n1\n1\nHarris\n2\n\n\n1\n1\nTrump\n2\n\n\n1\n2\nHarris\n3\n\n\n1\n2\nTrump\n3\n\n\n1\n2\nStein\n3\n\n\n\n\ndat &lt;- dat |&gt; \n    ## Your code here\n\n\nWe are going to focus on the Harris versus Trump comparison. Redefine dat to only include the rows providing information for Harris and Trump. Then pivot the dataset so that the percentages for Harris and Trump are in their own columns. Note that for pivot to work you will have to remove some columns. To avoid this keep only the columns you are pivoting and along with poll_id, question_id, state, pollster, start_date, end_date, numeric_grade, sample_size. Once you accomplish the pivot, add a column called spread with the difference between Harris and Trump.\n\nNote that the values stored in spread are estimates of the popular vote difference that we will use to predict for the competition:\nspread = % of the popular vote for Harris - % of the popular vote for Trump\nHowever, for the calculations in the rest of problem set to be consistent with the sampling model we have been discussing in class, save spread as a proportion, not a percentage. But remember to turn it back to a percentage when submitting your entry to the competition.\n\ndat &lt;- dat |&gt;\n  ## Your code here\n\n\nNote that some polls have multiple questions. We want to keep only one question per poll. We will keep likely voter (lv) polls when available, and prefer register voter (rv) over all voter polls (a). If more than one question was asked in one poll, take the most targeted question (smallest n). Save the resulting tabledat. Note that now each after you do this each row will represents exactly one poll/question, so can remove n, poll_id and question_id.\n\n\ndat &lt;- dat |&gt;\n  ## Your code here\n\n\nSeparate dat into two data frames: one with popular vote polls and one with state level polls. Call them popular_vote and polls respectively.\n\n\npopular_vote &lt;- ## Your code here\npolls &lt;- ## Your code here\n\n\nFor the popular vote, plot the spread reported by each poll against start date for polls starting after July 21, 2024. Rename all the pollsters with less than 5 polls during this period as Other. Use color to denote pollster. Make separate plots for likely voters and registered voters. Do not use all voter polls (a). Use geom_smooth with method loess to show a curve going through the points. You can change how adaptive the curve is to that through the span argument.\n\n\npopular_vote |&gt; \n  filter(start_date &gt; make_date(2024, 7, 21) & population != \"a\") |&gt;\n  ### Your code here\n\n\nTo show the pollster effect, make boxplots for the the spread for each popular vote poll. Include only likely voter polls starting after July 21, 2024. Rename all the pollsters with less than 5 polls during that time period as Other.\n\n\npopular_vote |&gt; \n  filter(start_date &gt; make_date(2024, 7, 21) & population == \"lv\") |&gt;\n  ## Your code here\n\n\nCompute a prediction and an interval for the competition and submit here Include the code you used to create your confidence interval for the popular vote here:\n\n\n## Your code here\n\nWe now move on to predicting the electoral votes.\n\nTo obtain the number of electoral votes for each state we will visit this website:\n\n\nurl &lt;- \"https://state.1keydata.com/state-electoral-votes.php\"\n\nWe can use the rvest package to download and extract the relevant table:\n\nlibrary(rvest)\nh &lt;- read_html(url) |&gt;\n  html_table() \n\nev &lt;- h[[4]]\n\nWrangle the data in ev to only have two columns state and electoral_votes. Make sure the electoral vote column is numeric. Add the electoral votes for Maine CD-1 (1), Maine CD-2 (1), Nebraska CD-2 (1), and District of Columbia (3) by hand.\n\n### Your code here\n\n\nThe presidential race in some states is a forgone conclusion. Because their is practically no uncertainty in who will win, polls are not taken. We will therefore assume that the party that won in 2020 will win again in 2024 if no polls are being collected for a state.\n\nDownload the following sheet:\n\nlibrary(gsheet)\nsheet_url &lt;- \"https://docs.google.com/spreadsheets/d/1D-edaVHTnZNhVU840EPUhz3Cgd7m39Urx7HM8Pq6Pus/edit?gid=29622862\"\nraw_res_2020 &lt;- gsheet2tbl(sheet_url) \n\nTidy the raw_res_2020 dataset so that you have two columns state and party, with D and R in the party column to indicate who won in 2020. Add Maine CD-1 (D), Maine CD-2 (R), Nebraska CD-2 (D), and District of Columbia (D) by hand. Save the result to res_2020. Hint use the janitor row_to_names function.\n\nlibrary(janitor)\nres_2020 &lt;- raw_res_2020[,c(1,4)] |&gt;  \n ### Your code here\n\n\nDecide on a period that you will use to compute your prediction. We will use spread as the outcome. Make sure the the outcomes is saved as a proportion not percentage. Create a results data frame with columns state, avg, sd, n and electoral_votes, with one row per state.\n\nSome ideas and recommendations:\n\nIf a state has enough polls, consider a short period, such as a week. For states with few polls you might need to increase the interval to increase the number of polls.\nDecide which polls to prioritize based on the population and numeric_grade columns.\nYou might want to weigh them differently, in which you might also consider using sample_size.\nIf you use fewer than 5 polls to calculate an average, your estimate of the standard deviation (SD) may be unreliable. With only one poll, you wont be able to estimate the SD at all. In these cases, consider using the SD from similar states to avoid unusual or inaccurate estimates.\n\n\nresults &lt;- polls |&gt; \n  ### Your code here\n\n\nNote you will not have polls for all states. Assume that lack of polls implies the state is not in play. Use the res_2020 data frame to compute the electoral votes Harris is practically guaranteed to have.\n\n\nharris_start &lt;- ## Your code here\n\n\nUse a Bayesian approach to compute posterior means and standard deviations for each state in results. Plot the posterior mean versus the observed average with the size of the point proportional to the number of polls.\n\n\n### Your code heer\n\n\nCompute a prediction and an interval for Harris’ electoral votes and submit to the competition here. Include the code you used to create your estimate and interval below.\n\n\n### Your code here"
  },
  {
    "objectID": "psets/pset-01-unix-quarto.html",
    "href": "psets/pset-01-unix-quarto.html",
    "title": "Problem set 1",
    "section": "",
    "text": "After finishing the homework, you are to turn in all the code to GitHub using git.\n\nStart an RStudio project. Pick a good name following a naming convention. Start a Quarto document called beginning.qmd.\nCreate a directory called img and save a screen shot of your RStudio session for the project. Include your screenshot in the Quarto document.\nNext, in your Quarto document, define variables \\(a=1, b=-1, c=-2\\) and print out the solutions to \\(f(x) = ax^2+bx+c=0\\). Do not report complex solutions, only real numbers.\nInclude a graph of \\(f(x)\\) versus \\(x\\) for \\(x \\in (-5,5)\\).\n\n\nx &lt;- seq(-5, 5, length = 100)\n# Hint: Use the plot function\n\n\nCreate a directory called docs. Use the command quarto render to create a PDF and save it to the docs directory. Show us the command you typed:\n\n# Your code here\n\nUse Unix to create a directory called data in the project home directory. Include the Unix command you used to create the directory.\n\n# Your code here\n\nUse a terminal-based text editor to create a file coefs.txt in the data directory and save three coefficients, 1 -1 -2 for example. Show us the Unix commands you used to achieve this:\n\n# Your code here\n\nMake a directory called code. Use Unix to copy the file beginning.qmd to a file called quadratic.qmd in the code directory. Show us the Unix commands you used.\n\n# Your code here\n\nEdit the quadratic.qmd file to read in a, b, and c from the file coefs.txt. Make sure to use a relative path when reading the file. As before, print out the solutions to \\(f(x) = ax^2+bx+c=0\\). Do not report complex solutions, only real numbers.\nChange the path of the file you are reading to the full path you get when you type file.path(getwd(), \"data/coefs.txt\"). Confirm that the file still renders. Then move the entire pset-01-rmarkdown project to a directory called RtmpyDknq4. Does the file render? Change the path back to a relative path and see if it renders."
  },
  {
    "objectID": "psets/pset-03-tidyverse.html",
    "href": "psets/pset-03-tidyverse.html",
    "title": "Problem set 3",
    "section": "",
    "text": "In these exercises, we will explore a subset of the NHANES dataset to investigate potential differences in systolic blood pressure across groups defined by self reported race.\n\nInstructions\n\nFor each exercise, we want you to write a single line of code using the pipe (|&gt;) to chain together multiple operations. This doesn’t mean the code must fit within 80 characters or be written on a single physical line, but rather that the entire sequence of operations can be executed as one continuous line of code without needing to assign intermediate values or create new variables.\nFor example, these are three separate lines of code:\n\n\nx &lt;- 100; x &lt;- sqrt(x); log10(x)\n\nWhereas this is considered one line of code using the pipe:\n\n100 |&gt; \n  sqrt() |&gt; \n  log10()\n\n\nGenerate an html document that shows the code for each exercise.\nFor the exercises that ask to generate a graph, show the graph as well.\nFor exercises that require you to display tabular results, use the kable function to format the output as a clean, readable table. Do not display the raw dataframe directly—only show the nicely formatted table using kable.\nUse only two significant digits for the numbers displayed in the tables.\nSubmit both the html and the qmd files using Git.\nYou will need the following libraries:\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(NHANES)\noptions(digits = 2)\n\n\nThe .qmd file must be able to render properly on the TFs’ computers. They will already have the necessary packages installed, so there is no need to include code for installing packages. Just focus on writing the code that uses these packages.\n\n\n\nExercises\n\nFilter the NHANES data to only include survey year 2011-2012. Save the resulting table in dat. This table should have 5,000 rows and 76 columns.\n\n\n## code here\n\n\nCompute the average and standard deviation (SD) for the combined systolic blood pressure (SBP) reading for males and females separately. Show us a data frame with two rows (female and male) and two columns (average and SD).\n\n\n## code here\n\n\nBecause of the large difference in the average between males and females, we will perform the rest of the analysis separately for males and females.\n\nCompute the average and SD for SBP for each race variable in column Race3 for females and males separately. The resulting table should have four columns for sex, race, average, and SD, respectively, and 12 rows (one for each strata). Arrange the result from highest to lowest average.\n\n## code here\n\n\nRepeat the previous exercise but add two columns to the final table to show a 95% confidence interval. Specifically, add columns with the lower and upper bounds of the interval with names lower and upper, respectively. The formula for these values is\n\n\\[\n\\bar{X} \\pm 1.96 \\, s / \\sqrt{n}\n\\] with \\(\\bar{X}\\) the sample average and \\(s\\) the sample standard deviation. This table will simply add two more columns to the table generated in the previous exercise: one column for the lower and upper bound, respectively.\n\n## code here\n\n\nMake a graph of showing the results from the previous exercise. Specifically, plot the averages for each group as points and confidence intervals as error bars (use the geometry geom_errorbar). Order the groups from lowest to highest average (the average of the males and females averages). Use facet_wrap to make a separate plot for females and males. Label your axes with Race and Average respectively, add the title Comparing systolic blood pressure across groups, and the caption Bars represent 95% confidence intervals.\n\n\n## code here\n\n\nIn the plot above we see that the confidence intervals don’t overlap when comparing the White and Mexican groups. We also see a substantial difference between Mexican and Hispnanic. Before concluding that there is a difference between groups, we will explore if differences in age, a very common confounder, explain the differences.\n\nCreate table like the one in the previous exercise but show the average SBP by sex and age group (AgeDecade). The the groups are order chronologically. As before make a separate plot for males and females. Make sure to filter our observations with no AgeDecade listed.\n\n## code here\n\n\nWe note that for both males and females the SBP increases with age. To explore if age is indeed a confounder we need to check if the groups have different age distributions.\n\nExplore the age distributions of each Race3 group to determine if the groups are comparable. Make a histogram of Age for each Race3 group and stack them vertically. Generate two columns of graphs for males and females, respectively. In the histograms, create bins increments of 5 years up to 80.\nBelow the graph, comment on what notice about the age distributions and how this can explain the difference between the White and Mexican groups.\n\n## code here\n\n\nSummarize the results shown in the graph by compute the median age for each Race3 group and the percent of individuals that are younger than 18. Order the rows by median age. The resulting data frame should have 6 rows (one for each group) and three columns to denote group, median age, and children respectively.\n\n\n## code here\n\nGiven these results provide an explanation for the difference in systolic pressure is lower when comparing the White and Mexican groups.\n\nWhen the age distribution between two populations we can’t conclude that there are differences in SBP based just on the population averages. The observed differences are likely due to age differences rather than genetic differences. We will therefore stratify by group and then compare SBP. But before we do this, we might need redefine dat to avoid small groups.\n\nWrite a function that computes the number of observations in each gender, age group and race combination. Show the groups with less than five observations. Make sure to remove the rows with no BPSysAve measurments before calculating the number of observations. Show a table with four columns representing gender, age strate, group, and the number of individuals in that group. Make sure to include combinations with 0 individuals (hint: use complete).\n\n## code here\n\n\nBased on the observations made in the previous exercise, we will redefine dat but with the following:\n\nAs before, include only survey year 2011-2012.\nRemove the observations with no age group reported.\nRemove the 0-9 age group.\nCombine the 60-69 and 70+ ageroups into a 60+ group.\nRemove observations reporting Other in Race3.\nRename the variable Race3 to Race.\n\nHints:\n\nNote that the levels in AgeDecade start with a space.\nYou can use the fct_collapse function in the forcats to combine factors.\n\n\n\n## code here\n\n\nCrete a plot that shows the averege BPS for each age decade. Show the different race groups with color and lines joining them. Generate a two plots, one for males and one for females.\n\n\n## code here\n\n\nBased on the plot above pick two groups that you think are consistently different and remake the plot from the previous exercise but just for these two groups, add confidence intervals, and remove the lines. Put the confidence intervals for each age strata next to each other and use color to represent the two groups. Comment on your finding.\n\n\n## code here\n\n\nFor the two groups that you selected above compute the difference in average BPS between the two groups for each age strata. Show a table with three columns representing age strata, difference for females, difference for males.\n\n\n## code here"
  },
  {
    "objectID": "psets/pset-05-dataviz.html",
    "href": "psets/pset-05-dataviz.html",
    "title": "Problem set 5",
    "section": "",
    "text": "In this problem set, we aim to use data visualization to explore the following questions:\n\nBased on SARS-Cov-2 cases, COVID-19 deaths and hospitalizations what periods defined the worst two waves of 2020-2021?\nDid states with higher vaccination rates experience lower COVID-19 death rates?\nWere there regional differences in vaccination rates?\n\nWe are not providing definitive answers to these questions but rather generating visualizations that may offer insights.\n\n\nWe will create a single data frame that contains relevant observations for each jurisdiction, for each Morbidity and Mortality Weekly Report (MMWR) period in 2020 and 2021. The key outcomes of interest are:\n\nSARS-CoV-2 cases\nCOVID-19 hospitalizations\nCOVID-19 deaths\nIndividuals receiving their first COVID-19 vaccine dose\nIndividuals receiving a booster dose\n\n\n\n\nYour task is divided into three parts:\n\nDownload the data: Retrieve population data from the US Census API and COVID-19 statistics from the CDC API.\nWrangle the data: Clean and join the datasets to create a final table containing all the necessary information.\nCreate visualizations: Generate graphs to explore potential insights into the questions posed above."
  },
  {
    "objectID": "psets/pset-05-dataviz.html#introduction",
    "href": "psets/pset-05-dataviz.html#introduction",
    "title": "Problem set 5",
    "section": "",
    "text": "In this problem set, we aim to use data visualization to explore the following questions:\n\nBased on SARS-Cov-2 cases, COVID-19 deaths and hospitalizations what periods defined the worst two waves of 2020-2021?\nDid states with higher vaccination rates experience lower COVID-19 death rates?\nWere there regional differences in vaccination rates?\n\nWe are not providing definitive answers to these questions but rather generating visualizations that may offer insights.\n\n\nWe will create a single data frame that contains relevant observations for each jurisdiction, for each Morbidity and Mortality Weekly Report (MMWR) period in 2020 and 2021. The key outcomes of interest are:\n\nSARS-CoV-2 cases\nCOVID-19 hospitalizations\nCOVID-19 deaths\nIndividuals receiving their first COVID-19 vaccine dose\nIndividuals receiving a booster dose\n\n\n\n\nYour task is divided into three parts:\n\nDownload the data: Retrieve population data from the US Census API and COVID-19 statistics from the CDC API.\nWrangle the data: Clean and join the datasets to create a final table containing all the necessary information.\nCreate visualizations: Generate graphs to explore potential insights into the questions posed above."
  },
  {
    "objectID": "psets/pset-05-dataviz.html#instructions",
    "href": "psets/pset-05-dataviz.html#instructions",
    "title": "Problem set 5",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a Git repository that includes the following directories:\n\ndata\ncode\nfigs\n\nInside the code directory, include the following files:\n\nfuncs.R\nwrangle.R\nanalysis.qmd\n\nThe figs directory should contain three PNG files, with each file corresponding to one of the figures you are asked to create.\n\nDetailed instructions follow for each of the tasks."
  },
  {
    "objectID": "psets/pset-05-dataviz.html#download-data",
    "href": "psets/pset-05-dataviz.html#download-data",
    "title": "Problem set 5",
    "section": "Download data",
    "text": "Download data\nFor this part we want the following:\n\nSave all your code in a file called wrangle.R that produces the final data frame.\nWhen executed, this code should save the final data frame in an RDA file in the data directory.\n\n\nCopy the relevant code from the previous homework to create the population data frame. Put this code in the the wrangle.R file in the code directory. Comment the code so we know where the population is create, where the regions are read in, and where we combine these.\nIn the previous problem set we wrote the following script to download cases data:\n\n\napi &lt;- \"https://data.cdc.gov/resource/pwn4-m3yp.json\"\ncases_raw &lt;- request(api) |&gt; \n  req_url_query(\"$limit\" = 10000000) |&gt;\n  req_perform() |&gt; \n  resp_body_json(simplifyVector = TRUE)\n\nWe are now going to download three other datasets from CDC that provide hospitalization, provisional COVID deaths, and vaccine data. A different endpoint is provided for each one, but the requests are the same otherwise. To avoid rewriting the same code more than once, write a function called get_cdc_data that receives and endpoint and returns a data frame. Save this code in a file called functions.R.\n\nUse the get_cdc Download the cases, hospitalization, deaths, and vaccination data and save the data frames. We recommend saving them into objects called: cases_raw, hosp_raw, deaths_raw, and vax_raw.\n\n\ncases - https://data.cdc.gov/resource/pwn4-m3yp.json\nhospitalizations - https://data.cdc.gov/resource/39z2-9zu6.json\ndeaths - https://data.cdc.gov/resource/r8kw-7aab.json\nvaccinations https://data.cdc.gov/resource/rh2h-3yt2.json\n\nWe recommend saving them into objects called: cases_raw, hosp_raw, deaths_raw, and vax_raw. Add the code to the wranling.R file. Add comments to describe we read in data here."
  },
  {
    "objectID": "psets/pset-05-dataviz.html#wrangling-challenge",
    "href": "psets/pset-05-dataviz.html#wrangling-challenge",
    "title": "Problem set 5",
    "section": "Wrangling Challenge",
    "text": "Wrangling Challenge\nIn this section, you will wrangle the files downloaded in the previous step into a single data frame containing all the necessary information. We recommend using the following column names: date, state, cases, hosp, deaths, vax, booster, and population.\n\nKey Considerations\n\nAlign reporting periods: Ensure that the time periods for which each outcome is reported are consistent. Specifically, calculate the totals for each Morbidity and Mortality Weekly Report (MMWR) period.\nHarmonize variable names: To facilitate the joining of datasets, rename variables so that they match across all datasets.\n\n\nOne challenge is data frames use different column names to represent the same variable. Examine each data frame and report back 1) the name of the column with state abbreviations, 2) if the it’s yearly, monthly, or weekly, daily data, 3) all the column names that provide date information.\n\n\n\n\nOutcome\nJurisdiction variable name\nRate\ntime variable names\n\n\n\n\ncases\n\n\n\n\n\nhospitalizations\n\n\n\n\n\ndeaths\n\n\n\n\n\nvaccines\n\n\n\n\n\n\n\nWrangle the cases data frame to keep state MMWR year, MMWR week, and the total number of cases for that week in that state. Keep only states for which we have population estimates. Hint: Use as_date, ymd_hms, epiweek and epiyear functions in the lubridate package. Comment appropriately.\nNow repeat the same exercise for hospitalizations. Note that you will have to collapse the data into weekly data and keep the same columns as in the cases dataset, except keep total weekly hospitalizations instead of cases. Remove weeks with less than 7 days reporting. Add this code to wrangle.R and comment appropriately.\nRepeat what you did in the previous two exercises for provisional COVID-19 deaths. Add this code to wrangle.R and comment appropriately.\nRepeat this now for vaccination data. Keep the variables series_complete and booster along with state and date. Add this code to wrangle.R and comment appropriately.\nNow we are ready to join the tables. We will only consider 2020 and 2021 as we don’t have population sizes for 2022. However, because we want to guarantee that all dates are included we will create a data frame with all possible weeks. Add this code to your wrangle.R file. We can use this:\n\n\n## Make dates data frame\nall_dates &lt;- data.frame(date = seq(make_date(2020, 1, 25),\n                                   make_date(2021, 12, 31), \n                                   by = \"week\")) |&gt;\n  mutate(date = ceiling_date(date, unit = \"week\", week_start = 7) - days(1)) |&gt;\n  mutate(mmwr_year = epiyear(date), mmwr_week = epiweek(date)) \n\ndates_and_pop &lt;- cross_join(all_dates, data.frame(state = unique(population$state))) |&gt; left_join(population, by = c(\"state\", \"mmwr_year\" = \"year\"))\n\nNow join all the table to create your final table. Make sure it is ordered by date within each state. Call it dat and save an RDS file to the data directory. Add this code to wrangle.R and comment appropriately."
  },
  {
    "objectID": "psets/pset-05-dataviz.html#data-visualization-generate-some-plots",
    "href": "psets/pset-05-dataviz.html#data-visualization-generate-some-plots",
    "title": "Problem set 5",
    "section": "Data visualization generate some plots",
    "text": "Data visualization generate some plots\nWe are now ready to create some figures. In the analysis.qmd file create a section for each figure. You should load the dat object stored in the RDS file in the dat directory.\nYou can call these sections Figure 1, Figure 2, and so on. Inlcude a short description of what the figure is before the code chunk. The rendered file should show both the code and figure.\n\nPlot a trend plot for cases, hospitalizations and deaths. Plot rates per \\(100,000\\) people. Place the plots on top of each other. Hint: Use pivot_longer and facet_wrap.\nTo determine when vaccination started and when most of the population was vaccinated, compute the percent of the US population (including DC and Puerto Rico) were vaccinated by date. Do the same for the booster. Then plot both percentages.\nDescribe the distribution of vaccination rates on July 1, 2021.\nIs there a difference across region? Discuss what the plot shows?\nUsing the two previous figures, identify two time periods that meet the following criteria:\n\n\nA significant COVID-19 wave occurred across the United States.\nA sufficient number of people had been vaccinated.\n\nNext, follow these steps:\n\nFor each state, calculate the COVID-19 deaths per day per 100,000 people during the selected time period.\nDetermine the vaccination rate (primary series) in each state as of the last day of the period.\nCreate a scatter plot to visualize the relationship between these two variables:\n\nThe x-axis should represent the vaccination rate.\nThe y-axis should represent the deaths per day per 100,000 people.\n\n\n\nRepeat the exercise for the booster."
  },
  {
    "objectID": "psets/pset-06-prob.html",
    "href": "psets/pset-06-prob.html",
    "title": "Problem set 6",
    "section": "",
    "text": "Please answer each of the exercises below. For those asking for a mathematical calculation please use LaTeX to show your work.\nImportant: Make sure that your document renders in less than 5 minutes.\n\nWrite a function called same_birthday that takes a number n as an argument, randomly generates n birthdays and returns TRUE if two or more birthdays are the same. You can assume nobody is born on February 29.\n\nHint: use the functions sample, duplicated, and any.\n\nsame_birthday &lt;- function(n){ \n  ## Your code here\n} \n\n\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Use a Monte Carlo simulation with $B=$1,000 trials based on the function same_birthday from the previous exercises.\n\n\nB &lt;- 10^3\n## Your code here\n\n\nRedo the previous exercises for several values on n to determine at what group size do the chances become greater than 50%. Set the seed at 1997.\n\n\nset.seed(1997)\ncompute_prob &lt;- function(n, B = 10^3){ \n ## Your code here\n} \n## Your code here\n\n\nThese probabilities can be computed exactly instead of relying on Monte Carlo approximations. We use the multiplication rule:\n\n\\[\n\\mbox{Pr}(n\\mbox{ different birthdays}) = 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nPlot the probabilities you obtained using Monte Carlos as a points and the exact probabilities with a red line.\nHint: use the function prod to compute the exact probabilities.\n\nexact_prob &lt;- function(n){ \n ## Your code here\n} \n## Your code here\n\n\nNote that the points don’t quite match the red line. This is because our Monte Carlos simulation was based on only 1,000 iterations. Repeat exercise 2 but for n = 23 and try B &lt;- seq(10, 250, 5)^2 number iterations. Plot the estimated probability against sqrt(b). Describe when it starts to stabilize in that the estimates are within 0.005 for the exact probability. Add horizontal lines around the exact probability \\(\\pm\\) 0.005. Note this could take several seconds to run. Set the seed to 1998.\n\n\nset.seed(1998)\nB &lt;- seq(10, 250, 5)^2\n## Your code here\n\n\nRepeat exercise 4 but use the the results of exercise 5 to select the number of iterations so that the points practically fall on the red curve.\n\nHint: If the number of iterations you chose is too large, you will achieve the correct plot but your document might not render in less than five minutes.\n\nn &lt;- seq(1,60) \n## Your code here\n\n\nIn American Roulette, with 18 red slots, 18 black slots, and 2 green slots (0 and 00), what is the probability of landing on a green slot?\n\n\\[\n\\mbox{Derivation here}\n\\]\n\nThe payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. If it lands on red or black you lose your dollar. Create a sampling model using sample to simulate the random variable \\(X\\) for the Casino’s winnings.\n\n\nn &lt;- 1\n## Your code here\n\n\nNow create a random variable \\(S\\) of the Casino’s total winnings if $n = $1,000 people bet on green. Use Monte Carlo simulation to estimate the probability that the Casino loses money.\n\n\nn &lt;- 1000\n## Your code here\n\n\nWhat is the expected value of \\(X\\)?\n\n\\[\n\\mbox{Your derivation here.}\n\\]\n\nWhat is the standard error of \\(X\\)?\n\n\\[\n\\mbox{Your derivation here.}\n\\]\n\nWhat is the expected value of \\(S\\)? Does the Monte Carlo simulation confirm this?\n\n\\[\n\\mbox{Your dereviation here}\n\\]\n\n## Your code here\n\n\nWhat is the standard error of \\(S\\)? Does the Monte Carlos simulation confirm this?\n\n\\[\n\\mbox{Your derivation here.}\n\\]\n\n## Your code here\n\n\nUse data visualization to convince yourself that the distribution of \\(S\\) is approximately normal. Make a histogram and a QQ-plot of standardized values of \\(S\\). The QQ-plot should be on the identity line.\n\n\n## Your code here\n\n\nNotice that the normal approximation is slightly off for the tails of the distribution. What would make this better? Increasing the number of people playing \\(n\\) or the number of Monte Carlo iterations \\(B\\)?\n\nAnswer here\n\nNow approximate the probability estimated using CLT. Does it agree with the Monte Carlo simulation?\n\n\\[\n\\mbox{Your derivation here.}\n\\]\n\n## Your code here\n\n\nHow many people \\(n\\) must bet on green for the Casino to reduce the probability of losing money to 1%. Check your answer with a Monte Carlo simulation.\n\n\\[\n\\mbox{Your derivation here.}\n\\]\n\n## Your code here"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Topic\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nFirst day\n\n\nIntroduction\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nUnix\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nRStudio\n\n\nWed, Sep 04\n\n\n\n\nProductivity Tools\n\n\nQuarto\n\n\nMon, Sep 09\n\n\n\n\nProductivity Tools\n\n\nGit and GitHub\n\n\nMon, Sep 09\n\n\n\n\nR\n\n\nR Basics\n\n\nMon, Sep 16\n\n\n\n\nR\n\n\nVectorization\n\n\nWed, Sep 18\n\n\n\n\nR\n\n\nTidyverse\n\n\nMon, Sep 23\n\n\n\n\nR\n\n\nggplot2\n\n\nMon, Sep 23\n\n\n\n\nR\n\n\nTidying data\n\n\nWed, Sep 25\n\n\n\n\nWrangling\n\n\nIntroduction to Wrangling\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nImporting files\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nDates And Times\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nLocales\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nData APIs\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nWeb Scraping\n\n\nMon, Sep 30\n\n\n\n\nWrangling\n\n\nJoining Tables\n\n\nMon, Sep 30\n\n\n\n\nData Visualization\n\n\nData Visualization Principles\n\n\nMon, Oct 07\n\n\n\n\nData Visualization\n\n\nDistributions\n\n\nWed, Oct 09\n\n\n\n\nData Visualization\n\n\nDataviz In Practice\n\n\nWed, Oct 09\n\n\n\n\nProbability\n\n\nIntroduction to Probability\n\n\nMon, Oct 21\n\n\n\n\nProbability\n\n\nFoundations of Statistical Inference\n\n\nMon, Oct 21\n\n\n\n\nInference\n\n\nIntroduction to Statistical Inference and Models\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nParameters and Estimates\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nConfidence Intervals\n\n\nWed, Oct 23\n\n\n\n\nInference\n\n\nData-driven models\n\n\nMon, Oct 28\n\n\n\n\nInference\n\n\nBayesian Models\n\n\nWed, Oct 30\n\n\n\n\nInference\n\n\nHierarchical Models\n\n\nWed, Oct 30\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "psets.html",
    "href": "psets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "Topic\n\n\nDue date (at 11:59 PM)\n\n\n\n\n\n\nProblem set 1\n\n\nWed, Sep 11\n\n\n\n\nProblem set 2\n\n\nThu, Sep 19\n\n\n\n\nProblem set 3\n\n\nFri, Sep 27\n\n\n\n\nProblem set 4\n\n\nFri, Oct 04\n\n\n\n\nProblem set 5\n\n\nFri, Oct 11\n\n\n\n\nProblem set 6\n\n\nFri, Oct 25\n\n\n\n\nProblem set 7\n\n\nMon, Nov 04\n\n\n\n\nProblem set 8\n\n\nFri, Nov 15\n\n\n\n\nProblem set 9\n\n\nFri, Nov 22\n\n\n\n\nProblem set 10\n\n\nMon, Dec 16\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Problem Sets"
    ]
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#visualizing-data-distributions",
    "href": "slides/dataviz/18-distributions.html#visualizing-data-distributions",
    "title": "Distributions",
    "section": "Visualizing data distributions",
    "text": "Visualizing data distributions\n\nSummarizing complex datasets is crucial in data analysis, allowing us to share insights drawn from the data more effectively.\nOne common method is to use the average value to summarize a list of numbers.\nFor instance, a high school’s quality might be represented by the average score in a standardized test.\nSometimes, an additional value, the standard deviation, is added."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#visualizing-data-distributions-1",
    "href": "slides/dataviz/18-distributions.html#visualizing-data-distributions-1",
    "title": "Distributions",
    "section": "Visualizing data distributions",
    "text": "Visualizing data distributions\n\nSo, a report might say the scores were 680 \\(\\pm\\) 50, boiling down a full set of scores to just two numbers.\nBut is this enough? Are we overlooking crucial information by relying solely on these summaries instead of the complete data?\nOur first data visualization building block is learning to summarize lists of numbers or categories.\nMore often than not, the best way to share or explore these summaries is through data visualization."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#visualizing-data-distributions-2",
    "href": "slides/dataviz/18-distributions.html#visualizing-data-distributions-2",
    "title": "Distributions",
    "section": "Visualizing data distributions",
    "text": "Visualizing data distributions\n\nThe most basic statistical summary of a list of objects or numbers is its distribution.\nOnce a data has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.\nUnderstanding distributions is therefore essential for creating useful data visualizations.\nNote: understanding distributions is also essential for understanding inference and statistical models"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#case-study-describing-student-heights",
    "href": "slides/dataviz/18-distributions.html#case-study-describing-student-heights",
    "title": "Distributions",
    "section": "Case study: describing student heights",
    "text": "Case study: describing student heights\n\nPretend that we have to describe the heights of our classmates to someone that has never seen humans.\nWe ask students to report their height in inches.\nWe also ask them to report sex because there are two different height distributions.\n\n\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#case-study",
    "href": "slides/dataviz/18-distributions.html#case-study",
    "title": "Distributions",
    "section": "Case study",
    "text": "Case study\n\nOne way to convey the heights to ET is to simply send him this list of 1,050 heights.\nBut there are much more effective ways to convey this information, and understanding the concept of a distribution will be key.\nTo simplify the explanation, we first focus on male heights.\nWe examine the female height data later."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#distributions",
    "href": "slides/dataviz/18-distributions.html#distributions",
    "title": "Distributions",
    "section": "Distributions",
    "text": "Distributions\n\nThe most basic statistical summary of a list of objects or numbers is its distribution.\nFor example, with categorical data, the distribution simply describes the proportion of each unique category:\n\n\n\n\nFemale   Male \n 0.227  0.773"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#distributions-1",
    "href": "slides/dataviz/18-distributions.html#distributions-1",
    "title": "Distributions",
    "section": "Distributions",
    "text": "Distributions\n\nTo visualize this we simply use a barplot.\nHere is an example with US state regions:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#histograms",
    "href": "slides/dataviz/18-distributions.html#histograms",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\n\nWhen the data is numerical, the task of displaying distributions is more challenging.\nWhen data is not categorical, reporting the frequency of each entry, as we did for categorical data, is not an effective summary since most entries are unique.\nFor example, in our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#histograms-1",
    "href": "slides/dataviz/18-distributions.html#histograms-1",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\n\nA more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\).\nThis function is called the empirical cumulative distribution function (eCDF), it can be plotted, and it provides a full description of the distribution of our data."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#histograms-2",
    "href": "slides/dataviz/18-distributions.html#histograms-2",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\n\nHere is the eCDF for male student heights:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#histograms-3",
    "href": "slides/dataviz/18-distributions.html#histograms-3",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\n\nHowever, summarizing data by plotting the eCDF is actually not very popular in practice.\nThe main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values?\nHistograms sacrifice just a bit of information to produce plots that are much easier to interpret."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#histograms-4",
    "href": "slides/dataviz/18-distributions.html#histograms-4",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\nHere is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5]\\), \\((50.5, 51.5]\\), \\((51.5,52.5]\\), \\((52.5,53.5]\\), \\(...\\), \\((82.5,83.5]\\).\n\n\nFrom this plot one immediately learn some important properties about our data."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#smoothed-density",
    "href": "slides/dataviz/18-distributions.html#smoothed-density",
    "title": "Distributions",
    "section": "Smoothed density",
    "text": "Smoothed density\n\nSmooth density plots relay the same information as a histogram but are aesthetically more appealing:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#smoothed-density-1",
    "href": "slides/dataviz/18-distributions.html#smoothed-density-1",
    "title": "Distributions",
    "section": "Smoothed density",
    "text": "Smoothed density\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed.\nThe scale of the y-axis changed from counts to density. Values shown y-axis are chosen so that the area under the curve adds up to 1.\nTo fully understand smooth densities, we have to understand estimates, a concept we cover later in the course."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#smoothed-density-2",
    "href": "slides/dataviz/18-distributions.html#smoothed-density-2",
    "title": "Distributions",
    "section": "Smoothed density",
    "text": "Smoothed density\n\nHere is an example comparing male and female heights:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nHistograms and density plots provide excellent summaries of a distribution.\nBut can we summarize even further?\nWe often see the average and standard deviation used as summary statistics\nTo understand what these summaries are and why they are so widely used, we need to understand the normal distribution."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution-1",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution-1",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nThe normal distribution, also known as the bell curve and as the Gaussian distribution."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution-2",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution-2",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nMany datasets can be approximated with normal distributions.\nThese include gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors.\nBut how can the same distribution approximate datasets with completely different ranges for values?"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution-3",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution-3",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nThe normal distribution can be adapted to different datasets by just adjusting two numbers, referred to as the average or mean and the standard deviation (SD).\nBecause we only need two numbers to adapt the normal distribution to a dataset implies that if our data distribution is approximated by a normal distribution, all the information needed to describe the distribution can be encoded by just two numbers.\nA normal distribution with average 0 and SD 1 is referred to as a standard normal."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution-4",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution-4",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nFor a list of numbers contained in a vector x:\n\n\nindex &lt;- heights$sex == \"Male\" \nx &lt;- heights$height[index] \n\n\nthe average is defined as.\n\n\nm &lt;- sum(x) / length(x) # or mean(x)\n\n\nand the SD is defined as:\n\n\ns &lt;- sqrt(sum((x - m)^2) / length(x)) # or sd(x)"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#the-normal-distribution-5",
    "href": "slides/dataviz/18-distributions.html#the-normal-distribution-5",
    "title": "Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\n\nHere is a plot of our male student height smooth density (blue) and the normal distribution (black) with mean = 69.3 and SD = 3.6:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#boxplot",
    "href": "slides/dataviz/18-distributions.html#boxplot",
    "title": "Distributions",
    "section": "Boxplot",
    "text": "Boxplot\n\nSuppose we want to summarize the murder rate distribution."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#boxplots",
    "href": "slides/dataviz/18-distributions.html#boxplots",
    "title": "Distributions",
    "section": "Boxplots",
    "text": "Boxplots\n\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.\nBut what if we want a more compact numerical summary?\nTwo summaries will not suffice here because the data is not normal."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#boxplots-1",
    "href": "slides/dataviz/18-distributions.html#boxplots-1",
    "title": "Distributions",
    "section": "Boxplots",
    "text": "Boxplots\n\nThe boxplot provides a five-number summary composed of the range (the minimum and maximum) along with the quartiles (the 25th, 50th, and 75th percentiles).\nThe R implementation of boxplots ignore outliers when computing the range and instead plot these as independent points.\nThe help file provides a specific definition of outliers."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#boxplots-2",
    "href": "slides/dataviz/18-distributions.html#boxplots-2",
    "title": "Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nThe boxplot sumarizes with a box with whiskers:\n\n\n\n\n\n\n\n\n\n\n\n\nFrom just this simple plot, we know that:\n\nthe median is about 2.5,\nthat the distribution is not symmetric, and that\nthe range is 0 to 5 for the great majority of states with two exceptions."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#boxplots-3",
    "href": "slides/dataviz/18-distributions.html#boxplots-3",
    "title": "Distributions",
    "section": "Boxplots",
    "text": "Boxplots\n\nIn data analysis we often divide observations into groups based on the values of one or more variables associated with those observations.\nWe call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distributions of variables differ across different subgroups.\nStratifying and then making boxplot is a common approach to visualizing these differences."
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#case-study-continued",
    "href": "slides/dataviz/18-distributions.html#case-study-continued",
    "title": "Distributions",
    "section": "Case study continued",
    "text": "Case study continued\nHere are the heights for men and women:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#case-study-continued-1",
    "href": "slides/dataviz/18-distributions.html#case-study-continued-1",
    "title": "Distributions",
    "section": "Case study continued",
    "text": "Case study continued\n\nThe plot immediately reveals that males are, on average, taller than females.\nHowever, exploratory plots reveal that the approximation is not as useful:"
  },
  {
    "objectID": "slides/dataviz/18-distributions.html#case-study-continued-2",
    "href": "slides/dataviz/18-distributions.html#case-study-continued-2",
    "title": "Distributions",
    "section": "Case study continued",
    "text": "Case study continued\n\nA likely for the second bump is that female as the default in the reporting tool.\nThe unexpected five smallest values are likely cases of 5'x'' reported as 5x\n\n\n\n[1] 51 53 55 52 52"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability",
    "href": "slides/prob/20-intro-to-prob.html#probability",
    "title": "Introduction to Probability",
    "section": "Probability",
    "text": "Probability\n\nThe term probability is used in everyday language.\nYet answering questions about probability is often hard, if not impossible.\nIn contrast probability has a very intuitive definition in games of chance.\nToday we discuss a mathematical definition of probability that allows us to give precise answers to certain questions."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-1",
    "href": "slides/prob/20-intro-to-prob.html#probability-1",
    "title": "Introduction to Probability",
    "section": "Probability",
    "text": "Probability\n\nProbability Theory was born because certain mathematcial computations can give an advantage in games of chance.\nProbability continues to be highly useful in modern games of chance.\nProbability theory is also useful whenever our data is affected by chance in some manner.\nA knowledge of probability is indispensable for addressing most data analysis challenges."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-2",
    "href": "slides/prob/20-intro-to-prob.html#probability-2",
    "title": "Introduction to Probability",
    "section": "Probability",
    "text": "Probability\n\nIn today’s lecture, we will use casino games to illustrate the fundamental concepts.\nInstead of diving into the mathematical theories, we will uses R to demonstrate these concepts.\nUnderstanding the connection between probability theory and real world data analysis is a bit more challenging. We will be discussing this connection throughout the rest of the course."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nEvents are fundamental concepts that help us understand and quantify uncertainty in various situations.\nAn event is defined as a specific outcome or a collection of outcomes from a random experiment."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation-1",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation-1",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nSimple examples of events can be constructed with urns.\nIf we have 2 red beads and 3 blue beads inside an urn, and we perform the random experiment of picking 1 bead, there are two outcomes: bead is red or blue.\n\n\nbeads &lt;- rep( c(\"red\", \"blue\"), times = c(2,3))\n\n\nThe four possible events: bead is red, bead is blue, bead is red or blue, and an event with no outcomes."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation-2",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation-2",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nIn more complex random experiment, we can define many more events.\nFor example if the random experiment is picking 2 beads, we can define events such as first bead is red, second bead is blue, both beads are red, and so on.\nIn a random experiment such as political poll, where we randomly phone 100 likely voters at random, we can form many million events, for example calling 48 Democrats and 52 Republicans."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation-3",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation-3",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nWe usually use capital letters \\(A\\), \\(B\\), \\(C\\), … to to denote events.\nIf we denote an event as \\(A\\) then we use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) occurring."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation-4",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation-4",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nWe can combine events in different ways to form new events. For example, if event\n\\(A\\)=first bead is red and second bead is blue, and\n\\(B\\)=first bead is red and second bead is red\nthen \\(A \\cup B\\) (\\(A\\) or \\(B\\)) is the event first bead is red,\nwhile \\(A \\cap B\\) (\\(A\\) and \\(B\\)) is the empty event since both can’t happen."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#definitions-and-notation-5",
    "href": "slides/prob/20-intro-to-prob.html#definitions-and-notation-5",
    "title": "Introduction to Probability",
    "section": "Definitions and Notation",
    "text": "Definitions and Notation\n\nWith continuous variables, events will relate to questions, such as Is this person taller than 6 feet?\nIn these cases, we represent events in a more mathematical form: \\(A = X &gt; 6\\)."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#independence",
    "href": "slides/prob/20-intro-to-prob.html#independence",
    "title": "Introduction to Probability",
    "section": "Independence",
    "text": "Independence\n\nMany examples of events that are not independent come from card games.\nWhen we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace.\nIf we deal a King for the first card, the probability of a second card being a King decreases because there are only three Kings left: The probability is 3 out of 51."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#independence-1",
    "href": "slides/prob/20-intro-to-prob.html#independence-1",
    "title": "Introduction to Probability",
    "section": "Independence",
    "text": "Independence\n\nBy detault, the sample function samples without replacement\n\n\nset.seed(1996)\nx &lt;- sample(beads, 5) \n\n\nIf you have to guess the color of the first bead, you will predict blue since blue has a 60% chance."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#independence-2",
    "href": "slides/prob/20-intro-to-prob.html#independence-2",
    "title": "Introduction to Probability",
    "section": "Independence",
    "text": "Independence\n\nHowever, if I show you the result of the last four outcomes:\n\n\nx[2:5] \n\n[1] \"blue\" \"blue\" \"red\"  \"blue\"\n\n\n\nwould you still guess blue? Of course not."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#conditional-probabilities",
    "href": "slides/prob/20-intro-to-prob.html#conditional-probabilities",
    "title": "Introduction to Probability",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nWhen events are not independent, conditional probabilities are useful.\nWe use the \\(|\\) to shorten conditional on. For example:\n\n\\[\n\\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#conditional-probabilities-1",
    "href": "slides/prob/20-intro-to-prob.html#conditional-probabilities-1",
    "title": "Introduction to Probability",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nWhen two events, say \\(A\\) and \\(B\\), are independent, we have:\n\n\\[\n\\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A)  \n\\]\n\nIn fact, this can be considered the mathematical definition of independence."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule",
    "title": "Introduction to Probability",
    "section": "Multiplication rule",
    "text": "Multiplication rule\n\nIf we want to determine the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule:\n\n\\[\n\\mbox{Pr}(A \\cup B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule-1",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule-1",
    "title": "Introduction to Probability",
    "section": "Multiplication rule",
    "text": "Multiplication rule\n\nFor example:\n\n\\[\n\\mbox{Pr}(\\mbox{Blackjack in first hand}) = \\\\\n\\mbox{Pr}(\\mbox{Ace first})\\mbox{Pr}(\\mbox{Face card second}\\mid \\mbox{Ace first}) +\\\\\n\\mbox{Pr}(\\mbox{Face card first})\\mbox{Pr}(\\mbox{Ace}\\mid \\mbox{Face card second}) =\\\\\n\\frac{1}{13}\\frac{16}{51} + \\frac{4}{13}\\frac{4}{51} \\approx 0.0483\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule-2",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule-2",
    "title": "Introduction to Probability",
    "section": "Multiplication rule",
    "text": "Multiplication rule\n\nWe can use induction to expand for more events:\n\n\\[\n\\mbox{Pr}(A \\cup B \\cup C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\cup B)\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule-3",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule-3",
    "title": "Introduction to Probability",
    "section": "Multiplication rule",
    "text": "Multiplication rule\n\nWhen dealing with independent events, the multiplication rule becomes simpler:\n\n\\[\n\\mbox{Pr}(A \\cup B \\cup C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C)\n\\]\n\nHowever, we have to be very careful before using this version of the multiplication rule, since assuming independence can result in very different and incorrect probability calculations when events are not actually independent."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule-example",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule-example",
    "title": "Introduction to Probability",
    "section": "Multiplication rule example",
    "text": "Multiplication rule example\n\nImagine a court case in which the suspect was described as having a mustache and a beard.\nThe defendant has both and an “expert” testifies that 1/10 men have beards and 1/5 have mustaches.\nUsing the multiplication rule, he concludes that \\(1/10 \\times 1/5\\) or 0.02 have both.\nBut this assumes independence!\nIf the conditional probability of a man having a mustache, conditional on him having a beard, is .95, then the probability is: \\(1/10 \\times 95/100 = 0.095\\)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#multiplication-rule-under",
    "href": "slides/prob/20-intro-to-prob.html#multiplication-rule-under",
    "title": "Introduction to Probability",
    "section": "Multiplication rule under",
    "text": "Multiplication rule under\n\nThe multiplication rule also gives us a general formula for computing conditional probabilities:\n\n\\[\n\\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\cup B)}{ \\mbox{Pr}(A)}\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#addition-rule",
    "href": "slides/prob/20-intro-to-prob.html#addition-rule",
    "title": "Introduction to Probability",
    "section": "Addition rule",
    "text": "Addition rule\n\nThe addition rule tells us that:\n\n\\[\n\\mbox{Pr}(A \\cap B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\cup B)\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#random-variables",
    "href": "slides/prob/20-intro-to-prob.html#random-variables",
    "title": "Introduction to Probability",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables are numeric outcomes resulting from random processes.\nWe can easily generate random variables using the simple examples we have shown.\nFor example, define X to be 1 if a bead is blue and red otherwise:\n\n\nbeads &lt;- rep( c(\"red\", \"blue\"), times = c(2,3))\nx &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#random-variables-1",
    "href": "slides/prob/20-intro-to-prob.html#random-variables-1",
    "title": "Introduction to Probability",
    "section": "Random Variables",
    "text": "Random Variables\n\nHere X is a random variable, changing randomly each time we select a new bead. Sometimes it’s 1 and sometimes it’s 0.\n\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 1\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#random-variables-2",
    "href": "slides/prob/20-intro-to-prob.html#random-variables-2",
    "title": "Introduction to Probability",
    "section": "Random Variables",
    "text": "Random Variables\n\nMore interesting random variables are:\n\nthe number of times we win in a game of chance,\nthe number of democrats in a random sample of 1,000 voters, and\nthe proportion of patients randomly assigned to a control group in drug trial."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#discrete-probability",
    "href": "slides/prob/20-intro-to-prob.html#discrete-probability",
    "title": "Introduction to Probability",
    "section": "Discrete probability",
    "text": "Discrete probability\nIf I have 2 red beads and 3 blue beads inside an urn and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#discrete-probability-1",
    "href": "slides/prob/20-intro-to-prob.html#discrete-probability-1",
    "title": "Introduction to Probability",
    "section": "Discrete probability",
    "text": "Discrete probability\n\nA precise definition can be given by noting that there are five possible outcomes, of which two satisfy the condition necessary for the event pick a red bead.\nSince each of the five outcomes has an equal chance of occurring, we conclude that the probability is .4 for red and .6 for blue."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#discrete-probability-2",
    "href": "slides/prob/20-intro-to-prob.html#discrete-probability-2",
    "title": "Introduction to Probability",
    "section": "Discrete probability",
    "text": "Discrete probability\n\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.\nThis is the frequentist way of thinking about probability."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nMonte Carlo simulations use computers to perform these experiments.\nRandom number generators permit us to mimic the process of picking at random.\nThe sample function in R uses a random number generator:\n\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\nsample(beads, 1)\n\n[1] \"blue\""
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-1",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-1",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nIf we repeat the experiment over and over, we can define the probability using the frequentists definition\n\n\nn &lt;- 10^7\nx &lt;- sample(beads, n, replace = TRUE)\ntable(x)/n\n\nx\n     blue       red \n0.6002232 0.3997768 \n\n\n\nNote the definition is for \\(n=\\infty\\). In practice we use very large numbers tp get very close."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-distributions",
    "href": "slides/prob/20-intro-to-prob.html#probability-distributions",
    "title": "Introduction to Probability",
    "section": "Probability distributions",
    "text": "Probability distributions\nAn example of a probability distribution is:\n\n\n\nPr(picking a Republican)\n=\n0.44\n\n\nPr(picking a Democrat)\n=\n0.44\n\n\nPr(picking an undecided)\n=\n0.10\n\n\nPr(picking a Green)\n=\n0.02"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#setting-the-random-seed",
    "href": "slides/prob/20-intro-to-prob.html#setting-the-random-seed",
    "title": "Introduction to Probability",
    "section": "Setting the random seed",
    "text": "Setting the random seed\n\nBefore we continue, we will briefly explain the function set.seed\n\n\nset.seed(2020-10-13)  \n\n\nWhen using random number generators you get a different answer each time.\nThis is fine, but if you want to ensure that results are consistent with each run, you can set R’s random number generation seed to a specific number."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#combinations-and-permutations",
    "href": "slides/prob/20-intro-to-prob.html#combinations-and-permutations",
    "title": "Introduction to Probability",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\n\nBeing able to count combinations and permutations is an important part of performing discrete probability computations.\nWe will not cover this but you should know the function expand.grid\nand the gtools functions permutatios and combinations."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-1",
    "href": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-1",
    "title": "Introduction to Probability",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\n\nHere is how we generate a deck of cards:\n\n\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\") \nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",  \n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\") \ndeck &lt;- expand.grid(number = numbers, suit = suits) \ndeck &lt;- paste(deck$number, deck$suit)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-2",
    "href": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-2",
    "title": "Introduction to Probability",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\n\nHere are all the ways we can choose two numbers from a list consisting of 1,2,3:\n\n\nlibrary(gtools) \npermutations(3, 2) \n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\n\nThe order matters here: 3,1 is different than 1,3.\n(1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-3",
    "href": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-3",
    "title": "Introduction to Probability",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\n\nTo compute all possible ways we can choose two cards when the order matters, we type, you can use the v option:\n\n\nhands &lt;- permutations(52, 2, v = deck)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-4",
    "href": "slides/prob/20-intro-to-prob.html#combinations-and-permutations-4",
    "title": "Introduction to Probability",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\n\nWhat about if the order does not matter? For example, in Blackjack, if you obtain an Ace and a face card in the first draw, it is called a Natural 21, and you win automatically.\nIf we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.\n\n\ncombinations(3,2) \n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#infinity-in-practice",
    "href": "slides/prob/20-intro-to-prob.html#infinity-in-practice",
    "title": "Introduction to Probability",
    "section": "Infinity in practice",
    "text": "Infinity in practice\n\nThe theory described here requires repeating experiments over and over indefinitely.\nIn practice, we can’t do this.\nIn the problem set you will be asked to explore how we implement asymptotic theory in practice."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#continuous-probability",
    "href": "slides/prob/20-intro-to-prob.html#continuous-probability",
    "title": "Introduction to Probability",
    "section": "Continuous probability",
    "text": "Continuous probability\n\nWhen summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome.\nSimilarly, for a random variable that can take any value in a continuous set, it impossible to assign a positive probabilities to the infinite number of possible values.\nHere, we outline the mathematical definitions of distributions for continuous random variables and useful approximations frequently employed in data analysis."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#cdf",
    "href": "slides/prob/20-intro-to-prob.html#cdf",
    "title": "Introduction to Probability",
    "section": "CDF",
    "text": "CDF\n\nWe used the heights of adult male students as an example:\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \nx &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% pull(height) \n\n\nand defined the empirical cumulative distribution function (eCDF) as.\n\n\nF &lt;- function(a) mean(x &lt;= a) \n\n\nwhich, for any value a, gives the proportion of values in the list x that are smaller or equal than a."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#cdf-1",
    "href": "slides/prob/20-intro-to-prob.html#cdf-1",
    "title": "Introduction to Probability",
    "section": "CDF",
    "text": "CDF\n\nThere is a connection to the empirical CDF.\nIf I randomly pick one of the male students, what is the chance that he is taller than 70.5 inches?\nSince every student has the same chance of being picked, the answer is the proportion of students that are taller than 70.5 inches."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#cdf-2",
    "href": "slides/prob/20-intro-to-prob.html#cdf-2",
    "title": "Introduction to Probability",
    "section": "CDF",
    "text": "CDF\n\nUsing the eCDF we obtain an answer by typing:\n\n\n1 - F(70) \n\n[1] 0.3768473\n\n\n\nThe CDF is a version of the eCDF that assigns theoretical probabilities for each \\(a\\) rather than proportions computed from data."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#cdf-3",
    "href": "slides/prob/20-intro-to-prob.html#cdf-3",
    "title": "Introduction to Probability",
    "section": "CDF",
    "text": "CDF\n\nAlthough, as we just demonstrated, proportions computed from data can be used to define probabilities for a random variable.\nSpecifically, the CDF for a random outcome \\(X\\) defines, for any number \\(a\\), the probability of observing a value larger than \\(a\\).\n\n\\[ F(a) = \\mbox{Pr}(X \\leq a) \\]\n\nOnce a CDF is defined, we can use it to compute the probability of any subset of values."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#cdf-4",
    "href": "slides/prob/20-intro-to-prob.html#cdf-4",
    "title": "Introduction to Probability",
    "section": "CDF",
    "text": "CDF\n\nFor instance, the probability of a student being between height a and height b is:\n\n\\[\n\\mbox{Pr}(a &lt; X \\leq b) = F(b)-F(a)\n\\]\n\nSince we can compute the probability for any possible event using this approach, the CDF defines the probability distribution."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nA mathematical result that is very useful in practice is that, for most CDFs, we can define a function, call it \\(f(x)\\), that permits us to construct the CDF using Calculus, like this:\n\n\\[\nF(b) - F(a) = \\int_a^b f(x)\\,dx\n\\]\n\n\\(f(x)\\) is referred to as the probability density function."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function-1",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function-1",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nThe intuition is that even for continuous outcomes we can define tiny intervals, that are almost as small as points, that have positive probabilities.\nIf we think of the size of these intervals as the base of a rectangle, the probability density function \\(f\\) determines the height of the rectangle so that the summing up of the area of these rectangles approximate the probability \\(F(b) - F(a)\\)."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function-2",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function-2",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nThis sum can be written as Reimann sum that is approximated by an integral:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function-3",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function-3",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nAn example of such a continuous distribution is the normal distribution.\nThe probability density function is given by:\n\n\\[f(x) = e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\]\n\nThe cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function-4",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function-4",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nWe say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:\n\n\nF(a) = pnorm(a, m, s)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#probability-density-function-5",
    "href": "slides/prob/20-intro-to-prob.html#probability-density-function-5",
    "title": "Introduction to Probability",
    "section": "Probability density function",
    "text": "Probability density function\n\nThis is useful because, if we are willing to use the normal approximation we don’t need the entire dataset to answer questions such as: What is the probability that a randomly selected student is taller then 70 inches?\nWe just need the average height and standard deviation:\n\n\nm &lt;- mean(x) \ns &lt;- sd(x) \n1 - pnorm(70.5, m, s) \n\n[1] 0.371369"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nThe normal distribution is derived mathematically; we do not need data to define it.\nFor practicing data scientists, almost everything we do involves data.\nData is always, technically speaking, discrete.\nFor example, we could consider our height data categorical, with each specific height a unique category.\nThe probability distribution is defined by the proportion of students reporting each height."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-1",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-1",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nBelow is a plot of that probability distribution:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-2",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-2",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nWhile most students rounded up their heights to the nearest inch, others reported values with more precision.\nOne student reported his height to be 69.6850393700787, which is 177 centimeters.\nThe probability assigned to this height is 0.0012315 or 1 in 812."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-3",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-3",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nThe probability for 70 inches is much higher at 0.1059113,\nDoes it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787?"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-4",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-4",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nClearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-5",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-5",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nWith continuous distributions, the probability of a singular value is not even defined.\nFor instance, it does not make sense to ask what is the probability that a normally distributed value is 70.\nInstead, we define probabilities for intervals."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-6",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-6",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nWe therefore could ask, what is the probability that someone is between 69.5 and 70.5?\nIn cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-7",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-7",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nFor example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:\n\n\nmean(x &lt;= 68.5) - mean(x &lt;= 67.5) \n\n[1] 0.114532\n\nmean(x &lt;= 69.5) - mean(x &lt;= 68.5) \n\n[1] 0.1194581\n\nmean(x &lt;= 70.5) - mean(x &lt;= 69.5) \n\n[1] 0.1219212"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-8",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-8",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nNote how close we get with the normal approximation:\n\n\npnorm(68.5, m, s) - pnorm(67.5, m, s)  \n\n[1] 0.1031077\n\npnorm(69.5, m, s) - pnorm(68.5, m, s)  \n\n[1] 0.1097121\n\npnorm(70.5, m, s) - pnorm(69.5, m, s)  \n\n[1] 0.1081743\n\n\n\nHowever, the approximation is not as useful for other intervals."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-9",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-9",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nFor instance, notice how the approximation breaks down when we try to estimate:\n\n\nmean(x &lt;= 70.9) - mean(x &lt;= 70.1) \n\n[1] 0.02216749\n\n\n\nwith:\n\n\npnorm(70.9, m, s) - pnorm(70.1, m, s) \n\n[1] 0.08359562"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-10",
    "href": "slides/prob/20-intro-to-prob.html#distributions-as-approximations-10",
    "title": "Introduction to Probability",
    "section": "Distributions as approximations",
    "text": "Distributions as approximations\n\nIn general, we call this situation discretization.\nAlthough the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding.\nAs long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nFor categorical distributions, we can define the probability of a category.\nFor example, a roll of a die, let’s call it \\(X\\), can be 1, 2, 3, 4, 5 or 6.\nThe probability of 4 is defined as:\n\n\\[\n\\mbox{Pr}(X=4) = 1/6\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-1",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-1",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nThe CDF can then easily be defined:\n\n\\[\nF(4) = \\mbox{Pr}(X\\leq 4) =  \\mbox{Pr}(X = 4) +  \\mbox{Pr}(X = 3) +  \\mbox{Pr}(X = 2) +  \\mbox{Pr}(X = 1)  \n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-2",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-2",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nAlthough for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation.\nThe probability density at \\(x\\) is defined as the function \\(f(a)\\) such that:\n\n\\[\nF(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx\n\\]"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-3",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-3",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nFor those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0.\nIf you don’t know calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve, up to the value \\(a\\), gives you the probability \\(\\mbox{Pr}(X\\leq a)\\)."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-4",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-4",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nFor example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:\n\n\n1 - pnorm(76, m, s) \n\n[1] 0.03206008"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-5",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-5",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nwhich mathematically is the grey area:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#the-probability-density-6",
    "href": "slides/prob/20-intro-to-prob.html#the-probability-density-6",
    "title": "Introduction to Probability",
    "section": "The probability density",
    "text": "The probability density\n\nThe curve you see is the probability density for the normal distribution.\nIn R, we get this using the function dnorm.\nWhile it may not be immediately apparent why knowing about probability densities is useful, understanding this concept is essential for individuals aiming to fit models to data for which predefined functions are not available."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-2",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-2",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nR provides functions to generate normally distributed outcomes.\nSpecifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1), and produces random numbers."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-3",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-3",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nHere is an example of how we could generate data that looks like our reported heights:\n\n\nn &lt;- length(x) \nm &lt;- mean(x) \ns &lt;- sd(x) \nsimulated_heights &lt;- rnorm(n, m, s)"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-4",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-4",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nNot surprisingly, the distribution looks normal:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-5",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-5",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nThis is one of the most useful functions in R, as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.\nIf, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven-footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-6",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-6",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nB &lt;- 10000 \ntallest &lt;- replicate(B, { \n  simulated_data &lt;- rnorm(800, m, s) \n  max(simulated_data) \n})"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-7",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-7",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nHaving a seven-footer is quite rare:\n\n\nmean(tallest &gt;= 7*12) \n\n[1] 0.0195"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#monte-carlo-8",
    "href": "slides/prob/20-intro-to-prob.html#monte-carlo-8",
    "title": "Introduction to Probability",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nHere is the resulting distribution:\n\n\n\nNote that it does not look normal."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#continuous-distributions",
    "href": "slides/prob/20-intro-to-prob.html#continuous-distributions",
    "title": "Introduction to Probability",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nThe normal distribution is not the only useful theoretical distribution.\nOther continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial.\nR provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations.\nR uses a convention that lets us remember the names, namely using the letters d, q, p, and r in front of a shorthand for the distribution."
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#continuous-distributions-1",
    "href": "slides/prob/20-intro-to-prob.html#continuous-distributions-1",
    "title": "Introduction to Probability",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nWe have already seen the functions dnorm, pnorm, and rnorm for the normal distribution.\nThe functions qnorm gives us the quantiles.\nWe can therefore draw a distribution like this:"
  },
  {
    "objectID": "slides/prob/20-intro-to-prob.html#continuous-distributions-2",
    "href": "slides/prob/20-intro-to-prob.html#continuous-distributions-2",
    "title": "Introduction to Probability",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nx &lt;- seq(-4, 4, length.out = 100) \nqplot(x, f, geom = \"line\", data = data.frame(x, f = dnorm(x)))"
  },
  {
    "objectID": "slides/00-intro.html#general-information",
    "href": "slides/00-intro.html#general-information",
    "title": "Introduction",
    "section": "General Information",
    "text": "General Information\n\nBST 260 Introduction to Data Science\nInstructor: Rafael A. Irizarry\nTFs: Corri Sept, Nikhil Vytla, Yuan Wang\nMondays we have lectures, Wednesday we have labs.\nWe work on problem sets together, in lab."
  },
  {
    "objectID": "slides/00-intro.html#course-description",
    "href": "slides/00-intro.html#course-description",
    "title": "Introduction",
    "section": "Course Description",
    "text": "Course Description\nLecture notes: https://datasciencelabs.github.io/2024/\n\nPlease read the syllabus!"
  },
  {
    "objectID": "slides/00-intro.html#important-details",
    "href": "slides/00-intro.html#important-details",
    "title": "Introduction",
    "section": "Important details",
    "text": "Important details\n\nComplete readings before class.\nMidterms are in person. There are no makeups.\nMake sure you read messages sent via Canvas\nYou can select your own final project, but need approval.\nYou should start final project by October 23.\nHelp us pick office hours: https://forms.gle/GiQXqDTaeYVxaXd78"
  },
  {
    "objectID": "slides/00-intro.html#whats-coming",
    "href": "slides/00-intro.html#whats-coming",
    "title": "Introduction",
    "section": "What’s coming",
    "text": "What’s coming\n\nUNIX/Linux shell.\nReproducible document preparation\nVersion control with git and GitHub\nR programming\nData wrangling with dplyr and data.table\nData visualization with ggplot2\nProbability theory, inference and modeling\nHigh-dimensional data techniques\nMachine learning"
  },
  {
    "objectID": "slides/00-intro.html#lets-get-started",
    "href": "slides/00-intro.html#lets-get-started",
    "title": "Introduction",
    "section": "Let’s get started",
    "text": "Let’s get started\n\nInstall R.\nInstall RStudio.\nMake sure you have access to a terminal."
  },
  {
    "objectID": "slides/R/09-tidyr.html#tidying-data",
    "href": "slides/R/09-tidyr.html#tidying-data",
    "title": "Tidying data",
    "section": "Tidying data",
    "text": "Tidying data\n\nWe will a wide format dataset as an example:\n\n\nlibrary(tidyverse) \nlibrary(dslabs)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfilename &lt;- file.path(path, \"fertility-two-countries-example.csv\")\nwide_data &lt;- read_csv(filename)\nselect(wide_data, 1:10)\n\n# A tibble: 2 × 10\n  country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37   2.28\n2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85   4.73"
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer",
    "href": "slides/R/09-tidyr.html#pivot_longer",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\n\nOne of the most used functions in tidyr is pivot_longer.\nThe first argument is a data frame, the one that will be converted.\nWe want to reshape rows represents a fertility observation.\nWe need three columns to store the year, country, and the observed value."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer-1",
    "href": "slides/R/09-tidyr.html#pivot_longer-1",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\n\nIn its current form, data from different years are in different columns with the year values stored in the column names.\nThe names_to and values_to argument tell pivot_longer the column names we want to assign to the columns containing the current column names and observations, respectively.\nThe default names are name and value, in our case a better choice is year and fertility."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer-2",
    "href": "slides/R/09-tidyr.html#pivot_longer-2",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\n\nThrough cols, the second argument, we specify the columns containing observed values; these are the columns that will be pivoted.\nThe default is to pivot all columns so, in most cases, we have to specify the columns. In our example we want columns 1960, 1961 up to 2015."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer-3",
    "href": "slides/R/09-tidyr.html#pivot_longer-3",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\nThe code to pivot the fertility data therefore looks like this:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(`1960`:`2015`, names_to = \"year\", values_to = \"fertility\")\nhead(new_tidy_data)\n\n# A tibble: 6 × 3\n  country year  fertility\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 Germany 1960       2.41\n2 Germany 1961       2.44\n3 Germany 1962       2.47\n4 Germany 1963       2.49\n5 Germany 1964       2.49\n6 Germany 1965       2.48\n\n\n\nData have been converted to tidy format with columns year and fertility."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer-4",
    "href": "slides/R/09-tidyr.html#pivot_longer-4",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\n\nA quicker way to write this code is to specify which column will not include in the pivot:\n\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\")"
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_longer-5",
    "href": "slides/R/09-tidyr.html#pivot_longer-5",
    "title": "Tidying data",
    "section": "pivot_longer",
    "text": "pivot_longer\n\nNow that the data is tidy, we can use it with other tidyverse functions, such ggplot2:\n\n\nnew_tidy_data |&gt; \n  mutate(year = as.numeric(year)) |&gt;\n  ggplot(aes(year, fertility, color = country)) + \n  geom_line()"
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_wider",
    "href": "slides/R/09-tidyr.html#pivot_wider",
    "title": "Tidying data",
    "section": "pivot_wider",
    "text": "pivot_wider\n\nIt is sometimes useful for data wrangling purposes to convert tidy data into wide data.\nWe often use this as an intermediate step in tidying up data.\nThe pivot_wider function is basically the inverse of pivot_longer."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_wider-1",
    "href": "slides/R/09-tidyr.html#pivot_wider-1",
    "title": "Tidying data",
    "section": "pivot_wider",
    "text": "pivot_wider\n\nThe first argument is for the data, but since we are using the pipe, we don’t show it.\nThe names_from argument tells pivot_wider which variable will be used as the column names.\nThe values_from argument specifies which variable to use to fill out the cells."
  },
  {
    "objectID": "slides/R/09-tidyr.html#pivot_wider-2",
    "href": "slides/R/09-tidyr.html#pivot_wider-2",
    "title": "Tidying data",
    "section": "pivot_wider",
    "text": "pivot_wider\nHere is some example code\n\nnew_wide_data &lt;- new_tidy_data |&gt; \n  pivot_wider(names_from = year, values_from = fertility)\nselect(new_wide_data, country, `1960`:`1967`)\n\n# A tibble: 2 × 9\n  country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37\n2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85\n\n\nSimilar to pivot_wider, names_from and values_from default to name and value."
  },
  {
    "objectID": "slides/R/09-tidyr.html#example",
    "href": "slides/R/09-tidyr.html#example",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\nWe now demonstrate with a more complex example in which we have to use both pivot_longer and pivot_wider to tidy the data.\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nraw_dat &lt;- read_csv(filename)\nselect(raw_dat, 1:5)\n\n# A tibble: 2 × 5\n  country     `1960_fertility` `1960_life_expectancy` `1961_fertility`\n  &lt;chr&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Germany                 2.41                   69.3             2.44\n2 South Korea             6.16                   53.0             5.99\n# ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-1",
    "href": "slides/R/09-tidyr.html#example-1",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\nNote that the data is in wide format.\nAlso that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable.\nEncoding information in the column names is not recommended but, unfortunately, it is quite common."
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-2",
    "href": "slides/R/09-tidyr.html#example-2",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\nWe start with the pivot_longer function, but we should no longer use the column name year for the new column since it also contains the variable type.\nWe will call it name, the default, for now:\n\n\nraw_dat |&gt; pivot_longer(-country) |&gt; head()\n\n# A tibble: 6 × 3\n  country name                 value\n  &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;\n1 Germany 1960_fertility        2.41\n2 Germany 1960_life_expectancy 69.3 \n3 Germany 1961_fertility        2.44\n4 Germany 1961_life_expectancy 69.8 \n5 Germany 1962_fertility        2.47\n6 Germany 1962_life_expectancy 70.0"
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-3",
    "href": "slides/R/09-tidyr.html#example-3",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\nThe result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows.\nWe want to have the values from the two variables, fertility and life expectancy, in two separate columns.\nThe first challenge to achieve this is to separate the name column into the year and the variable type."
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-4",
    "href": "slides/R/09-tidyr.html#example-4",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\nEncoding multiple variables in a column name is such a common problem that the tidyr package includes function to separate these columns into two or more:\n\n\nraw_dat |&gt; \n  pivot_longer(-country) |&gt; \n  separate_wider_delim(name, delim = \"_\",  names = c(\"year\", \"name\"), \n                       too_many = \"merge\") |&gt;\n  head()\n\n# A tibble: 6 × 4\n  country year  name            value\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 Germany 1960  fertility        2.41\n2 Germany 1960  life_expectancy 69.3 \n3 Germany 1961  fertility        2.44\n4 Germany 1961  life_expectancy 69.8 \n5 Germany 1962  fertility        2.47\n6 Germany 1962  life_expectancy 70.0"
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-5",
    "href": "slides/R/09-tidyr.html#example-5",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\nBut we are not done yet.\nWe need to create a column for each variable and change year to a number.\nAs we learned, the pivot_wider function can do this."
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-6",
    "href": "slides/R/09-tidyr.html#example-6",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\n\ndat &lt;- raw_dat |&gt; \n  pivot_longer(-country) |&gt; \n  separate_wider_delim(name, delim = \"_\", \n                       names = c(\"year\", \"name\"), \n                       too_many = \"merge\") |&gt;\n  pivot_wider() |&gt;\n  mutate(year = as.integer(year))\nhead(dat)\n\n# A tibble: 6 × 4\n  country  year fertility life_expectancy\n  &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n1 Germany  1960      2.41            69.3\n2 Germany  1961      2.44            69.8\n3 Germany  1962      2.47            70.0\n4 Germany  1963      2.49            70.1\n5 Germany  1964      2.49            70.7\n6 Germany  1965      2.48            70.6"
  },
  {
    "objectID": "slides/R/09-tidyr.html#example-7",
    "href": "slides/R/09-tidyr.html#example-7",
    "title": "Tidying data",
    "section": "Example",
    "text": "Example\nThe data is now is now in tidy form and we can use with other packages:\n\np1 &lt;- dat |&gt; ggplot(aes(year, fertility, color = country)) + geom_line(show.legend = FALSE)\np2 &lt;- dat |&gt; ggplot(aes(year, life_expectancy, color = country)) + geom_line()\ngridExtra::grid.arrange(p1, p2, ncol = 2, widths = c(3,4))"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#tidyverse",
    "href": "slides/R/07-tidyverse.html#tidyverse",
    "title": "Tidyverse",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nlibrary(tidyverse)\n\n\nThe tidyverse is not a package but a group of packages developed to work with each other.\nThe tidyverse makes data analysis simpler and code easier to read by sacrificing some flexibility.\nOne way code is simplified by ensuring all functions take and return tidy data."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#tidy-data",
    "href": "slides/R/07-tidyverse.html#tidy-data",
    "title": "Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nStored in a data frame.\nEach observation is exactly one row.\nVariables are stored in columns.\nNot all data can be represented this way, but a very large subset of data analysis challenges are based on tidy data.\nAssuming data is tidy simplifies coding and frees up our minds for statistical thinking."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#tidy-data-1",
    "href": "slides/R/07-tidyverse.html#tidy-data-1",
    "title": "Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nThis is an example of a tidy dataset:\n\n\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#tidy-data-2",
    "href": "slides/R/07-tidyverse.html#tidy-data-2",
    "title": "Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nOriginally, the data was in the following format:\n\n\n\n      country 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n1     Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 2.28 2.17 2.04\n2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 4.73 4.62 4.53\n\n\n\nThis is not tidy."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#tidyverse-packages",
    "href": "slides/R/07-tidyverse.html#tidyverse-packages",
    "title": "Tidyverse",
    "section": "Tidyverse packages",
    "text": "Tidyverse packages\n\ntibble - improves data frame class.\nreadr - import data.\ndplyr - used to modify data frames.\nggplot2 - simplifies plotting.\ntidyr - helps convert data into tidy format.\nstringr - string processing.\nforcats - utilities for categorical data.\npurrr - tidy version of apply functions."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#dplyr",
    "href": "slides/R/07-tidyverse.html#dplyr",
    "title": "Tidyverse",
    "section": "dplyr",
    "text": "dplyr\n\nIn this lecture we focus mainly on dplyr.\nIn particular the following functions:\n\nmutate\nselect\nacross\nfilter\ngroup_by\nsummarize"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#adding-a-column-with-mutate",
    "href": "slides/R/07-tidyverse.html#adding-a-column-with-mutate",
    "title": "Tidyverse",
    "section": "Adding a column with mutate",
    "text": "Adding a column with mutate\n\nmurders &lt;- mutate(murders, rate = total/population*100000)\n\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace.\nThis is known as non-standard evaluation where the context is used to know what variable names means.\nTidyverse extensively uses non-standard evaluation.\nThis can create confusion but it certainly simplifies code."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#subsetting-with-filter",
    "href": "slides/R/07-tidyverse.html#subsetting-with-filter",
    "title": "Tidyverse",
    "section": "Subsetting with filter",
    "text": "Subsetting with filter\n\nfilter(murders, rate &lt;= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#selecting-columns-with-select",
    "href": "slides/R/07-tidyverse.html#selecting-columns-with-select",
    "title": "Tidyverse",
    "section": "Selecting columns with select",
    "text": "Selecting columns with select\n\nnew_table &lt;- select(murders, state, region, rate)\nfilter(new_table, rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#transforming-variables",
    "href": "slides/R/07-tidyverse.html#transforming-variables",
    "title": "Tidyverse",
    "section": "Transforming variables",
    "text": "Transforming variables\n\nThe function mutate can also be used to transform variables.\nFor example, the following code takes the log transformation of the population variable:\n\n\nmutate(murders, population = log10(population))"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#transforming-variables-1",
    "href": "slides/R/07-tidyverse.html#transforming-variables-1",
    "title": "Tidyverse",
    "section": "Transforming variables",
    "text": "Transforming variables\n\nOften, we need to apply the same transformation to several variables.\nThe function across facilitates the operation.\nFor example if want to log transform both population and total murders we can use:\n\n\nmutate(murders, across(c(population, total), log10))"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#transforming-variables-2",
    "href": "slides/R/07-tidyverse.html#transforming-variables-2",
    "title": "Tidyverse",
    "section": "Transforming variables",
    "text": "Transforming variables\n\nThe helper functions come in handy when using across.\nAn example is if we want to apply the same transformation to all numeric variables:\n\n\nmutate(murders, across(where(is.numeric), log10))\n\n\nor all character variables:\n\n\nmutate(murders, across(where(is.character), tolower))\n\n\nThere are several other useful helper functions."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#the-pipe-or",
    "href": "slides/R/07-tidyverse.html#the-pipe-or",
    "title": "Tidyverse",
    "section": "The pipe: |> or %>%",
    "text": "The pipe: |&gt; or %&gt;%\n\nWe use the pipe to chain a series of operations.\nFor example if we want to select columns and then filter rows we chain like this:\n\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#the-pipe-or-1",
    "href": "slides/R/07-tidyverse.html#the-pipe-or-1",
    "title": "Tidyverse",
    "section": "The pipe: |> or %>%",
    "text": "The pipe: |&gt; or %&gt;%\n\nThe code looks like this:\n\n\nmurders |&gt; select(state, region, rate) |&gt; filter(rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\n\nThe object on the left of the pipe is used as the first argument for the function on the right.\nThe second argument becomes the first, the third the second, and so on…"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#the-pipe-or-2",
    "href": "slides/R/07-tidyverse.html#the-pipe-or-2",
    "title": "Tidyverse",
    "section": "The pipe: |> or %>%",
    "text": "The pipe: |&gt; or %&gt;%\n\nHere is a simple example:\n\n\n16 |&gt; sqrt() |&gt; log(base = 2)\n\n[1] 2"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#summarizing-data",
    "href": "slides/R/07-tidyverse.html#summarizing-data",
    "title": "Tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\n\nWe use the dplyr summarize function, not to be confused with summary from R base.\nHere is an example of how it works:\n\n\nmurders |&gt; summarize(avg = mean(rate))\n\n       avg\n1 2.779125\n\n\n\nLet’s compute murder rate for the US. Is the above it?"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#summarizing-data-1",
    "href": "slides/R/07-tidyverse.html#summarizing-data-1",
    "title": "Tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\n\nNo, the rate is NOT the average of rates.\nIt is the total murders divided by total population:\n\n\nmurders |&gt; summarize(rate = sum(total)/sum(population)*100000)\n\n      rate\n1 3.034555"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#multiple-summaries",
    "href": "slides/R/07-tidyverse.html#multiple-summaries",
    "title": "Tidyverse",
    "section": "Multiple summaries",
    "text": "Multiple summaries\n\nSuppose we want the median, minimum and max population size:\n\n\nmurders |&gt; summarize(median = median(population), min = min(population), max = max(population))\n\n   median    min      max\n1 4339367 563626 37253956\n\n\n\nWhy don’t we use quantiles?\n\n\nmurders |&gt; summarize(quantiles = quantile(population, c(0.5, 0, 1)))\n\n  quantiles\n1   4339367\n2    563626\n3  37253956"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#multiple-summaries-1",
    "href": "slides/R/07-tidyverse.html#multiple-summaries-1",
    "title": "Tidyverse",
    "section": "Multiple summaries",
    "text": "Multiple summaries\n\n\n\n\n\n\nWarning\n\n\nUsing a function that returns more than one number within summarize will soon be deprecated.\n\n\n\n\nFor multiple summaries we use reframe:\n\n\nmurders |&gt; reframe(quantiles = quantile(population, c(0.5, 0, 1)))\n\n  quantiles\n1   4339367\n2    563626\n3  37253956"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#multiple-summaries-2",
    "href": "slides/R/07-tidyverse.html#multiple-summaries-2",
    "title": "Tidyverse",
    "section": "Multiple summaries",
    "text": "Multiple summaries\n\nHowever, if we want a column per summary, as when we called min, median, and max separately, we have to define a function that returns a data frame like this:\n\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], min = qs[2], max = qs[3])\n}\n\n\nThen we can call summarize:\n\n\nmurders |&gt; summarize(median_min_max(population))\n\n   median    min      max\n1 4339367 563626 37253956"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#group-then-summarize",
    "href": "slides/R/07-tidyverse.html#group-then-summarize",
    "title": "Tidyverse",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nLet’s compute murder rate by region.\nTake a close look at this output?\n\n\nmurders |&gt; group_by(region) |&gt; head(4)\n\n# A tibble: 4 × 6\n# Groups:   region [2]\n  state    abb   region population total  rate\n  &lt;chr&gt;    &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Alabama  AL    South     4779736   135  2.82\n2 Alaska   AK    West       710231    19  2.68\n3 Arizona  AZ    West      6392017   232  3.63\n4 Arkansas AR    South     2915918    93  3.19\n\n\n\nNote the Groups: region [4] at the top.\nThis is a special data frame called a grouped data frame."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#group-then-summarize-1",
    "href": "slides/R/07-tidyverse.html#group-then-summarize-1",
    "title": "Tidyverse",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nIn particular summarize, will behave differently when acting on this object.\n\n\nmurders |&gt; \n  group_by(region) |&gt; \n  summarize(rate = sum(total) / sum(population) * 100000)\n\n# A tibble: 4 × 2\n  region         rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Northeast      2.66\n2 South          3.63\n3 North Central  2.73\n4 West           2.66\n\n\n\nThe summarize function applies the summarization to each group separately."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#group-then-summarize-2",
    "href": "slides/R/07-tidyverse.html#group-then-summarize-2",
    "title": "Tidyverse",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nFor another example, let’s compute the median, minimum, and maximum population in the four regions of the country using the median_min_max previously defined:\n\n\nmurders |&gt; group_by(region) |&gt; summarize(median_min_max(population))\n\n# A tibble: 4 × 4\n  region          median    min      max\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Northeast     3574097  625741 19378102\n2 South         4625364  601723 25145561\n3 North Central 5495456. 672591 12830632\n4 West          2700551  563626 37253956"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#group-then-summarize-3",
    "href": "slides/R/07-tidyverse.html#group-then-summarize-3",
    "title": "Tidyverse",
    "section": "Group then summarize",
    "text": "Group then summarize\n\nYou can also summarize a variable but not collapse the dataset.\nWe use mutate instead of summarize.\nHere is an example where we add a column with the population in each region and the number of states in the region, shown for each state.\n\n\nmurders |&gt; group_by(region) |&gt; \n  mutate(region_pop = sum(population), n = n())"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#ungroup",
    "href": "slides/R/07-tidyverse.html#ungroup",
    "title": "Tidyverse",
    "section": "ungroup",
    "text": "ungroup\n\nWhen we do this, we usually want to ungroup before continuing our analysis.\n\n\nmurders |&gt; group_by(region) |&gt; \n  mutate(region_pop = sum(population), n = n()) |&gt;\n  ungroup()\n\n\nThis avoids having a grouped data frame that we don’t need."
  },
  {
    "objectID": "slides/R/07-tidyverse.html#pull",
    "href": "slides/R/07-tidyverse.html#pull",
    "title": "Tidyverse",
    "section": "pull",
    "text": "pull\n\nTidyverse function always returns a data frame. Even if its just one number.\n\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  class()\n\n[1] \"data.frame\""
  },
  {
    "objectID": "slides/R/07-tidyverse.html#pull-1",
    "href": "slides/R/07-tidyverse.html#pull-1",
    "title": "Tidyverse",
    "section": "pull",
    "text": "pull\n\nTo get a numeric use pull:\n\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  pull(rate) \n\n[1] 3.034555"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#sorting-data-frames",
    "href": "slides/R/07-tidyverse.html#sorting-data-frames",
    "title": "Tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\n\nStates order by rate\n\n\nmurders |&gt; arrange(rate) |&gt; head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#sorting-data-frames-1",
    "href": "slides/R/07-tidyverse.html#sorting-data-frames-1",
    "title": "Tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\n\nIf we want decreasing we can either use the negative or, for more readability, use desc:\n\n\nmurders |&gt; arrange(desc(rate)) |&gt; head()\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Missouri  MO North Central    5988927   321  5.359892\n4             Maryland  MD         South    5773552   293  5.074866\n5       South Carolina  SC         South    4625364   207  4.475323\n6             Delaware  DE         South     897934    38  4.231937"
  },
  {
    "objectID": "slides/R/07-tidyverse.html#sorting-data-frames-2",
    "href": "slides/R/07-tidyverse.html#sorting-data-frames-2",
    "title": "Tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\n\nWe can use two variables as well:\n\n\nmurders |&gt; arrange(region, desc(rate)) |&gt; head(11)\n\n                  state abb    region population total       rate\n1          Pennsylvania  PA Northeast   12702379   457  3.5977513\n2            New Jersey  NJ Northeast    8791894   246  2.7980319\n3           Connecticut  CT Northeast    3574097    97  2.7139722\n4              New York  NY Northeast   19378102   517  2.6679599\n5         Massachusetts  MA Northeast    6547629   118  1.8021791\n6          Rhode Island  RI Northeast    1052567    16  1.5200933\n7                 Maine  ME Northeast    1328361    11  0.8280881\n8         New Hampshire  NH Northeast    1316470     5  0.3798036\n9               Vermont  VT Northeast     625741     2  0.3196211\n10 District of Columbia  DC     South     601723    99 16.4527532\n11            Louisiana  LA     South    4533372   351  7.7425810"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-tables",
    "href": "slides/wrangling/16-joining-tables.html#joining-tables",
    "title": "Joining Tables",
    "section": "Joining tables",
    "text": "Joining tables\n\nThe information we need for a given analysis may not be just in one table.\nHere we use a simple examples to illustrate the general challenge of combining tables."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-tables-1",
    "href": "slides/wrangling/16-joining-tables.html#joining-tables-1",
    "title": "Joining Tables",
    "section": "Joining tables",
    "text": "Joining tables\n\nSuppose we want to explore the relationship between population size for US states and electoral votes.\nWe have the population size in this table:\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \nhead(murders) \n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-tables-2",
    "href": "slides/wrangling/16-joining-tables.html#joining-tables-2",
    "title": "Joining Tables",
    "section": "Joining tables",
    "text": "Joining tables\n\nand electoral votes in this one:\n\n\nhead(results_us_election_2016) \n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-tables-3",
    "href": "slides/wrangling/16-joining-tables.html#joining-tables-3",
    "title": "Joining Tables",
    "section": "Joining tables",
    "text": "Joining tables\n\nJust concatenating these two tables together will not work since the order of the states is not the same.\n\n\nidentical(results_us_election_2016$state, murders$state) \n\n[1] FALSE"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins",
    "href": "slides/wrangling/16-joining-tables.html#joins",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n\nThe join functions are designed to handle this challenge.\nThe join functions in the dplyr package make sure that the tables are combined so that matching rows are together.\nThe general idea is that one needs to identify one or more columns that will serve to match the two tables.\nA new table with the combined information is returned."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins-1",
    "href": "slides/wrangling/16-joining-tables.html#joins-1",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n\nNotice what happens if we join the two tables above by state using left_join:\n\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") |&gt; \n  select(-others) |&gt; rename(ev = electoral_votes) \nhead(tab) \n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins-2",
    "href": "slides/wrangling/16-joining-tables.html#joins-2",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n\nThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins-3",
    "href": "slides/wrangling/16-joining-tables.html#joins-3",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n\nWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other.\nFor this reason, we have several versions of join.\nTo illustrate this challenge, we will take subsets of the tables above."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins-4",
    "href": "slides/wrangling/16-joining-tables.html#joins-4",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n.\nNote: These names are based on SQL."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joins-5",
    "href": "slides/wrangling/16-joining-tables.html#joins-5",
    "title": "Joining Tables",
    "section": "Joins",
    "text": "Joins\n\nWe create the tables tab1 and tab2 so that they have some states in common but not all:\n\n\ntab_1 &lt;- slice(murders, 1:6) |&gt; select(state, population) \ntab_2 &lt;- results_us_election_2016 |&gt;  \n  filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\",  \n                    \"California\", \"Connecticut\", \"Delaware\")) |&gt;  \n  select(state, electoral_votes) |&gt; rename(ev = electoral_votes) \n\n\nWe will use these two tables as examples in the next sections."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#left-join",
    "href": "slides/wrangling/16-joining-tables.html#left-join",
    "title": "Joining Tables",
    "section": "Left join",
    "text": "Left join\n\nSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available.\nFor this, we use left_join with tab_1 as the first argument.\nWe specify which column to use to match with the by argument.\n\n\nleft_join(tab_1, tab_2, by = \"state\") \n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4   Arkansas    2915918 NA\n5 California   37253956 55\n6   Colorado    5029196 NA\n\n\n\nNote that NAs are added to the two states not appearing in tab_2."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#left-join-1",
    "href": "slides/wrangling/16-joining-tables.html#left-join-1",
    "title": "Joining Tables",
    "section": "Left join",
    "text": "Left join\n\nAlso, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\n\n\ntab_1 |&gt; left_join(tab_2, by = \"state\")"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#right-join",
    "href": "slides/wrangling/16-joining-tables.html#right-join",
    "title": "Joining Tables",
    "section": "Right join",
    "text": "Right join\n\nIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\n\n\ntab_1 |&gt; right_join(tab_2, by = \"state\") \n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4  California   37253956 55\n5 Connecticut         NA  7\n6    Delaware         NA  3\n\n\n\nNow the NAs are in the column coming from tab_1."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#inner-join",
    "href": "slides/wrangling/16-joining-tables.html#inner-join",
    "title": "Joining Tables",
    "section": "Inner join",
    "text": "Inner join\n\nIf we want to keep only the rows that have information in both tables, we use inner_join.\nYou can think of this as an intersection:\n\n\ninner_join(tab_1, tab_2, by = \"state\") \n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4 California   37253956 55"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#full-join",
    "href": "slides/wrangling/16-joining-tables.html#full-join",
    "title": "Joining Tables",
    "section": "Full join",
    "text": "Full join\n\nIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join.\nYou can think of this as a union:\n\n\nfull_join(tab_1, tab_2, by = \"state\") \n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4    Arkansas    2915918 NA\n5  California   37253956 55\n6    Colorado    5029196 NA\n7 Connecticut         NA  7\n8    Delaware         NA  3"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#semi-join",
    "href": "slides/wrangling/16-joining-tables.html#semi-join",
    "title": "Joining Tables",
    "section": "Semi join",
    "text": "Semi join\n\nThe semi_join function lets us keep the part of first table for which we have information in the second.\nIt does not add the columns of the second:\n\n\nsemi_join(tab_1, tab_2, by = \"state\") \n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4 California   37253956"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#anti-join",
    "href": "slides/wrangling/16-joining-tables.html#anti-join",
    "title": "Joining Tables",
    "section": "Anti join",
    "text": "Anti join\n\nThe function anti_join is the opposite of semi_join.\nIt keeps the elements of the first table for which there is no information in the second:\n\n\nanti_join(tab_1, tab_2, by = \"state\") \n\n     state population\n1 Arkansas    2915918\n2 Colorado    5029196"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#binding",
    "href": "slides/wrangling/16-joining-tables.html#binding",
    "title": "Joining Tables",
    "section": "Binding",
    "text": "Binding\n\nAlthough we have yet to use it in this book, another common way in which datasets are combined is by binding them.\nUnlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets.\nIf the datasets don’t match by the appropriate dimensions, one obtains an error."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#binding-columns",
    "href": "slides/wrangling/16-joining-tables.html#binding-columns",
    "title": "Joining Tables",
    "section": "Binding columns",
    "text": "Binding columns\n\nThe dplyr function bind_cols binds two objects by making them columns in a tibble.\n\n\nbind_cols(a = 1:3, b = 4:6) \n\n# A tibble: 3 × 2\n      a     b\n  &lt;int&gt; &lt;int&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\n\nThis function requires that we assign names to the columns. Here we chose a and b.\nNote that there is an R-base function cbind with the exact same functionality."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#binding-columns-1",
    "href": "slides/wrangling/16-joining-tables.html#binding-columns-1",
    "title": "Joining Tables",
    "section": "Binding columns",
    "text": "Binding columns\n\nAn important difference is that cbind can create different types of objects, while bind_cols always produces a data frame.\nbind_cols can also bind two different data frames."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#binding-columns-2",
    "href": "slides/wrangling/16-joining-tables.html#binding-columns-2",
    "title": "Joining Tables",
    "section": "Binding columns",
    "text": "Binding columns\n\nFor example, here we break up the tab data frame and then bind them back together:\n\n\ntab_1 &lt;- tab[, 1:3] \ntab_2 &lt;- tab[, 4:6] \ntab_3 &lt;- tab[, 7:8] \nnew_tab &lt;- bind_cols(tab_1, tab_2, tab_3) \nhead(new_tab) \n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#binding-by-rows",
    "href": "slides/wrangling/16-joining-tables.html#binding-by-rows",
    "title": "Joining Tables",
    "section": "Binding by rows",
    "text": "Binding by rows\n\nThe bind_rows function is similar to bind_cols, but binds rows instead of columns:\n\n\ntab_1 &lt;- tab[1:2,] \ntab_2 &lt;- tab[3:4,] \nbind_rows(tab_1, tab_2) \n\n     state abb region population total ev clinton trump\n1  Alabama  AL  South    4779736   135  9    34.4  62.1\n2   Alaska  AK   West     710231    19  3    36.6  51.3\n3  Arizona  AZ   West    6392017   232 11    45.1  48.7\n4 Arkansas  AR  South    2915918    93  6    33.7  60.6\n\n\n\nThis is based on an R-base function rbind."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#set-operators",
    "href": "slides/wrangling/16-joining-tables.html#set-operators",
    "title": "Joining Tables",
    "section": "Set operators",
    "text": "Set operators\n\nAnother set of commands useful for combining datasets are the set operators.\nWhen applied to vectors, these behave as their names suggest.\nExamples are intersect, union, setdiff, and setequal.\nHowever, if the tidyverse, or more specifically dplyr, is loaded, these functions can be used on data frames as opposed to just on vectors."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#intersect",
    "href": "slides/wrangling/16-joining-tables.html#intersect",
    "title": "Joining Tables",
    "section": "Intersect",
    "text": "Intersect\n\nYou can take intersections of vectors of any type, such as numeric:\n\n\nintersect(1:10, 6:15) \n\n[1]  6  7  8  9 10\n\n\n\nor characters:\n\n\nintersect(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\")) \n\n[1] \"b\" \"c\"\n\n\n\ndplyr includes an intersect function that can be applied to tables with the same column names.\nThis function returns the rows in common between two tables."
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#intersect-1",
    "href": "slides/wrangling/16-joining-tables.html#intersect-1",
    "title": "Joining Tables",
    "section": "Intersect",
    "text": "Intersect\n\nTo make sure we use the dplyr version of intersect rather than the base R version, we can use dplyr::intersect like this:\n\n\ntab_1 &lt;- tab[1:5,] \ntab_2 &lt;- tab[3:7,] \ndplyr::intersect(tab_1, tab_2) \n\n       state abb region population total ev clinton trump\n1    Arizona  AZ   West    6392017   232 11    45.1  48.7\n2   Arkansas  AR  South    2915918    93  6    33.7  60.6\n3 California  CA   West   37253956  1257 55    61.7  31.6"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#union",
    "href": "slides/wrangling/16-joining-tables.html#union",
    "title": "Joining Tables",
    "section": "Union",
    "text": "Union\n\nSimilarly union takes the union of vectors.\nFor example:\n\n\nunion(1:10, 6:15) \n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nunion(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\")) \n\n[1] \"a\" \"b\" \"c\" \"d\""
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#union-1",
    "href": "slides/wrangling/16-joining-tables.html#union-1",
    "title": "Joining Tables",
    "section": "Union",
    "text": "Union\n\ndplyr includes a version of union that combines all the rows of two tables with the same column names.\n\n\ntab_1 &lt;- tab[1:5,] \ntab_2 &lt;- tab[3:7,] \ndplyr::union(tab_1, tab_2)  \n\n        state abb    region population total ev clinton trump\n1     Alabama  AL     South    4779736   135  9    34.4  62.1\n2      Alaska  AK      West     710231    19  3    36.6  51.3\n3     Arizona  AZ      West    6392017   232 11    45.1  48.7\n4    Arkansas  AR     South    2915918    93  6    33.7  60.6\n5  California  CA      West   37253956  1257 55    61.7  31.6\n6    Colorado  CO      West    5029196    65  9    48.2  43.3\n7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#setdiff",
    "href": "slides/wrangling/16-joining-tables.html#setdiff",
    "title": "Joining Tables",
    "section": "setdiff",
    "text": "setdiff\n\nThe set difference between a first and second argument can be obtained with setdiff.\nUnlike intersect and union, this function is not symmetric:\n\n\nsetdiff(1:10, 6:15) \n\n[1] 1 2 3 4 5\n\nsetdiff(6:15, 1:10) \n\n[1] 11 12 13 14 15"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#setdiff-1",
    "href": "slides/wrangling/16-joining-tables.html#setdiff-1",
    "title": "Joining Tables",
    "section": "setdiff",
    "text": "setdiff\n\nAs with the functions shown above, dplyr has a version for data frames:\n\n\ntab_1 &lt;- tab[1:5,] \ntab_2 &lt;- tab[3:7,] \ndplyr::setdiff(tab_1, tab_2) \n\n    state abb region population total ev clinton trump\n1 Alabama  AL  South    4779736   135  9    34.4  62.1\n2  Alaska  AK   West     710231    19  3    36.6  51.3"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#setequal",
    "href": "slides/wrangling/16-joining-tables.html#setequal",
    "title": "Joining Tables",
    "section": "setequal",
    "text": "setequal\n\nFinally, the function setequal tells us if two sets are the same, regardless of order.\nSo notice that:\n\n\nsetequal(1:5, 1:6) \n\n[1] FALSE\n\n\n\nbut:\n\n\nsetequal(1:5, 5:1) \n\n[1] TRUE"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#setequal-1",
    "href": "slides/wrangling/16-joining-tables.html#setequal-1",
    "title": "Joining Tables",
    "section": "setequal",
    "text": "setequal\n\nThe dplyr version checks whether data frames are equal, regardless of order of rows or columns:\n\n\ndplyr::setequal(tab_1, tab_2) \n\n[1] FALSE"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-with-data.table",
    "href": "slides/wrangling/16-joining-tables.html#joining-with-data.table",
    "title": "Joining Tables",
    "section": "Joining with data.table",
    "text": "Joining with data.table\n\nThe data.table package includes merge, a very efficient function for joining tables.\nIn tidyverse we joined two tables with left_join:\n\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\")"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-with-data.table-1",
    "href": "slides/wrangling/16-joining-tables.html#joining-with-data.table-1",
    "title": "Joining Tables",
    "section": "Joining with data.table",
    "text": "Joining with data.table\n\nIn data.table the merge functions works similarly:\n\n\nlibrary(data.table) \ntab &lt;- merge(murders, results_us_election_2016, by = \"state\", all.x = TRUE)"
  },
  {
    "objectID": "slides/wrangling/16-joining-tables.html#joining-with-data.table-2",
    "href": "slides/wrangling/16-joining-tables.html#joining-with-data.table-2",
    "title": "Joining Tables",
    "section": "Joining with data.table",
    "text": "Joining with data.table\n\nInstead of defining different functions for the different type of joins, merge uses the the logical arguments all (full join), all.x (left join), and all.y (right join)."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales",
    "href": "slides/wrangling/13-locales.html#locales",
    "title": "Locales",
    "section": "Locales",
    "text": "Locales\n\nComputer settings change depending on language and location, and being unaware of this possibility can make certain data wrangling challenges difficult to overcome."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-1",
    "href": "slides/wrangling/13-locales.html#locales-1",
    "title": "Locales",
    "section": "Locales",
    "text": "Locales\n\nThe purpose of locales is to group together common settings that can affect:\n\nMonth and day names, which are necessary for interpreting dates.\nThe standard date format.\nThe default time zone.\nCharacter encoding, vital for reading non-ASCII characters.\nThe symbols for decimals and number groupings, important for interpreting numerical values."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-2",
    "href": "slides/wrangling/13-locales.html#locales-2",
    "title": "Locales",
    "section": "Locales",
    "text": "Locales\n\nIn R, a locale refers to a suite of settings that dictate how the system should behave with respect to cultural conventions.\nThese affect the way data is formatted and presented, including date formatting, currency symbols, decimal separators, and other related aspects.\nLocales in R affect several areas, including how character vectors are sorted.\nAdditionally, errors, warnings, and other messages might be translated into languages other than English based on the locale."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-in-r",
    "href": "slides/wrangling/13-locales.html#locales-in-r",
    "title": "Locales",
    "section": "Locales in R",
    "text": "Locales in R\n\nTo access the current locale settings in R, you can use the Sys.getlocale() function:\n\n\nSys.getlocale() \n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\""
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-in-r-1",
    "href": "slides/wrangling/13-locales.html#locales-in-r-1",
    "title": "Locales",
    "section": "Locales in R",
    "text": "Locales in R\n\nTo set a specific locale, use the Sys.setlocale() function.\nFor example, to set the locale to US English:\n\n\nSys.setlocale(\"LC_ALL\", \"en_US.UTF-8\") \n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\"\n\n\n\nThe exact string to use for setting the locale (like “en_US.UTF-8”) can depend on your operating system and its configuration."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-in-r-2",
    "href": "slides/wrangling/13-locales.html#locales-in-r-2",
    "title": "Locales",
    "section": "Locales in R",
    "text": "Locales in R\n\nLC_ALL refers to all locale categories.\nR breaks down the locale into categories:\n\nLC_COLLATE: for string collation.\nLC_TIME: date and time formatting.\nLC_MONETARY: currency formatting.\nLC_MESSAGES: system message translations.\nLC_NUMERIC: number formatting.\n\nYou can set the locale for each category individually if you don’t want to use LC_ALL."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#locales-in-r-3",
    "href": "slides/wrangling/13-locales.html#locales-in-r-3",
    "title": "Locales",
    "section": "Locales in R",
    "text": "Locales in R\n\n\n\n\n\n\nWarning\n\n\n\nWe have shown tools to control locales.\nThese settings are important because they affect how your data looks and behaves.\nHowever, not all of these settings are available on every computer; their availability depends on what kind of computer you have and how it’s set up.\nChanging these settings, especially LC_NUMERIC, can lead to unexpected problems when you’re working with numbers in R.\nThese locale settings only last as long as one R session."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#the-locale-function",
    "href": "slides/wrangling/13-locales.html#the-locale-function",
    "title": "Locales",
    "section": "The locale function",
    "text": "The locale function\n\nThe readr package includes a locale() function that can be used to learn or change the current locale from within R:\n\n\nlibrary(readr) \nlocale() \n\n&lt;locale&gt;\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n&lt;date_names&gt;\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#the-locale-function-1",
    "href": "slides/wrangling/13-locales.html#the-locale-function-1",
    "title": "Locales",
    "section": "The locale function",
    "text": "The locale function\n\nYou can see all the locales available on your system by typing:\n\n\nsystem(\"locale -a\")"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#the-locale-function-2",
    "href": "slides/wrangling/13-locales.html#the-locale-function-2",
    "title": "Locales",
    "section": "The locale function",
    "text": "The locale function\n\nHere is what you obtain if you change the dates locale to Spanish:\n\n\nlocale(date_names = \"es\") \n\n&lt;locale&gt;\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n&lt;date_names&gt;\nDays:   domingo (dom.), lunes (lun.), martes (mar.), miércoles (mié.), jueves\n        (jue.), viernes (vie.), sábado (sáb.)\nMonths: enero (ene.), febrero (feb.), marzo (mar.), abril (abr.), mayo (may.),\n        junio (jun.), julio (jul.), agosto (ago.), septiembre (sept.),\n        octubre (oct.), noviembre (nov.), diciembre (dic.)\nAM/PM:  a. m./p. m."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example",
    "href": "slides/wrangling/13-locales.html#example",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nEarlier we noted that reading the file:\n\n\nfn &lt;- file.path(system.file(\"extdata\", package = \"dslabs\"), \"calificaciones.csv\") \n\n\nhad a encoding different than UTF-8, the default."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-1",
    "href": "slides/wrangling/13-locales.html#example-1",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nWe used guess_encoding to determine the correct one:\n\n\nguess_encoding(fn)$encoding[1] \n\n[1] \"ISO-8859-1\"\n\n\n\nand used the locale function to change this and read in this encoding instead:\n\n\ndat &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\"))"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-2",
    "href": "slides/wrangling/13-locales.html#example-2",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nThis file provides homework assignment scores for seven students. Columns represent the name, date of birth, the time they submitted their assignment, and their score:\n\n\nread_lines(fn, locale = locale(encoding = \"ISO-8859-1\")) \n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuación\\\"\"                       \n[2] \"\\\"Beyoncé\\\",\\\"04 de septiembre de 1981\\\",2023-09-22 02:11:02,\\\"87,5\\\"\"\n[3] \"\\\"Blümchen\\\",\\\"20 de abril de 1980\\\",2023-09-22 03:23:05,\\\"99,0\\\"\"    \n[4] \"\\\"João\\\",\\\"10 de junio de 1931\\\",2023-09-21 22:43:28,\\\"98,9\\\"\"        \n[5] \"\\\"López\\\",\\\"24 de julio de 1969\\\",2023-09-22 01:06:59,\\\"88,7\\\"\"       \n[6] \"\\\"Ñengo\\\",\\\"15 de diciembre de 1981\\\",2023-09-21 23:35:37,\\\"93,1\\\"\"   \n[7] \"\\\"Plácido\\\",\\\"24 de enero de 1941\\\",2023-09-21 23:17:21,\\\"88,7\\\"\"     \n[8] \"\\\"Thalía\\\",\\\"26 de agosto de 1971\\\",2023-09-21 23:08:02,\\\"83,0\\\"\""
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-3",
    "href": "slides/wrangling/13-locales.html#example-3",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nAs an illustrative example, we will write code to compute the students age and check if they turned in their assignment by the deadline of September 21, 2023, before midnight.\nWe can read in the file with correct encoding like this:\n\n\ndat &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\"))"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-4",
    "href": "slides/wrangling/13-locales.html#example-4",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nHowever, notice that the last column, which is supposed to contain exam scores between 0 and 100, shows numbers larger than 800:\n\n\ndat$puntuación \n\n[1] 875 990 989 887 931 887 830"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-5",
    "href": "slides/wrangling/13-locales.html#example-5",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nThis happens because the scores in the file use the European decimal point, which confuses read_csv.\nTo address this issue, we can also change the encoding to use European decimals, which fixes the problem:\n\n\ndat &lt;- read_csv(fn, locale = locale(decimal_mark = \",\", \n                                    encoding = \"ISO-8859-1\")) \ndat$puntuación \n\n[1] 87.5 99.0 98.9 88.7 93.1 88.7 83.0"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-6",
    "href": "slides/wrangling/13-locales.html#example-6",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nNow, to compute the student ages, let’s try changing the submission times to date format:\n\n\nlibrary(lubridate) \ndmy(dat$f.n.) \n\n[1] NA NA NA NA NA NA NA\n\n\n\nNothing gets converted correctly.\nThis is because the dates are in Spanish."
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-7",
    "href": "slides/wrangling/13-locales.html#example-7",
    "title": "Locales",
    "section": "Example",
    "text": "Example\nWe can change the locale to use Spanish as the language for dates:\n\nparse_date(dat$f.n., format = \"%d de %B de %Y\", locale = locale(date_names = \"es\")) \n\n[1] \"1981-09-04\" \"1980-04-20\" \"1931-06-10\" \"1969-07-24\" \"1981-12-15\"\n[6] \"1941-01-24\" \"1971-08-26\""
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-8",
    "href": "slides/wrangling/13-locales.html#example-8",
    "title": "Locales",
    "section": "Example",
    "text": "Example\nWe can also reread the file using the correct locales:\n\ndat &lt;- read_csv(fn, locale = locale(date_names = \"es\", \n                                    date_format = \"%d de %B de %Y\", \n                                    decimal_mark = \",\", \n                                    encoding = \"ISO-8859-1\"))"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-9",
    "href": "slides/wrangling/13-locales.html#example-9",
    "title": "Locales",
    "section": "Example",
    "text": "Example\nComputing the students’ ages is now straightforward:\n\ntime_length(today() - dat$f.n., unit = \"years\") |&gt; floor() \n\n[1] 43 44 93 55 42 83 53"
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-10",
    "href": "slides/wrangling/13-locales.html#example-10",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nLet’s check which students turned in their homework past the deadline of September 22:\n\n\ndat$estampa &gt;= make_date(2023, 9, 22) \n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\n\nWe see that two students where late.\nHowever, with times we have to be particularly careful as some functions default to the UTC timezone:\n\n\ntz(dat$estampa) \n\n[1] \"UTC\""
  },
  {
    "objectID": "slides/wrangling/13-locales.html#example-11",
    "href": "slides/wrangling/13-locales.html#example-11",
    "title": "Locales",
    "section": "Example",
    "text": "Example\n\nIf we change to the timezone to Eastern Standard Time (EST), we see no one was late:\n\n\nwith_tz(dat$estampa, tz =  \"EST\") &gt;= make_date(2023, 9, 22) \n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#parsing-dates-and-times",
    "href": "slides/wrangling/12-dates-and-times.html#parsing-dates-and-times",
    "title": "Dates And Times",
    "section": "Parsing dates and times",
    "text": "Parsing dates and times\n\nWe have described three main types of vectors: numeric, character, and logical.\nWhen analyzing data, we often encounter variables that are dates.\nAlthough we can represent a date with a string, for example November 2, 2017, once we pick a reference day, referred to as the epoch by computer programmers, they can be converted to numbers by calculating the number of days since the epoch.\nIn R and Unix, the epoch is defined as January 1, 1970."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#parsing-dates-and-times-1",
    "href": "slides/wrangling/12-dates-and-times.html#parsing-dates-and-times-1",
    "title": "Dates And Times",
    "section": "Parsing dates and times",
    "text": "Parsing dates and times\n\nIf I tell you it’s November 2, 2017, you know what this means immediately.\nIf I tell you it’s day 17,204, you will be quite confused.\nSimilar problems arise with times and even more complications can appear due to time zones.\nFor this reason, R defines a data type just for dates and times."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-date-data-type",
    "href": "slides/wrangling/12-dates-and-times.html#the-date-data-type",
    "title": "Dates And Times",
    "section": "The date data type",
    "text": "The date data type\n\nWe can see an example of the data type R uses for data here:\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \npolls_us_election_2016$startdate |&gt; head() \n\n[1] \"2016-11-03\" \"2016-11-01\" \"2016-11-02\" \"2016-11-04\" \"2016-11-03\"\n[6] \"2016-11-03\"\n\n\n\nThe dates look like strings, but they are not:\n\n\nclass(polls_us_election_2016$startdate) \n\n[1] \"Date\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-date-data-type-1",
    "href": "slides/wrangling/12-dates-and-times.html#the-date-data-type-1",
    "title": "Dates And Times",
    "section": "The date data type",
    "text": "The date data type\n\nLook at what happens when we convert them to numbers:\n\n\nas.numeric(polls_us_election_2016$startdate) |&gt; head() \n\n[1] 17108 17106 17107 17109 17108 17108\n\n\n\nIt turns them into days since the epoch."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-date-data-type-2",
    "href": "slides/wrangling/12-dates-and-times.html#the-date-data-type-2",
    "title": "Dates And Times",
    "section": "The date data type",
    "text": "The date data type\n\nas.Date converts characters into dates.\nSo to see that the epoch is day 0 we can type.\n\n\nas.Date(\"1970-01-01\") |&gt; as.numeric() \n\n[1] 0\n\n\n\nPlotting functions, such as those in ggplot, are aware of the date format."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-date-data-type-3",
    "href": "slides/wrangling/12-dates-and-times.html#the-date-data-type-3",
    "title": "Dates And Times",
    "section": "The date data type",
    "text": "The date data type\n\nScatterplots use the numeric representation to assign positions, but include the string in the labels:\n\n\npolls_us_election_2016 |&gt; \n  filter(startdate &gt;= make_date(2016, 6, 1)) |&gt;\n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  ggplot(aes(startdate, rawpoll_clinton)) + \n  geom_line()"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-date-data-type-4",
    "href": "slides/wrangling/12-dates-and-times.html#the-date-data-type-4",
    "title": "Dates And Times",
    "section": "The date data type",
    "text": "The date data type\n\nCodePlot\n\n\nggplot offers convenient functions to change labels:\n\npolls_us_election_2016 |&gt; \n  filter(startdate &gt;= make_date(2016, 6, 1)) |&gt;\n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  ggplot(aes(startdate, rawpoll_clinton)) + \n  geom_line() +\n  scale_x_date(date_breaks = \"2 weeks\", date_labels = \"%b %d\") + \n  geom_vline(xintercept = as.Date(\"2016-10-28\"), linetype = \"dashed\")"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe lubridate package provides tools to work with date and times.\n\n\nlibrary(lubridate) \n\n\nWe will take a random sample of dates to show some of the useful things one can do:\n\n\nset.seed(2002) \ndates &lt;- sample(polls_us_election_2016$startdate, 10) |&gt; sort() \ndates \n\n [1] \"2016-05-31\" \"2016-08-08\" \"2016-08-19\" \"2016-09-22\" \"2016-09-27\"\n [6] \"2016-10-12\" \"2016-10-24\" \"2016-10-26\" \"2016-10-29\" \"2016-10-30\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-1",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-1",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe functions year, month and day extract those values:\n\n\ntibble(date = dates, month = month(dates), day = day(dates), year = year(dates)) \n\n# A tibble: 10 × 4\n   date       month   day  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 2016-05-31     5    31  2016\n 2 2016-08-08     8     8  2016\n 3 2016-08-19     8    19  2016\n 4 2016-09-22     9    22  2016\n 5 2016-09-27     9    27  2016\n 6 2016-10-12    10    12  2016\n 7 2016-10-24    10    24  2016\n 8 2016-10-26    10    26  2016\n 9 2016-10-29    10    29  2016\n10 2016-10-30    10    30  2016"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-2",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-2",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nWe can also extract the month labels:\n\n\nmonth(dates, label = TRUE) \n\n [1] May Aug Aug Sep Sep Oct Oct Oct Oct Oct\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-3",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-3",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nAnother useful set of functions are the parsers that convert strings into dates.\nThe function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\n\n\nx &lt;- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\", \n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\") \nymd(x) \n\n[1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n[6] \"2009-01-06\" \"2009-01-07\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-4",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-4",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different.\nThe preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601.\nSpecifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date.\nYou can see the function ymd returns them in this format."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-5",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-5",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002.\nIn these cases, examining the entire vector of dates will help you determine what format it is by process of elimination.\nOnce you know, you can use the many parses provided by lubridate.\nFor example, if the string is:\n\n\nx &lt;- \"09/01/02\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-6",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-6",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe ymd function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to:\n\n\nymd(x) \n\n[1] \"2009-01-02\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-7",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-7",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe mdy function assumes the first entry is the month, then the day, then the year:\n\n\nmdy(x) \n\n[1] \"2002-09-01\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-8",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-8",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe lubridate package provides a function for every possibility.\nHere is the other common one:\n\n\ndmy(x) \n\n[1] \"2002-01-09\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-9",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-9",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe lubridate package is also useful for dealing with times.\nIn base R, you can get the current time typing Sys.time().\nThe lubridate package provides a slightly more advanced function, now, that permits you to define the time zone:\n\n\nnow() \n\n[1] \"2024-10-27 22:42:35 EDT\"\n\nnow(\"GMT\") \n\n[1] \"2024-10-28 02:42:35 GMT\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-10",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-10",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nYou can see all the available time zones with:\n\n\nOlsonNames()\n\n  [1] \"Africa/Abidjan\"                   \"Africa/Accra\"                    \n  [3] \"Africa/Addis_Ababa\"               \"Africa/Algiers\"                  \n  [5] \"Africa/Asmara\"                    \"Africa/Asmera\"                   \n  [7] \"Africa/Bamako\"                    \"Africa/Bangui\"                   \n  [9] \"Africa/Banjul\"                    \"Africa/Bissau\"                   \n [11] \"Africa/Blantyre\"                  \"Africa/Brazzaville\"              \n [13] \"Africa/Bujumbura\"                 \"Africa/Cairo\"                    \n [15] \"Africa/Casablanca\"                \"Africa/Ceuta\"                    \n [17] \"Africa/Conakry\"                   \"Africa/Dakar\"                    \n [19] \"Africa/Dar_es_Salaam\"             \"Africa/Djibouti\"                 \n [21] \"Africa/Douala\"                    \"Africa/El_Aaiun\"                 \n [23] \"Africa/Freetown\"                  \"Africa/Gaborone\"                 \n [25] \"Africa/Harare\"                    \"Africa/Johannesburg\"             \n [27] \"Africa/Juba\"                      \"Africa/Kampala\"                  \n [29] \"Africa/Khartoum\"                  \"Africa/Kigali\"                   \n [31] \"Africa/Kinshasa\"                  \"Africa/Lagos\"                    \n [33] \"Africa/Libreville\"                \"Africa/Lome\"                     \n [35] \"Africa/Luanda\"                    \"Africa/Lubumbashi\"               \n [37] \"Africa/Lusaka\"                    \"Africa/Malabo\"                   \n [39] \"Africa/Maputo\"                    \"Africa/Maseru\"                   \n [41] \"Africa/Mbabane\"                   \"Africa/Mogadishu\"                \n [43] \"Africa/Monrovia\"                  \"Africa/Nairobi\"                  \n [45] \"Africa/Ndjamena\"                  \"Africa/Niamey\"                   \n [47] \"Africa/Nouakchott\"                \"Africa/Ouagadougou\"              \n [49] \"Africa/Porto-Novo\"                \"Africa/Sao_Tome\"                 \n [51] \"Africa/Timbuktu\"                  \"Africa/Tripoli\"                  \n [53] \"Africa/Tunis\"                     \"Africa/Windhoek\"                 \n [55] \"America/Adak\"                     \"America/Anchorage\"               \n [57] \"America/Anguilla\"                 \"America/Antigua\"                 \n [59] \"America/Araguaina\"                \"America/Argentina/Buenos_Aires\"  \n [61] \"America/Argentina/Catamarca\"      \"America/Argentina/ComodRivadavia\"\n [63] \"America/Argentina/Cordoba\"        \"America/Argentina/Jujuy\"         \n [65] \"America/Argentina/La_Rioja\"       \"America/Argentina/Mendoza\"       \n [67] \"America/Argentina/Rio_Gallegos\"   \"America/Argentina/Salta\"         \n [69] \"America/Argentina/San_Juan\"       \"America/Argentina/San_Luis\"      \n [71] \"America/Argentina/Tucuman\"        \"America/Argentina/Ushuaia\"       \n [73] \"America/Aruba\"                    \"America/Asuncion\"                \n [75] \"America/Atikokan\"                 \"America/Atka\"                    \n [77] \"America/Bahia\"                    \"America/Bahia_Banderas\"          \n [79] \"America/Barbados\"                 \"America/Belem\"                   \n [81] \"America/Belize\"                   \"America/Blanc-Sablon\"            \n [83] \"America/Boa_Vista\"                \"America/Bogota\"                  \n [85] \"America/Boise\"                    \"America/Buenos_Aires\"            \n [87] \"America/Cambridge_Bay\"            \"America/Campo_Grande\"            \n [89] \"America/Cancun\"                   \"America/Caracas\"                 \n [91] \"America/Catamarca\"                \"America/Cayenne\"                 \n [93] \"America/Cayman\"                   \"America/Chicago\"                 \n [95] \"America/Chihuahua\"                \"America/Ciudad_Juarez\"           \n [97] \"America/Coral_Harbour\"            \"America/Cordoba\"                 \n [99] \"America/Costa_Rica\"               \"America/Creston\"                 \n[101] \"America/Cuiaba\"                   \"America/Curacao\"                 \n[103] \"America/Danmarkshavn\"             \"America/Dawson\"                  \n[105] \"America/Dawson_Creek\"             \"America/Denver\"                  \n[107] \"America/Detroit\"                  \"America/Dominica\"                \n[109] \"America/Edmonton\"                 \"America/Eirunepe\"                \n[111] \"America/El_Salvador\"              \"America/Ensenada\"                \n[113] \"America/Fort_Nelson\"              \"America/Fort_Wayne\"              \n[115] \"America/Fortaleza\"                \"America/Glace_Bay\"               \n[117] \"America/Godthab\"                  \"America/Goose_Bay\"               \n[119] \"America/Grand_Turk\"               \"America/Grenada\"                 \n[121] \"America/Guadeloupe\"               \"America/Guatemala\"               \n[123] \"America/Guayaquil\"                \"America/Guyana\"                  \n[125] \"America/Halifax\"                  \"America/Havana\"                  \n[127] \"America/Hermosillo\"               \"America/Indiana/Indianapolis\"    \n[129] \"America/Indiana/Knox\"             \"America/Indiana/Marengo\"         \n[131] \"America/Indiana/Petersburg\"       \"America/Indiana/Tell_City\"       \n[133] \"America/Indiana/Vevay\"            \"America/Indiana/Vincennes\"       \n[135] \"America/Indiana/Winamac\"          \"America/Indianapolis\"            \n[137] \"America/Inuvik\"                   \"America/Iqaluit\"                 \n[139] \"America/Jamaica\"                  \"America/Jujuy\"                   \n[141] \"America/Juneau\"                   \"America/Kentucky/Louisville\"     \n[143] \"America/Kentucky/Monticello\"      \"America/Knox_IN\"                 \n[145] \"America/Kralendijk\"               \"America/La_Paz\"                  \n[147] \"America/Lima\"                     \"America/Los_Angeles\"             \n[149] \"America/Louisville\"               \"America/Lower_Princes\"           \n[151] \"America/Maceio\"                   \"America/Managua\"                 \n[153] \"America/Manaus\"                   \"America/Marigot\"                 \n[155] \"America/Martinique\"               \"America/Matamoros\"               \n[157] \"America/Mazatlan\"                 \"America/Mendoza\"                 \n[159] \"America/Menominee\"                \"America/Merida\"                  \n[161] \"America/Metlakatla\"               \"America/Mexico_City\"             \n[163] \"America/Miquelon\"                 \"America/Moncton\"                 \n[165] \"America/Monterrey\"                \"America/Montevideo\"              \n[167] \"America/Montreal\"                 \"America/Montserrat\"              \n[169] \"America/Nassau\"                   \"America/New_York\"                \n[171] \"America/Nipigon\"                  \"America/Nome\"                    \n[173] \"America/Noronha\"                  \"America/North_Dakota/Beulah\"     \n[175] \"America/North_Dakota/Center\"      \"America/North_Dakota/New_Salem\"  \n[177] \"America/Nuuk\"                     \"America/Ojinaga\"                 \n[179] \"America/Panama\"                   \"America/Pangnirtung\"             \n[181] \"America/Paramaribo\"               \"America/Phoenix\"                 \n[183] \"America/Port_of_Spain\"            \"America/Port-au-Prince\"          \n[185] \"America/Porto_Acre\"               \"America/Porto_Velho\"             \n[187] \"America/Puerto_Rico\"              \"America/Punta_Arenas\"            \n[189] \"America/Rainy_River\"              \"America/Rankin_Inlet\"            \n[191] \"America/Recife\"                   \"America/Regina\"                  \n[193] \"America/Resolute\"                 \"America/Rio_Branco\"              \n[195] \"America/Rosario\"                  \"America/Santa_Isabel\"            \n[197] \"America/Santarem\"                 \"America/Santiago\"                \n[199] \"America/Santo_Domingo\"            \"America/Sao_Paulo\"               \n[201] \"America/Scoresbysund\"             \"America/Shiprock\"                \n[203] \"America/Sitka\"                    \"America/St_Barthelemy\"           \n[205] \"America/St_Johns\"                 \"America/St_Kitts\"                \n[207] \"America/St_Lucia\"                 \"America/St_Thomas\"               \n[209] \"America/St_Vincent\"               \"America/Swift_Current\"           \n[211] \"America/Tegucigalpa\"              \"America/Thule\"                   \n[213] \"America/Thunder_Bay\"              \"America/Tijuana\"                 \n[215] \"America/Toronto\"                  \"America/Tortola\"                 \n[217] \"America/Vancouver\"                \"America/Virgin\"                  \n[219] \"America/Whitehorse\"               \"America/Winnipeg\"                \n[221] \"America/Yakutat\"                  \"America/Yellowknife\"             \n[223] \"Antarctica/Casey\"                 \"Antarctica/Davis\"                \n[225] \"Antarctica/DumontDUrville\"        \"Antarctica/Macquarie\"            \n[227] \"Antarctica/Mawson\"                \"Antarctica/McMurdo\"              \n[229] \"Antarctica/Palmer\"                \"Antarctica/Rothera\"              \n[231] \"Antarctica/South_Pole\"            \"Antarctica/Syowa\"                \n[233] \"Antarctica/Troll\"                 \"Antarctica/Vostok\"               \n[235] \"Arctic/Longyearbyen\"              \"Asia/Aden\"                       \n[237] \"Asia/Almaty\"                      \"Asia/Amman\"                      \n[239] \"Asia/Anadyr\"                      \"Asia/Aqtau\"                      \n[241] \"Asia/Aqtobe\"                      \"Asia/Ashgabat\"                   \n[243] \"Asia/Ashkhabad\"                   \"Asia/Atyrau\"                     \n[245] \"Asia/Baghdad\"                     \"Asia/Bahrain\"                    \n[247] \"Asia/Baku\"                        \"Asia/Bangkok\"                    \n[249] \"Asia/Barnaul\"                     \"Asia/Beirut\"                     \n[251] \"Asia/Bishkek\"                     \"Asia/Brunei\"                     \n[253] \"Asia/Calcutta\"                    \"Asia/Chita\"                      \n[255] \"Asia/Choibalsan\"                  \"Asia/Chongqing\"                  \n[257] \"Asia/Chungking\"                   \"Asia/Colombo\"                    \n[259] \"Asia/Dacca\"                       \"Asia/Damascus\"                   \n[261] \"Asia/Dhaka\"                       \"Asia/Dili\"                       \n[263] \"Asia/Dubai\"                       \"Asia/Dushanbe\"                   \n[265] \"Asia/Famagusta\"                   \"Asia/Gaza\"                       \n[267] \"Asia/Harbin\"                      \"Asia/Hebron\"                     \n[269] \"Asia/Ho_Chi_Minh\"                 \"Asia/Hong_Kong\"                  \n[271] \"Asia/Hovd\"                        \"Asia/Irkutsk\"                    \n[273] \"Asia/Istanbul\"                    \"Asia/Jakarta\"                    \n[275] \"Asia/Jayapura\"                    \"Asia/Jerusalem\"                  \n[277] \"Asia/Kabul\"                       \"Asia/Kamchatka\"                  \n[279] \"Asia/Karachi\"                     \"Asia/Kashgar\"                    \n[281] \"Asia/Kathmandu\"                   \"Asia/Katmandu\"                   \n[283] \"Asia/Khandyga\"                    \"Asia/Kolkata\"                    \n[285] \"Asia/Krasnoyarsk\"                 \"Asia/Kuala_Lumpur\"               \n[287] \"Asia/Kuching\"                     \"Asia/Kuwait\"                     \n[289] \"Asia/Macao\"                       \"Asia/Macau\"                      \n[291] \"Asia/Magadan\"                     \"Asia/Makassar\"                   \n[293] \"Asia/Manila\"                      \"Asia/Muscat\"                     \n[295] \"Asia/Nicosia\"                     \"Asia/Novokuznetsk\"               \n[297] \"Asia/Novosibirsk\"                 \"Asia/Omsk\"                       \n[299] \"Asia/Oral\"                        \"Asia/Phnom_Penh\"                 \n[301] \"Asia/Pontianak\"                   \"Asia/Pyongyang\"                  \n[303] \"Asia/Qatar\"                       \"Asia/Qostanay\"                   \n[305] \"Asia/Qyzylorda\"                   \"Asia/Rangoon\"                    \n[307] \"Asia/Riyadh\"                      \"Asia/Saigon\"                     \n[309] \"Asia/Sakhalin\"                    \"Asia/Samarkand\"                  \n[311] \"Asia/Seoul\"                       \"Asia/Shanghai\"                   \n[313] \"Asia/Singapore\"                   \"Asia/Srednekolymsk\"              \n[315] \"Asia/Taipei\"                      \"Asia/Tashkent\"                   \n[317] \"Asia/Tbilisi\"                     \"Asia/Tehran\"                     \n[319] \"Asia/Tel_Aviv\"                    \"Asia/Thimbu\"                     \n[321] \"Asia/Thimphu\"                     \"Asia/Tokyo\"                      \n[323] \"Asia/Tomsk\"                       \"Asia/Ujung_Pandang\"              \n[325] \"Asia/Ulaanbaatar\"                 \"Asia/Ulan_Bator\"                 \n[327] \"Asia/Urumqi\"                      \"Asia/Ust-Nera\"                   \n[329] \"Asia/Vientiane\"                   \"Asia/Vladivostok\"                \n[331] \"Asia/Yakutsk\"                     \"Asia/Yangon\"                     \n[333] \"Asia/Yekaterinburg\"               \"Asia/Yerevan\"                    \n[335] \"Atlantic/Azores\"                  \"Atlantic/Bermuda\"                \n[337] \"Atlantic/Canary\"                  \"Atlantic/Cape_Verde\"             \n[339] \"Atlantic/Faeroe\"                  \"Atlantic/Faroe\"                  \n[341] \"Atlantic/Jan_Mayen\"               \"Atlantic/Madeira\"                \n[343] \"Atlantic/Reykjavik\"               \"Atlantic/South_Georgia\"          \n[345] \"Atlantic/St_Helena\"               \"Atlantic/Stanley\"                \n[347] \"Australia/ACT\"                    \"Australia/Adelaide\"              \n[349] \"Australia/Brisbane\"               \"Australia/Broken_Hill\"           \n[351] \"Australia/Canberra\"               \"Australia/Currie\"                \n[353] \"Australia/Darwin\"                 \"Australia/Eucla\"                 \n[355] \"Australia/Hobart\"                 \"Australia/LHI\"                   \n[357] \"Australia/Lindeman\"               \"Australia/Lord_Howe\"             \n[359] \"Australia/Melbourne\"              \"Australia/North\"                 \n[361] \"Australia/NSW\"                    \"Australia/Perth\"                 \n[363] \"Australia/Queensland\"             \"Australia/South\"                 \n[365] \"Australia/Sydney\"                 \"Australia/Tasmania\"              \n[367] \"Australia/Victoria\"               \"Australia/West\"                  \n[369] \"Australia/Yancowinna\"             \"Brazil/Acre\"                     \n[371] \"Brazil/DeNoronha\"                 \"Brazil/East\"                     \n[373] \"Brazil/West\"                      \"Canada/Atlantic\"                 \n[375] \"Canada/Central\"                   \"Canada/Eastern\"                  \n[377] \"Canada/Mountain\"                  \"Canada/Newfoundland\"             \n[379] \"Canada/Pacific\"                   \"Canada/Saskatchewan\"             \n[381] \"Canada/Yukon\"                     \"CET\"                             \n[383] \"Chile/Continental\"                \"Chile/EasterIsland\"              \n[385] \"CST6CDT\"                          \"Cuba\"                            \n[387] \"EET\"                              \"Egypt\"                           \n[389] \"Eire\"                             \"EST\"                             \n[391] \"EST5EDT\"                          \"Etc/GMT\"                         \n[393] \"Etc/GMT-0\"                        \"Etc/GMT-1\"                       \n[395] \"Etc/GMT-10\"                       \"Etc/GMT-11\"                      \n[397] \"Etc/GMT-12\"                       \"Etc/GMT-13\"                      \n[399] \"Etc/GMT-14\"                       \"Etc/GMT-2\"                       \n[401] \"Etc/GMT-3\"                        \"Etc/GMT-4\"                       \n[403] \"Etc/GMT-5\"                        \"Etc/GMT-6\"                       \n[405] \"Etc/GMT-7\"                        \"Etc/GMT-8\"                       \n[407] \"Etc/GMT-9\"                        \"Etc/GMT+0\"                       \n[409] \"Etc/GMT+1\"                        \"Etc/GMT+10\"                      \n[411] \"Etc/GMT+11\"                       \"Etc/GMT+12\"                      \n[413] \"Etc/GMT+2\"                        \"Etc/GMT+3\"                       \n[415] \"Etc/GMT+4\"                        \"Etc/GMT+5\"                       \n[417] \"Etc/GMT+6\"                        \"Etc/GMT+7\"                       \n[419] \"Etc/GMT+8\"                        \"Etc/GMT+9\"                       \n[421] \"Etc/GMT0\"                         \"Etc/Greenwich\"                   \n[423] \"Etc/UCT\"                          \"Etc/Universal\"                   \n[425] \"Etc/UTC\"                          \"Etc/Zulu\"                        \n[427] \"Europe/Amsterdam\"                 \"Europe/Andorra\"                  \n[429] \"Europe/Astrakhan\"                 \"Europe/Athens\"                   \n[431] \"Europe/Belfast\"                   \"Europe/Belgrade\"                 \n[433] \"Europe/Berlin\"                    \"Europe/Bratislava\"               \n[435] \"Europe/Brussels\"                  \"Europe/Bucharest\"                \n[437] \"Europe/Budapest\"                  \"Europe/Busingen\"                 \n[439] \"Europe/Chisinau\"                  \"Europe/Copenhagen\"               \n[441] \"Europe/Dublin\"                    \"Europe/Gibraltar\"                \n[443] \"Europe/Guernsey\"                  \"Europe/Helsinki\"                 \n[445] \"Europe/Isle_of_Man\"               \"Europe/Istanbul\"                 \n[447] \"Europe/Jersey\"                    \"Europe/Kaliningrad\"              \n[449] \"Europe/Kiev\"                      \"Europe/Kirov\"                    \n[451] \"Europe/Kyiv\"                      \"Europe/Lisbon\"                   \n[453] \"Europe/Ljubljana\"                 \"Europe/London\"                   \n[455] \"Europe/Luxembourg\"                \"Europe/Madrid\"                   \n[457] \"Europe/Malta\"                     \"Europe/Mariehamn\"                \n[459] \"Europe/Minsk\"                     \"Europe/Monaco\"                   \n[461] \"Europe/Moscow\"                    \"Europe/Nicosia\"                  \n[463] \"Europe/Oslo\"                      \"Europe/Paris\"                    \n[465] \"Europe/Podgorica\"                 \"Europe/Prague\"                   \n[467] \"Europe/Riga\"                      \"Europe/Rome\"                     \n[469] \"Europe/Samara\"                    \"Europe/San_Marino\"               \n[471] \"Europe/Sarajevo\"                  \"Europe/Saratov\"                  \n[473] \"Europe/Simferopol\"                \"Europe/Skopje\"                   \n[475] \"Europe/Sofia\"                     \"Europe/Stockholm\"                \n[477] \"Europe/Tallinn\"                   \"Europe/Tirane\"                   \n[479] \"Europe/Tiraspol\"                  \"Europe/Ulyanovsk\"                \n[481] \"Europe/Uzhgorod\"                  \"Europe/Vaduz\"                    \n[483] \"Europe/Vatican\"                   \"Europe/Vienna\"                   \n[485] \"Europe/Vilnius\"                   \"Europe/Volgograd\"                \n[487] \"Europe/Warsaw\"                    \"Europe/Zagreb\"                   \n[489] \"Europe/Zaporozhye\"                \"Europe/Zurich\"                   \n[491] \"Factory\"                          \"GB\"                              \n[493] \"GB-Eire\"                          \"GMT\"                             \n[495] \"GMT-0\"                            \"GMT+0\"                           \n[497] \"GMT0\"                             \"Greenwich\"                       \n[499] \"Hongkong\"                         \"HST\"                             \n[501] \"Iceland\"                          \"Indian/Antananarivo\"             \n[503] \"Indian/Chagos\"                    \"Indian/Christmas\"                \n[505] \"Indian/Cocos\"                     \"Indian/Comoro\"                   \n[507] \"Indian/Kerguelen\"                 \"Indian/Mahe\"                     \n[509] \"Indian/Maldives\"                  \"Indian/Mauritius\"                \n[511] \"Indian/Mayotte\"                   \"Indian/Reunion\"                  \n[513] \"Iran\"                             \"Israel\"                          \n[515] \"Jamaica\"                          \"Japan\"                           \n[517] \"Kwajalein\"                        \"Libya\"                           \n[519] \"MET\"                              \"Mexico/BajaNorte\"                \n[521] \"Mexico/BajaSur\"                   \"Mexico/General\"                  \n[523] \"MST\"                              \"MST7MDT\"                         \n[525] \"Navajo\"                           \"NZ\"                              \n[527] \"NZ-CHAT\"                          \"Pacific/Apia\"                    \n[529] \"Pacific/Auckland\"                 \"Pacific/Bougainville\"            \n[531] \"Pacific/Chatham\"                  \"Pacific/Chuuk\"                   \n[533] \"Pacific/Easter\"                   \"Pacific/Efate\"                   \n[535] \"Pacific/Enderbury\"                \"Pacific/Fakaofo\"                 \n[537] \"Pacific/Fiji\"                     \"Pacific/Funafuti\"                \n[539] \"Pacific/Galapagos\"                \"Pacific/Gambier\"                 \n[541] \"Pacific/Guadalcanal\"              \"Pacific/Guam\"                    \n[543] \"Pacific/Honolulu\"                 \"Pacific/Johnston\"                \n[545] \"Pacific/Kanton\"                   \"Pacific/Kiritimati\"              \n[547] \"Pacific/Kosrae\"                   \"Pacific/Kwajalein\"               \n[549] \"Pacific/Majuro\"                   \"Pacific/Marquesas\"               \n[551] \"Pacific/Midway\"                   \"Pacific/Nauru\"                   \n[553] \"Pacific/Niue\"                     \"Pacific/Norfolk\"                 \n[555] \"Pacific/Noumea\"                   \"Pacific/Pago_Pago\"               \n[557] \"Pacific/Palau\"                    \"Pacific/Pitcairn\"                \n[559] \"Pacific/Pohnpei\"                  \"Pacific/Ponape\"                  \n[561] \"Pacific/Port_Moresby\"             \"Pacific/Rarotonga\"               \n[563] \"Pacific/Saipan\"                   \"Pacific/Samoa\"                   \n[565] \"Pacific/Tahiti\"                   \"Pacific/Tarawa\"                  \n[567] \"Pacific/Tongatapu\"                \"Pacific/Truk\"                    \n[569] \"Pacific/Wake\"                     \"Pacific/Wallis\"                  \n[571] \"Pacific/Yap\"                      \"Poland\"                          \n[573] \"Portugal\"                         \"PRC\"                             \n[575] \"PST8PDT\"                          \"ROC\"                             \n[577] \"ROK\"                              \"Singapore\"                       \n[579] \"Turkey\"                           \"UCT\"                             \n[581] \"Universal\"                        \"US/Alaska\"                       \n[583] \"US/Aleutian\"                      \"US/Arizona\"                      \n[585] \"US/Central\"                       \"US/East-Indiana\"                 \n[587] \"US/Eastern\"                       \"US/Hawaii\"                       \n[589] \"US/Indiana-Starke\"                \"US/Michigan\"                     \n[591] \"US/Mountain\"                      \"US/Pacific\"                      \n[593] \"US/Samoa\"                         \"UTC\"                             \n[595] \"W-SU\"                             \"WET\"                             \n[597] \"Zulu\"                            \nattr(,\"Version\")\n[1] \"2024a\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-11",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-11",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nWe can also extract hours, minutes, and seconds:\n\n\nnow() |&gt; hour() \n\n[1] 22\n\nnow() |&gt; minute() \n\n[1] 42\n\nnow() |&gt; second() \n\n[1] 35.90475"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-12",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-12",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\n\n\nx &lt;- c(\"12:34:56\") \nhms(x) \n\n[1] \"12H 34M 56S\"\n\nx &lt;- \"Nov/2/2012 12:34:56\" \nmdy_hms(x) \n\n[1] \"2012-11-02 12:34:56 UTC\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-13",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-13",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nThe make_date function can be used to quickly create a date object.\nFor example, to create an date object representing, for example, July 6, 2019 we write:\n\n\nmake_date(2019, 7, 6) \n\n[1] \"2019-07-06\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-14",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-14",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nWe can use it to make vectors of dates.\nTo make a vector of January 1 for the 80s we write:\n\n\nmake_date(1980:1989) \n\n [1] \"1980-01-01\" \"1981-01-01\" \"1982-01-01\" \"1983-01-01\" \"1984-01-01\"\n [6] \"1985-01-01\" \"1986-01-01\" \"1987-01-01\" \"1988-01-01\" \"1989-01-01\""
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-15",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-15",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nAnother very useful function is round_date.\nIt can be used to round dates to nearest year, quarter, month, week, day, hour, minutes, or seconds."
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-16",
    "href": "slides/wrangling/12-dates-and-times.html#the-lubridate-package-16",
    "title": "Dates And Times",
    "section": "The lubridate package",
    "text": "The lubridate package\n\nCodePlot\n\n\n\nExample: if we want to group all the polls by week of the year we can do the following:\n\n\npolls_us_election_2016 |&gt;  \n  mutate(week = round_date(startdate, \"week\")) |&gt; \n  group_by(week) |&gt; \n  summarize(margin = mean(rawpoll_clinton - rawpoll_trump)) |&gt; \n  ggplot(aes(week, margin)) + \n  geom_point()"
  },
  {
    "objectID": "slides/wrangling/12-dates-and-times.html#final-pointers",
    "href": "slides/wrangling/12-dates-and-times.html#final-pointers",
    "title": "Dates And Times",
    "section": "Final pointers",
    "text": "Final pointers\n\nYou should be aware the there are useful function for computing operations on time such a difftime, time_length, and interval.\nWe don’t cover it here but be aware that The data.table package includes some of the same functionality as lubridate."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#scraping-html",
    "href": "slides/wrangling/15-web-scraping.html#scraping-html",
    "title": "Web Scraping",
    "section": "Scraping HTML",
    "text": "Scraping HTML\n\nThe data we need to answer a question is not always in a spreadsheet ready for us to read.\nFor example, the US murders dataset we used in the R Basics chapter originally comes from this Wikipedia page:\n\n\nurl &lt;- paste0(\"https://en.wikipedia.org/w/index.php?title=\", \n              \"Gun_violence_in_the_United_States_by_state\", \n              \"&direction=prev&oldid=810166167\")"
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#scraping-html-1",
    "href": "slides/wrangling/15-web-scraping.html#scraping-html-1",
    "title": "Web Scraping",
    "section": "Scraping HTML",
    "text": "Scraping HTML\n\nYou can see the data table when you visit the webpage:\n\n.\n\nWeb scraping, or web harvesting, is the term we use to describe the process of extracting data from a website."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#scraping-html-2",
    "href": "slides/wrangling/15-web-scraping.html#scraping-html-2",
    "title": "Web Scraping",
    "section": "Scraping HTML",
    "text": "Scraping HTML\n\nThe reason we can do this is because the information used by a browser to render webpages is received as a text file from a server.\nThe text is code written in hyper text markup language (HTML).\nEvery browser has a way to show the html source code for a page, each one different.\nOn Chrome, you can use Control-U on a PC and command+alt+U on a Mac."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#scraping-html-3",
    "href": "slides/wrangling/15-web-scraping.html#scraping-html-3",
    "title": "Web Scraping",
    "section": "Scraping HTML",
    "text": "Scraping HTML\n."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#html",
    "href": "slides/wrangling/15-web-scraping.html#html",
    "title": "Web Scraping",
    "section": "HTML",
    "text": "HTML\n\nBecause this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page.\nHowever, once we look at HTML code, this might seem like a daunting task.\nBut we will show you some convenient tools to facilitate the process."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#html-1",
    "href": "slides/wrangling/15-web-scraping.html#html-1",
    "title": "Web Scraping",
    "section": "HTML",
    "text": "HTML\n\nTo get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:\n\n&lt;table class=\"wikitable sortable\"&gt; \n&lt;tr&gt; \n&lt;th&gt;State&lt;/th&gt; \n&lt;th&gt;&lt;a href=\"/wiki/List_of_U.S._states_and_territories_by_population\"  \ntitle=\"List of U.S. states and territories by population\"&gt;Population&lt;/a&gt;&lt;br /&gt; \n&lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt; \n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-1\" class=\"reference\"&gt; \n&lt;a href=\"#cite_note-1\"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt; \n&lt;th&gt;Murders and Nonnegligent \n&lt;p&gt;Manslaughter&lt;br /&gt; \n&lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt; \n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-2\" class=\"reference\"&gt; \n&lt;a href=\"#cite_note-2\"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; \n&lt;/th&gt; \n&lt;th&gt;Murder and Nonnegligent \n&lt;p&gt;Manslaughter Rate&lt;br /&gt; \n&lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt; \n&lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt; \n&lt;/th&gt; \n&lt;/tr&gt; \n&lt;tr&gt; \n&lt;td&gt;&lt;a href=\"/wiki/Alabama\" title=\"Alabama\"&gt;Alabama&lt;/a&gt;&lt;/td&gt; \n&lt;td&gt;4,853,875&lt;/td&gt; \n&lt;td&gt;348&lt;/td&gt; \n&lt;td&gt;7.2&lt;/td&gt; \n&lt;/tr&gt; \n&lt;tr&gt; \n&lt;td&gt;&lt;a href=\"/wiki/Alaska\" title=\"Alaska\"&gt;Alaska&lt;/a&gt;&lt;/td&gt; \n&lt;td&gt;737,709&lt;/td&gt; \n&lt;td&gt;59&lt;/td&gt; \n&lt;td&gt;8.0&lt;/td&gt; \n&lt;/tr&gt; \n&lt;tr&gt;"
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#html-2",
    "href": "slides/wrangling/15-web-scraping.html#html-2",
    "title": "Web Scraping",
    "section": "HTML",
    "text": "HTML\n\nYou can actually see the data, except data values are surrounded by html code such as &lt;td&gt;.\nWe can also see a pattern of how it is stored.\nIf you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want.\nWe also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS)."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#html-3",
    "href": "slides/wrangling/15-web-scraping.html#html-3",
    "title": "Web Scraping",
    "section": "HTML",
    "text": "HTML\n\nAlthough we provide tools that make it possible to scrape data without knowing HTML, it is useful to learn some HTML and CSS.\nNot only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work.\nThere are plenty of online courses and tutorials for learning these.\nTwo examples are Codeacademy and W3schools."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nThe tidyverse provides a web harvesting package called rvest.\nThe first step using this package is to import the webpage into R.\nThe package makes this quite simple:\n\n\nlibrary(tidyverse) \nlibrary(rvest) \nh &lt;- read_html(url) \n\n\nNote that the entire Murders in the US Wikipedia webpage is now contained in h."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-1",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-1",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nThe class of this object is:\n\n\nclass(h) \n\n[1] \"xml_document\" \"xml_node\"    \n\n\n\nThe rvest package is actually more general; it handles XML documents.\nXML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data.\nHTML is a specific type of XML specifically developed for representing webpages."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-2",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-2",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nNow, how do we extract the table from the object h? If you were to print h, we would see information about the object that is not very informative.\nWe can see all the code that defines the downloaded webpage using the html_text function like this:\n\n\nhtml_text(h)"
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-3",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-3",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nWe don’t show the output here because it includes thousands of characters.\nBut if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=\"wikitable sortable\"&gt;."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-4",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-4",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nThe different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes.\nThe rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-5",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-5",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nTo extract the tables from the html code we use:\n\n\ntab &lt;- h |&gt; html_nodes(\"table\") \n\n\nNow, instead of the entire webpage, we just have the html code for the tables in the page:"
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-6",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-6",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\ntab \n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n\n\nThe table we are interested is the first one:\n\n\ntab[[1]] \n\n{html_node}\n&lt;table class=\"wikitable sortable\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=\"/wiki/List_of_U.S._states ..."
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-7",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-7",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nThis is clearly not a tidy dataset, not even a data frame.\nIn the code above, you can definitely see a pattern and writing code to extract just the data is very doable.\nIn fact, rvest includes a function just for converting HTML tables into data frames:\n\n\ntab &lt;- tab[[1]] |&gt; html_table() \nclass(tab) \n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#the-rvest-package-8",
    "href": "slides/wrangling/15-web-scraping.html#the-rvest-package-8",
    "title": "Web Scraping",
    "section": "The rvest package",
    "text": "The rvest package\n\nWe can now make the data frame:\n\n\ntab &lt;- tab |&gt; \n  setNames(c(\"state\", \"population\", \"total\", \"murder_rate\")) |&gt;\n  mutate(across(c(population, total), parse_number))\nhead(tab) \n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Alabama       4853875   348         7.2\n2 Alaska         737709    59         8  \n3 Arizona       6817565   309         4.5\n4 Arkansas      2977853   181         6.1\n5 California   38993940  1861         4.8\n6 Colorado      5448819   176         3.2"
  },
  {
    "objectID": "slides/wrangling/15-web-scraping.html#css-selectors",
    "href": "slides/wrangling/15-web-scraping.html#css-selectors",
    "title": "Web Scraping",
    "section": "CSS selectors",
    "text": "CSS selectors\n\nhttps://rvest.tidyverse.org/articles/selectorgadget.html]\nhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls",
    "href": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls",
    "title": "Parameters and Estimates",
    "section": "The sampling model for polls",
    "text": "The sampling model for polls\nThe week before the election Real Clear Politics showed this:"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-1",
    "href": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-1",
    "title": "Parameters and Estimates",
    "section": "The sampling model for polls",
    "text": "The sampling model for polls\n\nTo help us understand the connection between polls and what we have learned, let’s construct a situation similar to what pollsters face.\nTo simulate the challenge pollsters encounter in terms of competing with other pollsters for media attention, we will use an urn filled with beads to represent voters, and pretend we are competing for a $25 dollar prize.\nThe challenge is to guess the spread between the proportion of blue and red beads in an urn."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-2",
    "href": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-2",
    "title": "Parameters and Estimates",
    "section": "The sampling model for polls",
    "text": "The sampling model for polls\n\nBefore making a prediction, you can take a sample (with replacement) from the urn.\nTo reflect the fact that running polls is expensive, it costs you $0.10 for each bead you sample.\nTherefore, if your sample size is 250, and you win, you will break even since you would have paid $25 to collect your $25 prize.\nYour entry into the competition can be an interval."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-3",
    "href": "slides/inference/23-parameters-estimates.html#the-sampling-model-for-polls-3",
    "title": "Parameters and Estimates",
    "section": "The sampling model for polls",
    "text": "The sampling model for polls\n\nIf the interval you submit contains the true proportion, you receive half what you paid and proceed to the second phase of the competition.\nIn the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\n\n\nset.seed(1)\nlibrary(tidyverse) \nlibrary(dslabs) \ntake_poll(25)"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates",
    "href": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates",
    "title": "Parameters and Estimates",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\n\nWe want to predict difference proportion of blue beads - proportion of red beads\nLet’s the proportion of blue \\(p\\)\nWe want to estimate \\(p - (1-p)\\) = \\(2p - 1\\)."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-1",
    "href": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-1",
    "title": "Parameters and Estimates",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\n\nIn statistical class, the beads in the urn are called the population.\nThe proportion of blue beads in the population \\(p\\) is called a parameter.\nThe 25 beads we see in the previous plot are called a sample."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-2",
    "href": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-2",
    "title": "Parameters and Estimates",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\n\nThe goal of statistical inference is to predict the parameter \\(p\\) based on the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative.\nFor example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1.\nWe want to construct an estimate of \\(p\\) using only the information we observe."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-3",
    "href": "slides/inference/23-parameters-estimates.html#populations-samples-parameters-and-estimates-3",
    "title": "Parameters and Estimates",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\n\nObserve that in the four random samples shown above, the sample proportions range from 0.44 to 0.60."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sample-average",
    "href": "slides/inference/23-parameters-estimates.html#the-sample-average",
    "title": "Parameters and Estimates",
    "section": "The sample average",
    "text": "The sample average\n\nWe use the symbol \\(\\bar{X}\\) to represent this average.\nIn statistics textbooks, a bar on top of a symbol typically denotes the average.\nThe theory we just covered about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\n\\[\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-sample-average-1",
    "href": "slides/inference/23-parameters-estimates.html#the-sample-average-1",
    "title": "Parameters and Estimates",
    "section": "The sample average",
    "text": "The sample average\n\nWhat do we know about the distribution of the sum?\nWe know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn.\nWe know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nThere is an important difference compared to what we did previously: we don’t know the composition of the urn.\nThis is what we want to find out: we are trying to estimate \\(p\\)."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#parameters",
    "href": "slides/inference/23-parameters-estimates.html#parameters",
    "title": "Parameters and Estimates",
    "section": "Parameters",
    "text": "Parameters\n\nJust as we use variables to define unknowns in systems of equations, in statistical inference, we define parameters to represent unknown parts of our models.\nWe define the parameters \\(p\\) to represent the the proportion of blue beads in the urn.\nSince our main goal is determining \\(p\\), we are going to estimate this parameter.\nThe concepts presented here on how we estimate parameters, and provide insights into how good these estimates are, extend to many data analysis tasks."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#parameters-1",
    "href": "slides/inference/23-parameters-estimates.html#parameters-1",
    "title": "Parameters and Estimates",
    "section": "Parameters",
    "text": "Parameters\n\nFor example, we may want to determine\n\nthe difference in health improvement between patients receiving treatment and a control group,\ninvestigate the health effects of smoking on a population,\nanalyze the differences in racial groups of fatal shootings by police, or\nassess the rate of change in life expectancy in the US during the last 10 years."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#parameters-2",
    "href": "slides/inference/23-parameters-estimates.html#parameters-2",
    "title": "Parameters and Estimates",
    "section": "Parameters",
    "text": "Parameters\n\nAll these questions can be framed as a task of estimating a parameter from a sample.\nThe properties we learned tell us that:\n\n\\[\n\\mbox{E}(\\bar{X}) = p\n\\]\nand\n\\[\n\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate",
    "href": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate",
    "title": "Parameters and Estimates",
    "section": "Properties of our estimate",
    "text": "Properties of our estimate\n\nThe law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win.\nBut how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-1",
    "href": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-1",
    "title": "Parameters and Estimates",
    "section": "Properties of our estimate",
    "text": "Properties of our estimate\n\nFor illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-2",
    "href": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-2",
    "title": "Parameters and Estimates",
    "section": "Properties of our estimate",
    "text": "Properties of our estimate\n\nThe plot shows that we would need a poll of over 10,000 people to achieve a standard error that low.\nWe rarely see polls of this size due in part to the associated costs.\nAccording to the Real Clear Politics table, sample sizes in opinion polls range from 500-3,500 people.\nFor a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\n\nsqrt(p*(1 - p))/sqrt(1000) \n\n[1] 0.01580823"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-3",
    "href": "slides/inference/23-parameters-estimates.html#properties-of-our-estimate-3",
    "title": "Parameters and Estimates",
    "section": "Properties of our estimate",
    "text": "Properties of our estimate\n\nThe CLT tells us that the distribution function for a sum of draws is approximately normal.\nUsing the properties we learned: \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\)."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nNow how can answer questions like what is the probability that we are within 1% from \\(p\\).\nWe are basically asking what is:\n\n\\[\n\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\n\\]\nwhich is the same as:\n\\[\n\\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01)\n\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-1",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-1",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nSince \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error, we get:\n\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\n\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)  \n\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-2",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-2",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\).\nHowever, it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\).\nWe say that we plug-in the estimate.\nOur estimate of the standard error is therefore:\n\n\\[\n\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\n\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-3",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-3",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nNow we continue with our calculation, but dividing by\n\n\\[\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-4",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-4",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nIn our first sample, we had 12 blue and 13 red, so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\n\n\nx_hat &lt;- 0.48 \nse &lt;- sqrt(x_hat*(1-x_hat)/25) \nse \n\n[1] 0.09991997"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-5",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-5",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nNow, we can answer the question of the probability of being close to \\(p\\).\nThe answer is:\n\n\npnorm(0.01/se) - pnorm(-0.01/se) \n\n[1] 0.07971926\n\n\n\nTherefore, there is a small chance that we will be close."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-6",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-6",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nEarlier, we mentioned the margin of error.\nNow, we can define it simply as two times the standard error, which we can now estimate.\n\n\n1.96*se \n\n[1] 0.1958431"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-7",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-7",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nWhy do we multiply by 1.96?\nBecause if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\n\\[\n\\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\mbox{SE}(\\bar{X})  / \\mbox{SE}(\\bar{X}) \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\, \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right)  \n\\]\nwhich is:\n\\[\n\\mbox{Pr}\\left(Z \\leq 1.96 \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\right)  = 0.95\n\\]"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-8",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-8",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nToo see this:\n\npnorm(1.96) - pnorm(-1.96) \n\n[1] 0.9500042"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#central-limit-theorem-9",
    "href": "slides/inference/23-parameters-estimates.html#central-limit-theorem-9",
    "title": "Parameters and Estimates",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThere is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\).\nObserve that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nB &lt;- 10000 \nN &lt;- 1000 \nx_hat &lt;- replicate(B, { \n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p)) \n  mean(x) \n}) \n\n\nThe problem is, of course, that we don’t know p."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-1",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-1",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nLet’s set p=0.45.\nWe can then simulate a poll:\n\n\np &lt;- 0.45 \nN &lt;- 1000 \nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p)) \nx_hat &lt;- mean(x) \n\n\nIn this particular sample, our estimate is x_hat.\nWe can use that code to do a Monte Carlo simulation:"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-2",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-2",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nB &lt;- 10000 \nx_hat &lt;- replicate(B, { \n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p)) \n  mean(x) \n})"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-3",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-3",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45, and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321.\nThe simulation confirms this:\n\n\nmean(x_hat) \n\n[1] 0.4499836\n\nsd(x_hat) \n\n[1] 0.01556674"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-4",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-4",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nA histogram and qqplot confirm that the normal approximation is also accurate:"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-5",
    "href": "slides/inference/23-parameters-estimates.html#a-monte-carlo-simulation-5",
    "title": "Parameters and Estimates",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n\nOf course, in real life, we would never be able to run such an experiment because we don’t know \\(p\\).\nHowever, we can run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values.\nYou can easily do this by rerunning the code above after changing the values of p and N."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#the-spread",
    "href": "slides/inference/23-parameters-estimates.html#the-spread",
    "title": "Parameters and Estimates",
    "section": "The spread",
    "text": "The spread\n\nWe did all this theory for the estimate of \\(p\\). How do we estimate the spread \\(2p - 1\\)?"
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#why-not-run-a-very-large-poll",
    "href": "slides/inference/23-parameters-estimates.html#why-not-run-a-very-large-poll",
    "title": "Parameters and Estimates",
    "section": "Why not run a very large poll?",
    "text": "Why not run a very large poll?\n\nFor realistic values of \\(p\\), let’s say ranging from 0.35 to 0.65, if we conduct a very large poll with 100,000 people, theory tells us that we would predict the election perfectly, as the largest possible margin of error is around 0.3%."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#why-not-run-a-very-large-poll-1",
    "href": "slides/inference/23-parameters-estimates.html#why-not-run-a-very-large-poll-1",
    "title": "Parameters and Estimates",
    "section": "Why not run a very large poll?",
    "text": "Why not run a very large poll?\n\nOne reason is that conducting such a poll is very expensive.\nAnother, and possibly more important reason, is that theory has its limitations.\nPolling is much more complicated than simply picking beads from an urn.\nSome people might lie to pollsters, and others might not have phones."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#bias-why-not-run-a-very-large-poll",
    "href": "slides/inference/23-parameters-estimates.html#bias-why-not-run-a-very-large-poll",
    "title": "Parameters and Estimates",
    "section": "Bias: Why not run a very large poll?",
    "text": "Bias: Why not run a very large poll?\n\nHowever, perhaps the most important way an actual poll differs from an urn model is that we don’t actually know for sure who is in our population and who is not.\nHow do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\).\nWe call this bias."
  },
  {
    "objectID": "slides/inference/23-parameters-estimates.html#bias-why-not-run-a-very-large-poll-1",
    "href": "slides/inference/23-parameters-estimates.html#bias-why-not-run-a-very-large-poll-1",
    "title": "Parameters and Estimates",
    "section": "Bias: Why not run a very large poll?",
    "text": "Bias: Why not run a very large poll?\n\nHistorically, we observe that polls are indeed biased, although not by a substantial amount.\nThe typical bias appears to be about 2-3%."
  },
  {
    "objectID": "slides/inference/25-models.html#data-driven-models",
    "href": "slides/inference/25-models.html#data-driven-models",
    "title": "Data-driven models",
    "section": "Data-driven models",
    "text": "Data-driven models\n\n\n“All models are wrong, but some are useful.” –George E.P. Box."
  },
  {
    "objectID": "slides/inference/25-models.html#data-driven-models-1",
    "href": "slides/inference/25-models.html#data-driven-models-1",
    "title": "Data-driven models",
    "section": "Data-driven models",
    "text": "Data-driven models\n\nOur analysis of poll-related results has been based on a simple sampling model.\nThis model assumes that each voter has an equal chance of being selected for the poll, similar to picking beads from an urn with two colors.\nHowever, in this section, we explore real-world data and discover that this model is incorrect."
  },
  {
    "objectID": "slides/inference/25-models.html#data-driven-models-2",
    "href": "slides/inference/25-models.html#data-driven-models-2",
    "title": "Data-driven models",
    "section": "Data-driven models",
    "text": "Data-driven models\n\nHowever, these introductions are just scratching the surface, and readers interested in statistical modeling should supplement the material presented in this book with additional references."
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nWe generate results for 12 polls taken the week before the election.\nWe mimic sample sizes from actual polls, construct, and report 95% confidence intervals for each of the 12 polls.\nWe save the results from this simulation in a data frame and add a poll ID column."
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-1",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-1",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nlibrary(tidyverse) \nlibrary(dslabs) \nmu &lt;- 0.039 \nNs &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516) \np &lt;- (mu + 1) / 2 \npolls &lt;- map_df(Ns, function(N) { \n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p)) \n  x_hat &lt;- mean(x) \n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N) \n  list(estimate = 2 * x_hat - 1,  \n    low = 2*(x_hat - 1.96*se_hat) - 1,  \n    high = 2*(x_hat + 1.96*se_hat) - 1, \n    sample_size = N) \n}) |&gt; \n  mutate(poll = seq_along(Ns))"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-2",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-2",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-3",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-3",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nPoll aggregators combine polls to improve power.\nBy doing this, we are effectively conducting a poll with a huge sample size.\nAlthough, as aggregators, we do not have access to the raw poll data.\nWe can use math to reconstruct what we would have obtained with one large poll with sample size:\n\n\nsum(polls$sample_size) \n\n[1] 11269"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-4",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-4",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nBasically, we construct an estimate of the spread, let’s call it \\(\\mu\\), with a weighted average in the following way:\n\n\nmu_hat &lt;- polls |&gt;  \n  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) |&gt;  \n  pull(avg) \nprint(mu_hat)\n\n[1] 0.0311474"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-5",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-5",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nWe can reconstruct the margin of error as well:\n\n\np_hat &lt;- (1 + mu_hat)/2\nmoe &lt;- 2*1.96*sqrt(p_hat*(1 - p_hat)/sum(polls$sample_size))\nprint(moe)\n\n[1] 0.01845451\n\n\n\nWhich is much smaller than any of the individual polls."
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-6",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-6",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\nThe aggregator estimate is in red:"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-7",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-7",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nHowever, this was just a simulation to illustrate the idea.\nLet’s look at real data from the 2016 presidential election.\n\n\nlibrary(dslabs) \npolls &lt;- polls_us_election_2016 |&gt;  \n  filter(state == \"U.S.\" & population == \"lv\" & \n           enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-8",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-8",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nWe add a spread estimate:\n\n\npolls &lt;- polls |&gt;  \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-9",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-9",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nWe are interested in the spread \\(2p-1\\).\nLet’s call the spread \\(\\mu\\) (for difference).\nWe have 49 estimates of the spread.\nThe theory we learned from sampling models tells us that these estimates are a random variable with a probability distribution that is approximately normal.\nThe expected value is the election night spread \\(\\mu\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\)."
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-10",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-10",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nWe construct a confidence interval assuming the data is from an urn model:\n\n\nmu_hat &lt;- polls |&gt;  \n  summarize(mu_hat = sum(spread*samplesize)/sum(samplesize)) |&gt;  \n  pull(mu_hat) \n\n\nand the standard error is:\n\n\np_hat &lt;- (mu_hat + 1)/2  \nmoe &lt;- 1.96*2*sqrt(p_hat*(1 - p_hat)/sum(polls$samplesize)) \nmoe \n\n[1] 0.006623178"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-11",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-11",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nSo we report a spread of 1.43% with a margin of error of 0.66%.\nOn election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval.\nWhat happened?"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-12",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-12",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\nA histogram of the reported spreads reveals a problem:"
  },
  {
    "objectID": "slides/inference/25-models.html#case-study-poll-aggregators-13",
    "href": "slides/inference/25-models.html#case-study-poll-aggregators-13",
    "title": "Data-driven models",
    "section": "Case study: poll aggregators",
    "text": "Case study: poll aggregators\n\nThe data does not appear to be normally distributed\nThe standard error appears to be larger than 0.0066232.\nThe theory is not working her\nThis motivates the use of a data-driven model."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nData come from various pollsters, some taking several polls:\n\n\npolls |&gt; count(pollster) |&gt; arrange(desc(n))\n\n                                                     pollster n\n1                                                    IBD/TIPP 8\n2                                    The Times-Picayune/Lucid 8\n3                                       USC Dornsife/LA Times 8\n4                                    ABC News/Washington Post 7\n5                                                       Ipsos 6\n6                                     CBS News/New York Times 2\n7  Fox News/Anderson Robbins Research/Shaw & Company Research 2\n8                                           Angus Reid Global 1\n9                                               Insights West 1\n10                                             Marist College 1\n11                                        Monmouth University 1\n12                                            Morning Consult 1\n13                               NBC News/Wall Street Journal 1\n14                      RKM Research and Communications, Inc. 1\n15                                           Selzer & Company 1"
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-1",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-1",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nLet’s visualize the data for the pollsters that are regularly polling:"
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-2",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-2",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nThis plot reveals an unexpected result.\nThe standard error predicted by theory is between 0.018 and 0.033:\n\n\npolls |&gt; group_by(pollster) |&gt;  \n  filter(n() &gt;= 6) |&gt; \n  summarize(se = 2*sqrt(p_hat*(1 - p_hat)/median(samplesize))) \n\n# A tibble: 5 × 2\n  pollster                     se\n  &lt;fct&gt;                     &lt;dbl&gt;\n1 ABC News/Washington Post 0.0265\n2 IBD/TIPP                 0.0333\n3 Ipsos                    0.0225\n4 The Times-Picayune/Lucid 0.0196\n5 USC Dornsife/LA Times    0.0183\n\n\n\nThis agrees with the within poll variation we see."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-3",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-3",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nHowever, there appears to be differences across the polls:"
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-4",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-4",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nSompling theory says nothing about different pollsters having expected values.\nFiveThirtyEight refers to these differences as house effects.\nWe also call them pollster bias.\nRather than modeling the process generating these values with an urn model, we instead model the pollster results directly."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-5",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-5",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nTo do this, we start by collecting some data.\nSpecifically, for each pollster, we look at the last reported result before the election:\n\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt;  \n  filter(enddate == max(enddate)) |&gt; \n  ungroup()"
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-6",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-6",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nHere is a histogram of the data for these 15 pollsters:\n\n\nhist(one_poll_per_pollster$spread, 10)"
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-7",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-7",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nAlthough we are no longer using a model with red (Republicans) and blue (Democrats) beads in an urn, our new model can also be thought of as an urn model, but containing poll results from all possible pollsters.\nThink of our $N=$15 data points \\(X_1,\\dots X_N\\) as a random sample from this urn.\nTo develop a useful model, we assume that the expected value of our urn is the actual spread \\(\\mu=2p-1\\), which implies that the sample average has expected value \\(\\mu\\)."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-8",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-8",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nNow, because instead of 0s and 1s, our urn contains continuous numbers, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\).\nRather than voter sampling variability, the standard error now includes the pollster-to-pollster variability.\nOur new urn also includes the sampling variability from the polling.\nRegardless, this standard deviation is now an unknown parameter."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-9",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-9",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nIn statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nSo our new statistical model is that \\(X_1, \\dots, X_N\\) are a random sample with expected \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe distribution, for now, is unspecified."
  },
  {
    "objectID": "slides/inference/25-models.html#beyond-the-sampling-model-10",
    "href": "slides/inference/25-models.html#beyond-the-sampling-model-10",
    "title": "Data-driven models",
    "section": "Beyond the sampling model",
    "text": "Beyond the sampling model\n\nAssume \\(N\\) is large enough to assume the sample average \\(\\bar{X} = \\sum_{i=1}^N X_i\\) follows a normal distribution with expected value \\(\\mu\\) and standard error \\(\\sigma / \\sqrt{N}\\).\n\n\\[\n\\bar{X} \\sim \\mbox{N}(\\mu, \\sigma / \\sqrt{N})\n\\]\n\nHere the \\(\\sim\\) symbol tells us that the random variable on the left follows the distribution on the right.\nWe use the notation \\(N(a,b)\\) to represent the normal distribution with mean \\(a\\) and standard deviation \\(b\\)."
  },
  {
    "objectID": "slides/inference/25-models.html#estimating-the-sd",
    "href": "slides/inference/25-models.html#estimating-the-sd",
    "title": "Data-driven models",
    "section": "Estimating the SD",
    "text": "Estimating the SD\n\nOur new model has two unknown parameters: the expected value \\(\\mu\\) and the standard deviation \\(\\sigma\\).\nWe know that the sample average \\(\\bar{X}\\) will be our estimate of \\(\\mu\\).\nBut what about \\(\\sigma\\)?"
  },
  {
    "objectID": "slides/inference/25-models.html#estimating-the-sd-1",
    "href": "slides/inference/25-models.html#estimating-the-sd-1",
    "title": "Data-driven models",
    "section": "Estimating the SD",
    "text": "Estimating the SD\n\nTheory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as:\n\n\\[\ns = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2 }\n\\]\n\nUnlike for the population standard deviation definition, we now divide by \\(N-1\\).\nThis makes \\(s\\) a better estimate of \\(\\sigma\\).\nThere is a mathematical explanation for this, which is explained in most statistics textbooks."
  },
  {
    "objectID": "slides/inference/25-models.html#estimating-the-sd-2",
    "href": "slides/inference/25-models.html#estimating-the-sd-2",
    "title": "Data-driven models",
    "section": "Estimating the SD",
    "text": "Estimating the SD\n\nThe sd function in R computes the sample standard deviation:\n\n\nsd(one_poll_per_pollster$spread) \n\n[1] 0.02419369"
  },
  {
    "objectID": "slides/inference/25-models.html#computing-a-confidence-interval",
    "href": "slides/inference/25-models.html#computing-a-confidence-interval",
    "title": "Data-driven models",
    "section": "Computing a confidence interval",
    "text": "Computing a confidence interval\n\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\n\nresults &lt;- one_poll_per_pollster |&gt;  \n  summarize(avg = mean(spread),  \n            se = sd(spread)/sqrt(length(spread))) |&gt;  \n  mutate(start = avg - 1.96*se,  \n         end = avg + 1.96*se)  \nround(results*100, 1) \n\n  avg  se start end\n1 2.9 0.6   1.7 4.1\n\n\n\nOur confidence interval is wider now since it incorporates the pollster variability.\nIt does include the election night result of 2.1%.\nAlso, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution",
    "href": "slides/inference/25-models.html#the-t-distribution",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nAbove, we made use of the CLT with a sample size of 15.\nBecause we are estimating a second parameters \\(\\sigma\\), further variability is introduced into our confidence interval, which results in intervals that are too small."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-1",
    "href": "slides/inference/25-models.html#the-t-distribution-1",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nFor very large sample sizes, this extra variability is negligible, but in general, for values smaller than 30, we need to be cautious about using the CLT.\nHowever, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of \\(\\sigma\\).\nApplying this theory, we can construct confidence intervals for any \\(N\\).\nBut again, this works only if the data in the urn is known to follow a normal distribution.\nSo for the 0, 1 data of our previous urn model, this theory definitely does not apply."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-2",
    "href": "slides/inference/25-models.html#the-t-distribution-2",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nThe statistic on which confidence intervals for \\(\\mu\\) are based is:\n\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{N}}\n\\]\n\nCLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-3",
    "href": "slides/inference/25-models.html#the-t-distribution-3",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nBut in practice we don’t know \\(\\sigma\\), so we use:\n\n\\[\nt = \\frac{\\bar{X} - \\mu}{s/\\sqrt{N}}\n\\]\n\nThis is referred to as a t-statistic."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-4",
    "href": "slides/inference/25-models.html#the-t-distribution-4",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nBy substituting \\(\\sigma\\) with \\(s\\), we introduce some variability.\nTheory tells us that \\(t\\) follows a student t-distribution with \\(N-1\\) degrees of freedom (df).\nThe df is a parameter that controls the variability via fatter tails:"
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-5",
    "href": "slides/inference/25-models.html#the-t-distribution-5",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nIf we are willing to assume the pollster effect data is normally distributed, based on the sample data \\(X_1, \\dots, X_N\\),\n\n\none_poll_per_pollster |&gt; \n  ggplot(aes(sample = spread)) + stat_qq()"
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-6",
    "href": "slides/inference/25-models.html#the-t-distribution-6",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\n\nthen \\(t\\) follows a t-distribution with \\(N-1\\) degrees of freedom."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-7",
    "href": "slides/inference/25-models.html#the-t-distribution-7",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nSo perhaps a better confidence interval for \\(\\mu\\) is:\n\n\nz &lt;- qt(0.975,  nrow(one_poll_per_pollster) - 1) \none_poll_per_pollster |&gt;  \n  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) |&gt;  \n  mutate(start = avg - moe, end = avg + moe)  \n\n# A tibble: 1 × 4\n     avg    moe  start    end\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0290 0.0134 0.0156 0.0424\n\n\n\nA bit larger than the one using normal is:\n\n\nqt(0.975, 14) \n\n[1] 2.144787\n\nqnorm(0.975) \n\n[1] 1.959964"
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-8",
    "href": "slides/inference/25-models.html#the-t-distribution-8",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nThis results in a slightly larger confidence interval than we obtained before:\n\n\n\n  start end\n1   1.6 4.2"
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-9",
    "href": "slides/inference/25-models.html#the-t-distribution-9",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nNote that using the t-distribution and the t-statistic is the basis for t-tests, a widely used approach for computing p-values.\nTo learn more about t-tests, you can consult any statistics textbook.\nThe t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously observed.\nFiveThirtyEight uses the t-distribution to generate errors that better model the deviations we see in election data."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-10",
    "href": "slides/inference/25-models.html#the-t-distribution-10",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nFor example, in Wisconsin, the average of six polls was 7% in favor of Clinton with a standard deviation of 1%, but Trump won by 0.7%.\nEven after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution."
  },
  {
    "objectID": "slides/inference/25-models.html#the-t-distribution-11",
    "href": "slides/inference/25-models.html#the-t-distribution-11",
    "title": "Data-driven models",
    "section": "The t-distribution",
    "text": "The t-distribution\n\npolls_us_election_2016 |&gt; \n  filter(state == \"Wisconsin\" & \n           enddate &gt;= \"2016-10-31\" &  \n           (grade %in% c(\"A+\", \"A\", \"A-\", \"B+\") | is.na(grade))) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt; \n  mutate(state = as.character(state)) |&gt; \n  left_join(results_us_election_2016, by = \"state\") |&gt; \n  mutate(actual = clinton/100 - trump/100) |&gt; \n  summarize(actual = first(actual), avg = mean(spread),  \n            sd = sd(spread), n = n()) |&gt; \n  select(actual, avg, sd, n) \n\n  actual        avg         sd n\n1 -0.007 0.07106667 0.01041454 6"
  },
  {
    "objectID": "slides/productivity/02-rstudio.html#the-panes",
    "href": "slides/productivity/02-rstudio.html#the-panes",
    "title": "RStudio",
    "section": "The panes",
    "text": "The panes"
  },
  {
    "objectID": "slides/productivity/02-rstudio.html#the-basics",
    "href": "slides/productivity/02-rstudio.html#the-basics",
    "title": "RStudio",
    "section": "The Basics",
    "text": "The Basics\nLet’s try a few things together:\n\nOpen a new R script file.\nLearn tab complete.\nRun commands while editing scripts.\nRun the entire script.\nMake a plot.\nChange options to never save workspace.\nChange IDE appearance."
  },
  {
    "objectID": "slides/productivity/02-rstudio.html#projects",
    "href": "slides/productivity/02-rstudio.html#projects",
    "title": "RStudio",
    "section": "Projects",
    "text": "Projects\nLet’s try this together:\n\nStart new project in new directory.\nStart new project in existing directory.\nChange projects."
  },
  {
    "objectID": "slides/productivity/02-rstudio.html#type-of-editor",
    "href": "slides/productivity/02-rstudio.html#type-of-editor",
    "title": "RStudio",
    "section": "Type of editor",
    "text": "Type of editor\nLet’s examine the two types of editors available:\n\nSource - See the actual code (WYSIWYG)\nVisual - Partial preview of final document.\n\nNote: You can state your preference in a the header:\neditor: source\n\nWe learn about headers in the Quarto lecture."
  },
  {
    "objectID": "slides/productivity/04-git.html#goal-for-the-day",
    "href": "slides/productivity/04-git.html#goal-for-the-day",
    "title": "Git and GitHub",
    "section": "Goal for the day",
    "text": "Goal for the day\n\nCreate a repository\npush something to the repository\nconnect RStudio to GitHub"
  },
  {
    "objectID": "slides/productivity/04-git.html#do-you-have-git",
    "href": "slides/productivity/04-git.html#do-you-have-git",
    "title": "Git and GitHub",
    "section": "Do you have git?",
    "text": "Do you have git?\nBefore we start:\n\nMake sure you have Git installed.\nOpen a terminal and type:\n\n\ngit --version\n\nIf not installed\n\non a Mac, follow the instructions after typing the above.\non Windows follow these instructions"
  },
  {
    "objectID": "slides/productivity/04-git.html#motivation",
    "href": "slides/productivity/04-git.html#motivation",
    "title": "Git and GitHub",
    "section": "Motivation",
    "text": "Motivation\nWe want to avoid this:\n\nPosted by rjkb041 on r/ProgrammerHumor"
  },
  {
    "objectID": "slides/productivity/04-git.html#motivation-1",
    "href": "slides/productivity/04-git.html#motivation-1",
    "title": "Git and GitHub",
    "section": "Motivation",
    "text": "Motivation\n\nThis is particularly true when more than one person is collaborating and editing the file.\nEven more important when there are multiple files, as there is in software development, and to some extend data analysis."
  },
  {
    "objectID": "slides/productivity/04-git.html#motivation-2",
    "href": "slides/productivity/04-git.html#motivation-2",
    "title": "Git and GitHub",
    "section": "Motivation",
    "text": "Motivation\n\nGit is a version control system that provides a systematic approach to keeping versions of files.\n\n\nPosted on devrant.com/ by bhimanshukalra"
  },
  {
    "objectID": "slides/productivity/04-git.html#motivation-3",
    "href": "slides/productivity/04-git.html#motivation-3",
    "title": "Git and GitHub",
    "section": "Motivation",
    "text": "Motivation\nBut we have to learn some things.\n\nFrom Meme Git Compilation by Lulu Ilmaknun Qurotaini"
  },
  {
    "objectID": "slides/productivity/04-git.html#why-use-git-and-github",
    "href": "slides/productivity/04-git.html#why-use-git-and-github",
    "title": "Git and GitHub",
    "section": "Why use Git and GitHub?",
    "text": "Why use Git and GitHub?\n\nSharing.\nCollaborating.\nVersion control.\n\nWe focus on the sharing aspects of Git and GitHub, but introduce some of the basics that permit you to collaborate and use version control."
  },
  {
    "objectID": "slides/productivity/04-git.html#what-is-git",
    "href": "slides/productivity/04-git.html#what-is-git",
    "title": "Git and GitHub",
    "section": "What is Git?",
    "text": "What is Git?"
  },
  {
    "objectID": "slides/productivity/04-git.html#what-is-github",
    "href": "slides/productivity/04-git.html#what-is-github",
    "title": "Git and GitHub",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\nDescribed a social network for software developers.\nBasically, it’s a service that hosts the remote repository (repo) on the web.\nThis facilitates collaboration and sharing greatly."
  },
  {
    "objectID": "slides/productivity/04-git.html#what-is-github-1",
    "href": "slides/productivity/04-git.html#what-is-github-1",
    "title": "Git and GitHub",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nThere many other features such as\n\nRecognition system: reward, badges and stars.\nYou can host web pages, like the class notes for example.\nPermits contributions via forks and pull requests.\nIssue tracking\nAutomation tools."
  },
  {
    "objectID": "slides/productivity/04-git.html#what-is-github-2",
    "href": "slides/productivity/04-git.html#what-is-github-2",
    "title": "Git and GitHub",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\nThe main tool behind GitHub is Git.\nSimilar to how the main tool behind RStudio is R."
  },
  {
    "objectID": "slides/productivity/04-git.html#github-accounts",
    "href": "slides/productivity/04-git.html#github-accounts",
    "title": "Git and GitHub",
    "section": "GitHub accounts",
    "text": "GitHub accounts\n\nPick a professional sounding name.\nConsider adding a profile README.md.\nInstructions are here.\nExample here."
  },
  {
    "objectID": "slides/productivity/04-git.html#repositories",
    "href": "slides/productivity/04-git.html#repositories",
    "title": "Git and GitHub",
    "section": "Repositories",
    "text": "Repositories\n\nWe are ready to create a GitHub repository (repo).\nYou will have at least two copies of your code: one on your computer and one on GitHub.\nIf you add collaborators to this repo, then each will have a copy on their computer.\nThe GitHub copy is considered the main (previously called master) copy that everybody syncs to.\nGit will help you keep all the different copies synced."
  },
  {
    "objectID": "slides/productivity/04-git.html#repositories-1",
    "href": "slides/productivity/04-git.html#repositories-1",
    "title": "Git and GitHub",
    "section": "Repositories",
    "text": "Repositories\n\nLet’s go make one on GitHub…\nThen create a directory on your computer, this will be the local repo, and connect it to the Github repository.\nFirst copy and paste the location of your git repository. It should look something like this:\n\nhttps://github.com/your-username/your-repo-name.git"
  },
  {
    "objectID": "slides/productivity/04-git.html#connect-git-and-github",
    "href": "slides/productivity/04-git.html#connect-git-and-github",
    "title": "Git and GitHub",
    "section": "Connect Git and GitHub",
    "text": "Connect Git and GitHub\n\nWhen accessing GitHub you need credentials to verify your identity.\nThere are two ways to connect: HTTPS or SSH, each requiring different credentials.\nWe recommend using HTTPS, which uses a Personal Access Token (PAT).\nNote that your GitHub website password isn’t your access token."
  },
  {
    "objectID": "slides/productivity/04-git.html#connect-git-and-github-1",
    "href": "slides/productivity/04-git.html#connect-git-and-github-1",
    "title": "Git and GitHub",
    "section": "Connect Git and GitHub",
    "text": "Connect Git and GitHub\n\nDetailed instructions are here.\nCarefully follow the instructions provided by GitHub.\nWhen setting permissions for the token, choose non-expiring and select the repo option in the scopes section.\nOnce you complete these steps, GitHub will display your token—a lengthy string of characters.\nImmediately copy this token to your clipboard. This is the only time GitHub will show it to you."
  },
  {
    "objectID": "slides/productivity/04-git.html#generate-a-token",
    "href": "slides/productivity/04-git.html#generate-a-token",
    "title": "Git and GitHub",
    "section": "Generate a token:",
    "text": "Generate a token:\n\nFor security, save this token in a password manager. This ensures you can access it if needed later on.\nWhen git prompts you to enter your password, paste the token you’ve copied. After this, password prompts should no longer appear.\nIf you ever need the token again, retrieve it from your password manager.\n\nMore details available from Happy Git and GitHub for the use."
  },
  {
    "objectID": "slides/productivity/04-git.html#connect-git-and-github-2",
    "href": "slides/productivity/04-git.html#connect-git-and-github-2",
    "title": "Git and GitHub",
    "section": "Connect Git and GitHub",
    "text": "Connect Git and GitHub\n\nThe next step is to let Git know who we are on Github.\nTo to this type the following two commands in our terminal window:\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.mail \"your@email.com\""
  },
  {
    "objectID": "slides/productivity/04-git.html#connect-git-and-github-3",
    "href": "slides/productivity/04-git.html#connect-git-and-github-3",
    "title": "Git and GitHub",
    "section": "Connect Git and GitHub",
    "text": "Connect Git and GitHub\n\nThis will change the Git configuration in way that anytime you use Git, it will know this information.\nNote that you need to use the email account that you used to open your GitHub account."
  },
  {
    "objectID": "slides/productivity/04-git.html#connect-git-and-github-4",
    "href": "slides/productivity/04-git.html#connect-git-and-github-4",
    "title": "Git and GitHub",
    "section": "Connect Git and GitHub",
    "text": "Connect Git and GitHub\nTo connect working directory to the GitHub repo\n\ninitialize the directory:\n\n\ngit init\n\n\nLet Git know what is the remote repository.\n\n\ngit remote add origin &lt;remote-url&gt;\n\nNow the two are connected.\n\n\n\n\n\n\nNote\n\n\norigin is a nickname we will use for the remote. We can call it something else, but everybody calls it origin so best to stick with that."
  },
  {
    "objectID": "slides/productivity/04-git.html#overview-of-git",
    "href": "slides/productivity/04-git.html#overview-of-git",
    "title": "Git and GitHub",
    "section": "Overview of Git",
    "text": "Overview of Git\nThe main actions in Git are to:\n\npull changes from the remote repo.\nadd files, or as we say in the Git lingo stage files.\ncommit changes to the local repo.\npush changes to the remote repo."
  },
  {
    "objectID": "slides/productivity/04-git.html#the-four-areas-of-git",
    "href": "slides/productivity/04-git.html#the-four-areas-of-git",
    "title": "Git and GitHub",
    "section": "The four areas of Git",
    "text": "The four areas of Git"
  },
  {
    "objectID": "slides/productivity/04-git.html#status",
    "href": "slides/productivity/04-git.html#status",
    "title": "Git and GitHub",
    "section": "Status",
    "text": "Status\n\n\ngit status filename"
  },
  {
    "objectID": "slides/productivity/04-git.html#add",
    "href": "slides/productivity/04-git.html#add",
    "title": "Git and GitHub",
    "section": "Add",
    "text": "Add\nUse git add to put file to staging area.\n\n\ngit add &lt;filename&gt;\n\nWe say that this file has been staged. Check to see what happened:\n\ngit status &lt;filename&gt;"
  },
  {
    "objectID": "slides/productivity/04-git.html#commit",
    "href": "slides/productivity/04-git.html#commit",
    "title": "Git and GitHub",
    "section": "Commit",
    "text": "Commit\n\nTo move all the staged files to the local repository we use git commit.\n\n\n\ngit commit -m \"must add comment\"\n\n\nOnce committed the files are tracked and a copy of this version is kept going forward.\nThis is like adding V1 to your filename."
  },
  {
    "objectID": "slides/productivity/04-git.html#commit-1",
    "href": "slides/productivity/04-git.html#commit-1",
    "title": "Git and GitHub",
    "section": "Commit",
    "text": "Commit\n\n\n\n\n\n\nNote\n\n\nYou can commit files directly without using add by explicitely writing the files at the end of the commit:\n\n\n\n\ngit commit -m \"must add comment\" &lt;filename&gt;"
  },
  {
    "objectID": "slides/productivity/04-git.html#push",
    "href": "slides/productivity/04-git.html#push",
    "title": "Git and GitHub",
    "section": "Push",
    "text": "Push\n\nTo move to upstream repo we use git push\n\n\n\ngit push -u origin main\n\n\nThe -u flag sets the upstream repo.\nBy using this flag, going forward you can simply use git push to push changes.\nSo going forward we can just type:\n\n\ngit push"
  },
  {
    "objectID": "slides/productivity/04-git.html#push-1",
    "href": "slides/productivity/04-git.html#push-1",
    "title": "Git and GitHub",
    "section": "Push",
    "text": "Push\n\nWhen using git push we need to be careful as if collaborating this will affect the work of others.\nIt might also create a conflict.\n\n\nPosted by andortang on Nothing is Impossible!"
  },
  {
    "objectID": "slides/productivity/04-git.html#fetch",
    "href": "slides/productivity/04-git.html#fetch",
    "title": "Git and GitHub",
    "section": "Fetch",
    "text": "Fetch\n\nTo update our local repository to the remote one we use\n\n\ngit fetch"
  },
  {
    "objectID": "slides/productivity/04-git.html#merge",
    "href": "slides/productivity/04-git.html#merge",
    "title": "Git and GitHub",
    "section": "Merge",
    "text": "Merge\n\nOnce we are sure this is good, we can merge with our local files:\n\n\ngit merge"
  },
  {
    "objectID": "slides/productivity/04-git.html#pull",
    "href": "slides/productivity/04-git.html#pull",
    "title": "Git and GitHub",
    "section": "Pull",
    "text": "Pull\n\nI rarely use fetch and merge and instead use pull which does both of these in one step\n\n\ngit pull"
  },
  {
    "objectID": "slides/productivity/04-git.html#checkout",
    "href": "slides/productivity/04-git.html#checkout",
    "title": "Git and GitHub",
    "section": "Checkout",
    "text": "Checkout\n\nIf you want to pull down a specific file you from the remote repo you can use:\n\n\ngit checkout filename\n\n\nI use this when I make changes but decide I want to go back to original version on remote repo.\n\n\n\n\n\n\n\nWarning\n\n\nIf you have a newer version in your local repository this will create a conflict. It won’t let you do it. If you are sure you want to get rid of your local copy you can remove it and then use checkout."
  },
  {
    "objectID": "slides/productivity/04-git.html#checkout-1",
    "href": "slides/productivity/04-git.html#checkout-1",
    "title": "Git and GitHub",
    "section": "Checkout",
    "text": "Checkout\n\nYou can also use checkout to obtain older version:\n\n\ngit checkout &lt;commit-id&gt; &lt;filename&gt;\n\n\nYou can get the commit-id either on the GitHub webpage or using\n\n\ngit log filename"
  },
  {
    "objectID": "slides/productivity/04-git.html#reset",
    "href": "slides/productivity/04-git.html#reset",
    "title": "Git and GitHub",
    "section": "Reset",
    "text": "Reset\n\nWhat if I commit and realize it was a mistake?\n\n\ngit reset HEAD~1\n\nundos the commit and unstages the files, but keeps your local copies. I use this on very often.\n\nThere are many wasy of using get reset and it covers most scenarios.\nChatGPT and stackoverflow are great resources to learn more."
  },
  {
    "objectID": "slides/productivity/04-git.html#branches",
    "href": "slides/productivity/04-git.html#branches",
    "title": "Git and GitHub",
    "section": "Branches",
    "text": "Branches\n\nWe are just sratching the surface of Git.\nOne advanced feature to be aware of is that you can have several branches, useful for working in parallel or testing stuff out that might not make the main repo.\n\n\nArt by: Allison Horst"
  },
  {
    "objectID": "slides/productivity/04-git.html#branches-1",
    "href": "slides/productivity/04-git.html#branches-1",
    "title": "Git and GitHub",
    "section": "Branches",
    "text": "Branches\n\nWe wont go over this, but we might need to use these two related commands:\n\n\ngit remote -v\ngit brach"
  },
  {
    "objectID": "slides/productivity/04-git.html#clone",
    "href": "slides/productivity/04-git.html#clone",
    "title": "Git and GitHub",
    "section": "Clone",
    "text": "Clone\n\nAnother common command is git clone.\nIt let’s download an entire repo, including version history.\n\n\ngit clone &lt;repo-url&gt;"
  },
  {
    "objectID": "slides/productivity/04-git.html#using-git-in-rstudio",
    "href": "slides/productivity/04-git.html#using-git-in-rstudio",
    "title": "Git and GitHub",
    "section": "Using Git in RStudio",
    "text": "Using Git in RStudio\n\nGo to file, new project, version control, and follow the instructions.\nThen notice the Git tab in the preferences."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics",
    "href": "slides/inference/26-bayes.html#bayesian-statistics",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nIn 2016 FiveThirtyEight showed this diagram:\n\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics-1",
    "href": "slides/inference/26-bayes.html#bayesian-statistics-1",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nWhat does this mean in the context of the theory we have discussed?\nThe statement “Obama has a 90% chance of winning the election” is equivalent to “the probability \\(p&gt;0.5\\) is 90%” for our urn challenge.\nHowever, the urn model \\(p\\) is a fixed parameter so it does not make sense to talk about probability.\nWith Bayesian statistics, we assume \\(p\\) is random variable, and thus, a statement such as “90% chance of winning” is consistent with the mathematical approach."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics-2",
    "href": "slides/inference/26-bayes.html#bayesian-statistics-2",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nForecasters also use models to describe variability at different levels.\n\nsampling variability\npollster to pollster variability\nday to day variability\nelection to election variability.\n\nOne of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics-3",
    "href": "slides/inference/26-bayes.html#bayesian-statistics-3",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nThe approach described in the previous chapters, where the parameters are considered fixed, is often referred to as frequentist.\n:::\nIn this chapter, we will briefly describe Bayesian statistics.\nWe use three cases studies: 1) interpreting diagnostic tests for a rare disease, 2) predicting the performance of an athlete, and 3) estimating the probability of Hillary Clinton winning in 2016 using pre-election poll data."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics-4",
    "href": "slides/inference/26-bayes.html#bayesian-statistics-4",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nFor an in-depth treatment of this topic, we recommend one of the following textbooks:\n\nBerger JO (1985).\n\nStatistical Decision Theory and Bayesian Analysis, 2nd edition.\nSpringer-Verlag.\n\nLee PM (1989).\n\nBayesian Statistics: An Introduction."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayesian-statistics-5",
    "href": "slides/inference/26-bayes.html#bayesian-statistics-5",
    "title": "Bayesian Models",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nOxford."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem",
    "href": "slides/inference/26-bayes.html#bayes-theorem",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n_ As a case study: we ill explain the math behind the 538 diagram we started with.\n\nWe start by describing Bayes theorem, using a hypothetical cystic fibrosis test as an example."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-1",
    "href": "slides/inference/26-bayes.html#bayes-theorem-1",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nSuppose a test for cystic fibrosis has an accuracy of 99%.\n\n\\[\n\\mbox{Pr}(+ \\mid D=1)=0.99, \\mbox{Pr}(- \\mid D=0)=0.99  \n\\]\n\n\\(+\\) means a positive test and \\(D = 1\\) if you actually have the disease, \\(D=0\\) if not."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-2",
    "href": "slides/inference/26-bayes.html#bayes-theorem-2",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nImagine we select a random person and they test positive.\nWhat is the probability that they have the disease?\nWe write this as\n\n\\[\n\\mbox{Pr}(D=1 \\mid +)\n\\]\n\nThe cystic fibrosis rate is 1 in 3,900, which implies that \\(\\mbox{Pr}(D=1) \\approx 0.00025\\)."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-3",
    "href": "slides/inference/26-bayes.html#bayes-theorem-3",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nTo answer this question, we will use Bayes theorem, which in general tells us that:\n\n\\[\n\\mbox{Pr}(A \\mid B)  =  \\frac{\\mbox{Pr}(B \\mid A)\\mbox{Pr}(A)}{\\mbox{Pr}(B)}  \n\\]"
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-simulation",
    "href": "slides/inference/26-bayes.html#bayes-theorem-simulation",
    "title": "Bayesian Models",
    "section": "Bayes theorem simulation",
    "text": "Bayes theorem simulation\n\nThe following simulation is meant to help you visualize Bayes theorem.\nWe start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.\n\n\nprev &lt;- 0.00025 \nN &lt;- 100000 \noutcome &lt;- sample(c(\"Disease\",\"Healthy\"), N, replace = TRUE,  \n                  prob = c(prev, 1 - prev)) \n\n\nNote that there are very few people with the disease:"
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-simulation-1",
    "href": "slides/inference/26-bayes.html#bayes-theorem-simulation-1",
    "title": "Bayesian Models",
    "section": "Bayes theorem simulation",
    "text": "Bayes theorem simulation\n\nN_D &lt;- sum(outcome == \"Disease\") \nN_D \n\n[1] 23\n\nN_H &lt;- sum(outcome == \"Healthy\") \nN_H \n\n[1] 99977\n\n\n\nAlso, there are many people without the disease, which makes it more probable that we will see some false positives given that the test is not perfect.\nNow, each person gets the test, which is correct 99% of the time:"
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-simulation-2",
    "href": "slides/inference/26-bayes.html#bayes-theorem-simulation-2",
    "title": "Bayesian Models",
    "section": "Bayes theorem simulation",
    "text": "Bayes theorem simulation\n\naccuracy &lt;- 0.99 \ntest &lt;- vector(\"character\", N) \ntest[outcome == \"Disease\"]  &lt;- sample(c(\"+\", \"-\"), N_D, replace = TRUE,  \n                                    prob = c(accuracy, 1 - accuracy)) \ntest[outcome == \"Healthy\"]  &lt;- sample(c(\"-\", \"+\"), N_H, replace = TRUE,  \n                                    prob = c(accuracy, 1 - accuracy)) \n\n\nGiven that there are so many more controls than cases, even with a low false positive rate, we end up with more controls than cases in the group that tested positive:\n\n\ntable(outcome, test) \n\n         test\noutcome       -     +\n  Disease     0    23\n  Healthy 99012   965\n\n\n\nFrom this table, we see that the proportion of positive tests that have the disease is 23 out of 988."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-simulation-3",
    "href": "slides/inference/26-bayes.html#bayes-theorem-simulation-3",
    "title": "Bayesian Models",
    "section": "Bayes theorem simulation",
    "text": "Bayes theorem simulation\n\nWe can run this over and over again to see that, in fact, the probability converges to about 0.022."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nIn the previous chapter, we computed an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump.\nWe denoted the parameter, the the difference in popular votes, with \\(\\mu\\).\nThe estimate was between 2 and 3 percent, and the confidence interval did not include 0.\nA forecaster would use this to predict Hillary Clinton would win the popular vote."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-1",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-1",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nBut to make a probabilistic statement about winning the election, we need to use a Bayesian approach.\nWe start the Bayesian approach by quantifying our knowledge before seeing any data.\nThis is done using a probability distribution referred to as a prior.\nFor our example, we could write:\n\n\\[\n\\mu \\sim N(\\theta, \\tau)\n\\]"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-2",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-2",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nWe can think of \\(\\theta\\) as our best guess for the popular vote difference had we not seen any polling data, and we can think of \\(\\tau\\) as quantifying how certain we feel about this guess.\nGenerally, if we have expert knowledge related to \\(\\mu\\), we can try to quantify it with the prior distribution.\nIn the case of election polls, experts use fundamentals, which include, for example, the state of the economy, to develop prior distributions.\nThe data is used to update our initial guess or prior belief."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-3",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-3",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nThis can be done mathematically if we define the distribution for the observed data for any given \\(\\mu\\).\nIn our particular example, we would write down a model the average of our polls, which is the same as before:\n\n\\[\n\\bar{X} \\mid \\mu \\sim N(\\mu, \\sigma/\\sqrt{N})\n\\]\n\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-4",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-4",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nIn the Bayesian contexts, this is referred to as the sampling distribution.\nNote that we write the conditional \\(\\bar{X} \\mid \\mu\\) because \\(\\mu\\) is now considered a random variable.\nWe do not show the derivations here, but we can now use calculus and a version of Bayes’ Theorem to derive the distribution of \\(\\mu\\) conditional of the data, referred to as the posterior distribution.\nSpecifically, we can show the \\(\\mu \\mid \\bar{X}\\) follows a normal distribution with expected value:"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-5",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-5",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\\[\n\\begin{aligned}\n\\mbox{E}(\\mu \\mid \\bar{X}) &= B \\theta + (1-B) \\bar{X}\\\\\n&= \\theta + (1-B)(\\bar{X}-\\theta)\\\\\n\\mbox{with } B &= \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2}\n\\end{aligned}\n\\]\n\nand standard error :"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-6",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-6",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\\[\n\\mbox{SE}(\\mu \\mid \\bar{X})^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2}.\n\\]\n\nNote that the expected value is a weighted average of our prior guess \\(\\theta\\) and the observed data \\(\\bar{X}\\).\nThe weight depends on how certain we are about our prior belief, quantified by \\(\\tau\\), and the precision \\(\\sigma/N\\) of the summary of our observed data.\nThis weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-7",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-7",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nThese quantities are useful for updating our beliefs.\nSpecifically, we use the posterior distribution not only to compute the expected value of \\(\\mu\\) given the observed data, but also, for any probability \\(\\alpha\\), we can construct intervals centered at our estimate and with \\(\\alpha\\) chance of occurring."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-8",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-8",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-9",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-9",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nTo compute a posterior distribution and construct a credible interval, we define a prior distribution with mean 0% and standard error 3.5%, which can be interpreted as follows: before seeing polling data, we don’t think any candidate has the advantage, and a difference of up to 7% either way is possible.\nWe compute the posterior distribution using the equations above:"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-10",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-10",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\ntheta &lt;- 0 \ntau &lt;- 0.035 \nsigma &lt;- results$se \nx_bar &lt;- results$avg \nB &lt;- sigma^2 / (sigma^2 + tau^2) \nposterior_mean &lt;- B*theta + (1 - B)*x_bar \nposterior_se &lt;- sqrt(1/(1/sigma^2 + 1/tau^2)) \nposterior_mean \n\n[1] 0.02808534\n\nposterior_se \n\n[1] 0.006149604\n\n\n\nSince we know the posterior distribution is normal, we can construct a credible interval like this:\n\n\nposterior_mean + c(-1, 1) * qnorm(0.975) * posterior_se \n\n[1] 0.01603234 0.04013834"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-11",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-11",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nFurthermore, we can now make the probabilistic statement that we could not make with the frequentists approach by computing the posterior probability of Hillary winning the popular vote.\nSpecifically, \\(\\mbox{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed as follows:\n\n\n1 - pnorm(0, posterior_mean, posterior_se) \n\n[1] 0.9999975\n\n\n\nAccording to the above, we are 100% sure Clinton will win the popular vote, which seems too overconfident.\nAdditionally, it is not in agreement with FiveThirtyEight’s 81.4%."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-12",
    "href": "slides/inference/26-bayes.html#priors-posteriors-and-and-credible-intervals-12",
    "title": "Bayesian Models",
    "section": "Priors, posteriors and and credible intervals",
    "text": "Priors, posteriors and and credible intervals\n\nWhat explains this difference? There is a level of uncertainty that we are not yet describing, and we will return to that in ?@sec-election-forecasting."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-4",
    "href": "slides/inference/26-bayes.html#bayes-theorem-4",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nThis equation, when applied to our problem, becomes:\n\n\\[\n\\begin{aligned}\n\\mbox{Pr}(D=1 \\mid +) & =  \\frac{ P(+ \\mid D=1) \\cdot P(D=1)} {\\mbox{Pr}(+)} \\\\\n& =  \\frac{\\mbox{Pr}(+ \\mid D=1)\\cdot P(D=1)} {\\mbox{Pr}(+ \\mid D=1) \\cdot P(D=1) + \\mbox{Pr}(+ \\mid D=0) \\mbox{Pr}( D=0)}  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-5",
    "href": "slides/inference/26-bayes.html#bayes-theorem-5",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nPlugging in the numbers, we get:\n\n\\[\n\\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)}  =  0.02  \n\\]\n\nAccording to the above, despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02.\nThis might seem counter-intuitive, but it is because we must factor in the very rare probability that a randomly chosen person has the disease."
  },
  {
    "objectID": "slides/inference/26-bayes.html#bayes-theorem-6",
    "href": "slides/inference/26-bayes.html#bayes-theorem-6",
    "title": "Bayesian Models",
    "section": "Bayes theorem",
    "text": "Bayes theorem"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nIn the previous lecture, we computed an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump.\nWe denoted the parameter, the the difference in popular votes, with \\(\\mu\\).\nThe estimate was between 2 and 3 percent, and the confidence interval did not include 0.\nA forecaster would use this to predict Hillary Clinton would win the popular vote."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-1",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-1",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nBut to make a probabilistic statement about winning the election, we need to use a Bayesian approach.\nWe start by quantifying our knowledge before seeing any data.\nThis is done using a probability distribution referred to as a prior."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-2",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-2",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nFor our example, we could write:\n\n\\[\n\\mu \\sim N(\\theta, \\tau)\n\\]\n\nWe can think of \\(\\theta\\) as our best guess for the popular vote difference had we not seen any polling data.\nWe can think of \\(\\tau\\) as quantifying how certain we feel about this guess."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-3",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-3",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nGenerally, if we have expert knowledge related to \\(\\mu\\), we can try to quantify it with the prior distribution.\nIn the case of election polls, experts use fundamentals, for example, the state of the economy, to develop prior distributions.\nThe data is used to update our initial guess or prior belief.\nThis can be done mathematically if we define the distribution for the observed data for any given \\(\\mu\\)."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-4",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-4",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nIn our example, we write down a model for the average of our polls:\n\n\\[\n\\bar{X} \\mid \\mu \\sim N(\\mu, \\sigma/\\sqrt{N})\n\\]\n\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects.\nIn the Bayesian contexts, this is referred to as the sampling distribution.\nNote that we write the conditional \\(\\bar{X} \\mid \\mu\\) because \\(\\mu\\) is now considered a random variable."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-5",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-5",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nWe can now use calculus and a version of Bayes’ Theorem to derive the distribution of \\(\\mu\\) conditional of the data, referred to as the posterior distribution.\nSpecifically, we can show the \\(\\mu \\mid \\bar{X}\\) follows a normal distribution with expected value:\n\n\\[\n\\begin{aligned}\n\\mbox{E}(\\mu \\mid \\bar{X}) &= B \\theta + (1-B) \\bar{X}\\\\\n&= \\theta + (1-B)(\\bar{X}-\\theta)\\\\\n\\mbox{with } B &= \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2} \\mbox{ and }\n\\mbox{SE}(\\mu \\mid \\bar{X})^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-6",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-6",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nThe expected value is a weighted average of our prior guess \\(\\theta\\) and the observed data \\(\\bar{X}\\).\nThe weight depends on how certain we are about our prior belief, quantified by \\(\\tau\\), and the precision \\(\\sigma/N\\) of the summary of our observed data.\nThis weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value."
  },
  {
    "objectID": "slides/inference/26-bayes.html#priors-and-posteriors-7",
    "href": "slides/inference/26-bayes.html#priors-and-posteriors-7",
    "title": "Bayesian Models",
    "section": "Priors and posteriors",
    "text": "Priors and posteriors\n\nThese quantities are useful for updating our beliefs.\nSpecifically, we use the posterior distribution not only to compute the expected value of \\(\\mu\\) given the observed data, but also, for any probability \\(\\alpha\\), we can construct intervals centered at our estimate and with \\(\\alpha\\) chance of occurring."
  },
  {
    "objectID": "slides/inference/26-bayes.html#example",
    "href": "slides/inference/26-bayes.html#example",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\nWe will use these data again:\n\nlibrary(dslabs) \npolls &lt;- polls_us_election_2016 |&gt;  \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) |&gt;  \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt;  \n  filter(enddate == max(enddate)) |&gt; \n  ungroup() \nresults &lt;- one_poll_per_pollster |&gt;  \n  summarize(avg = mean(spread),  \n            se = sd(spread) / sqrt(length(spread))) |&gt;  \n  mutate(start = avg - 1.96 * se,  \n         end = avg + 1.96 * se)"
  },
  {
    "objectID": "slides/inference/26-bayes.html#example-1",
    "href": "slides/inference/26-bayes.html#example-1",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\n\nTo compute a posterior distribution and construct a credible interval, we define a prior distribution with mean 0% and standard error 3.5%, which can be interpreted as follows: before seeing polling data, we don’t think any candidate has the advantage, and a difference of up to 7% either way is possible.\nWe compute the posterior distribution using the equations above:"
  },
  {
    "objectID": "slides/inference/26-bayes.html#example-2",
    "href": "slides/inference/26-bayes.html#example-2",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\n\ntheta &lt;- 0 \ntau &lt;- 0.035 \nsigma &lt;- results$se \nx_bar &lt;- results$avg \nB &lt;- sigma^2 / (sigma^2 + tau^2) \nposterior_mean &lt;- B*theta + (1 - B)*x_bar \nposterior_se &lt;- sqrt(1/(1/sigma^2 + 1/tau^2)) \nposterior_mean \n\n[1] 0.02808534\n\nposterior_se \n\n[1] 0.006149604"
  },
  {
    "objectID": "slides/inference/26-bayes.html#example-3",
    "href": "slides/inference/26-bayes.html#example-3",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\n\nSince we know the posterior distribution is normal, we can construct a credible interval like this:\n\n\nposterior_mean + c(-1, 1) * qnorm(0.975) * posterior_se \n\n[1] 0.01603234 0.04013834"
  },
  {
    "objectID": "slides/inference/26-bayes.html#example-4",
    "href": "slides/inference/26-bayes.html#example-4",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\n\nFurthermore, we can now make the probabilistic statement that we could not make with the frequentists approach by computing the posterior probability of Hillary winning the popular vote.\nSpecifically, \\(\\mbox{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed as follows:\n\n\n1 - pnorm(0, posterior_mean, posterior_se) \n\n[1] 0.9999975"
  },
  {
    "objectID": "slides/inference/26-bayes.html#example-5",
    "href": "slides/inference/26-bayes.html#example-5",
    "title": "Bayesian Models",
    "section": "Example",
    "text": "Example\n\nAccording to this calculation, we are 100% sure Clinton will win the popular vote, which seems too overconfident.\nIt is not in agreement with FiveThirtyEight’s 81.4%.\nWhat explains this difference?\nThere is a level of uncertainty that we are not yet describing, and for which we will need to learn about hierarchical modesl*."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchichal-models",
    "href": "slides/inference/27-hierarchical-models.html#hierarchichal-models",
    "title": "Hierarchical Models",
    "section": "Hierarchichal Models",
    "text": "Hierarchichal Models\n\nHierarchical models are useful for quantifying different levels of variability or uncertainty.\nOne can use them using a Bayesian or Frequentist framework.\nWe illustrate the use of hierarchical models with an example from election forecasting. In this context, hierarchical modeling helps to manage expectations by accounting for various levels of uncertainty including polling biases that change direction from election to election."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting",
    "href": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting",
    "title": "Hierarchical Models",
    "section": "Case study: election forecasting",
    "text": "Case study: election forecasting\n\nIn 2016, most forecasters underestimated Trump’s chances of winning greatly.\nThe day before the election, the New York Times reported the following probabilities for Hillary Clinton winning the presidency:\n\n\n\n\n\n\n\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-1",
    "href": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-1",
    "title": "Hierarchical Models",
    "section": "Case study: election forecasting",
    "text": "Case study: election forecasting\n\nNote that FiveThirtyEight had Trump’s probability of winning at 29%, substantially higher than the others.\nIn fact, four days before the election, FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton.\nSo why did FiveThirtyEight’s model fair so much better than others?"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-2",
    "href": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-2",
    "title": "Hierarchical Models",
    "section": "Case study: election forecasting",
    "text": "Case study: election forecasting\n\nFor illustrative purposes, we will continue examining the popular vote example.\nAt the end we will describe a more complex approach used to forecast the electoral college result."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-3",
    "href": "slides/inference/27-hierarchical-models.html#case-study-election-forecasting-3",
    "title": "Bayesian Models",
    "section": "Case study: election forecasting",
    "text": "Case study: election forecasting\n\nSo why did FiveThirtyEight’s model fair so much better than others? How could PEC and Huffington Post get it so wrong if they were using the same data? In this chapter, we describe how FiveThirtyEight used a hierarchical model to correctly account for key sources of variability and outperform all other forecasters.\nFor illustrative purposes, we will continue examining our popular vote example.\nIn the final section, we will describe the more complex approach used to forecast the electoral college result."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias\n\nWe previously computed the posterior probability of Hillary Clinton winning the popular vote with a standard Bayesian analysis and found it to be very close to 100%.\nHowever, FiveThirtyEight gave her a 81.4% chance.\nWhat explains this difference?"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias-1",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias-1",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias\n\nAfter elections are over, one can look at the difference between the pollster predictions and the actual result.\nAn important observation, that our initial models did not take into account, is that it is common to see a general bias that affects most pollsters in the same way, making the observed data correlated."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias-2",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias-2",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias\nPredicted versus observed:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nSuppose we are collecting data from one pollster and we assume there is no general bias.\nThe pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\).\nSuppose the real proportion for Hillary is \\(p\\) and the difference is \\(\\mu\\).\nThe urn model theory tells us that these random variables are normally distributed, with expected value \\(\\mu\\) and standard error \\(2 \\sqrt{p(1-p)/N}\\):"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-1",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-1",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\\[\nX_j \\sim \\mbox{N}\\left(\\mu, 2\\sqrt{p(1-p)/N}\\right)\n\\]\n\nWe use the index \\(j\\) to represent the different polls conducted by this pollster.\nBelow is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-2",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-2",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nset.seed(3) \nJ &lt;- 6 \nN &lt;- 2000 \nmu &lt;- .021 \np &lt;- (mu + 1)/2 \nX &lt;- rnorm(J, mu, 2*sqrt(p*(1 - p)/N)) \n\n\nNow, suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters.\nFor simplicity, let’s say all polls had the same sample size \\(N\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-3",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-3",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nThe urn model tell us the distribution is the same for all pollsters, so to simulate data, we use the same model for each:\n\n\nI &lt;- 5 \nJ &lt;- 6 \nN &lt;- 2000 \nX &lt;- sapply(1:I, function(i){ \n  rnorm(J, mu, 2*sqrt(p*(1 - p)/N)) \n}) \n\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-4",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-4",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-5",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-5",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes.\nWe use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster.\nThe model is now augmented to include pollster effects \\(h_i\\), referred to as “house effects” by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-6",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-6",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nX_{i,j} \\mid h_i &\\sim \\mbox{N}\\left(\\mu + h_i, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\n\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\), and then generate individual poll data after adding this effect."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-7",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-7",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nHere is how we would do it for one specific pollster.\nWe assume \\(\\sigma_h\\) is 0.025:\n\n\nI &lt;- 5 \nJ &lt;- 6 \nN &lt;- 2000 \nmu &lt;- .021 \np &lt;- (mu + 1)/2 \nh &lt;- rnorm(I, 0, 0.025) \nX &lt;- sapply(1:I, function(i){ \n  mu + h[i] + rnorm(J, 0, 2*sqrt(p*(1 - p)/N)) \n}) \n\n\nThe simulated data now looks more like the actual data:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-8",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-8",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-9",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-9",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nNote that \\(h_i\\) is common to all the observed spreads from a specific pollster.\nDifferent pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in the model above, we assume the average house effect is 0.\nWe think that for every pollster biased in favor of our party, there is another one in favor of the other, and assume the standard deviation is \\(\\sigma_h\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-10",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-10",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nBut, historically, we see that every election has a general bias affecting all polls.\nWe can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict.\nTo see this, we would take the average of polls for each election year and compare it to the actual value.\nIf we did this, we would see a difference with a standard deviation of between 2-3%."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-11",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-11",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nTo account for this variability we can add another level to the model as follows:\n\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nX_{i,j} | \\, h_j, b &\\sim \\mbox{N}\\left(\\mu + h_j, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-12",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-12",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nThis model accounts for three levels of variability: 1) variability in the bias observed from election to election, quantified by \\(\\sigma_b\\), 2) pollster-to-pollster or house effect variability, quantified by \\(\\sigma_h\\), and 3) poll sampling variability, which we can derive to be \\(\\sqrt(p(1-p)/N)\\).\nNote that not including a term like \\(b\\) in the models is what led many forecasters to be overconfident.\nThis random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within one election (note it does not have an index)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-13",
    "href": "slides/inference/27-hierarchical-models.html#mathematical-representations-of-the-hierarchical-model-13",
    "title": "Hierarchical Models",
    "section": "Mathematical representations of the hierarchical model",
    "text": "Mathematical representations of the hierarchical model\n\nThis implies that we can’t estimate \\(\\sigma_h\\) with data from just one election.\nIt also implies that the random variables \\(X_{i,j}\\) for a fixed election year share the same \\(b\\) and are therefore correlated.\nOne way to interpret \\(b\\) is as the difference between the average of all polls from all pollsters and the actual result of the election.\nSince we don’t know the actual result until after the election, we can’t estimate \\(b\\) until then."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nNow, let’s fit the model above to data:\n\n\npolls &lt;- polls_us_election_2016 |&gt;  \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\", \"B\"))) |&gt;  \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-1",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-1",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nHere, we have just one poll per pollster, so we will drop the \\(j\\) index and represent the data as before with \\(X_1, \\dots, X_I\\).\n\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt;  \n  filter(enddate == max(enddate)) |&gt; \n  ungroup()"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-2",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-2",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nAs a reminder, we have data from \\(I=15\\) pollsters.\nBased on the model assumptions described above, we can mathematically show that the average \\(\\bar{X}\\):\n\n\nx_bar &lt;- mean(one_poll_per_pollster$spread) \n\n\nthis has expected value \\(\\mu\\).\nBut how precise is this estimate?"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-3",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-3",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nBecause the \\(X_i\\) are correlated, estimating the standard error is more complex than what we have described up to now.\nSpecifically, using advanced statistical calculations not shown here, we can show that the typical variance (standard error squared) estimate:\n\n\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2/length(spread)) \n\n\nwill consistently underestimate the true standard error by about \\(\\sigma_b^2\\).\nAnd, as mentioned earlier, to estimate \\(\\sigma_b\\), we need data from several elections."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-4",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-4",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nBy collecting and analyzing polling data from several elections, we estimates this variability with \\(\\sigma_b \\approx 0.025\\).\nWe can therefore greatly improve our standard error estimate by adding this quantity:\n\n\nsigma_b &lt;- 0.025 \nse &lt;- sqrt(s2 + sigma_b^2)"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-5",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-5",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nIf we redo the Bayesian calculation taking this variability into account, we obtain a result much closer to FiveThirtyEight’s:\n\n\nmu &lt;- 0 \ntau &lt;- 0.035 \nB &lt;- se^2/(se^2 + tau^2) \nposterior_mean &lt;- B*mu + (1 - B)*x_bar \nposterior_se &lt;- sqrt(1/(1/se^2 + 1/tau^2)) \n1 - pnorm(0, posterior_mean, posterior_se) \n\n[1] 0.8414884\n\n\n\nBy accounting for the general bias term, we produce a posterior probability similar to that reported by FiveThirtyEight."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-6",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-6",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nmu &lt;- 0 \ntau &lt;- 0.035 \nB &lt;- se^2/(se^2 + tau^2) \nposterior_mean &lt;- B*mu + (1 - B)*x_bar \nposterior_se &lt;- sqrt(1/(1/se^2 + 1/tau^2)) \n1 - pnorm(0, posterior_mean, posterior_se) \n\n[1] 0.8174373\n\n\n\nNotice that by accounting for the general bias term, our Bayesian analysis now produces a posterior probability similar to that reported by FiveThirtyEight.\n:::{.callout-note}.\nKeep in mind that we are simplifying FiveThirtyEight’s calculations related to the general bias \\(b\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-7",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-probability-7",
    "title": "Hierarchical Models",
    "section": "Computing a posterior probability",
    "text": "Computing a posterior probability\n\nFor example, one of the many ways their analysis is more complex than the one presented here is that FiveThirtyEight permits \\(b\\) to vary across regions of the country.\nThis helps because, historically, we have observed geographical patterns in voting behaviors.\n:::"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nUp to now, we have focused on the popular vote.\nHowever, in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college.\nEach state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-1",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-1",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nHere are the top 5 states ranked by electoral votes in 2016:\n\n\nresults_us_election_2016 |&gt; top_n(5, electoral_votes) \n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\n\nWith some minor exceptions the electoral votes are won on an all-or-nothing basis."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-2",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-2",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nFor example, if you won California in 2016 by just 1 vote, you still get all 55 of its electoral votes.\nThis means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college.\nThis happened in 1876, 1888, 2000, and 2016."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-3",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-3",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nWe are now ready to predict the electoral college result for 2016.\nWe start by aggregating results from a poll taken during the last week before the election:\n\n\nresults &lt;- polls_us_election_2016 |&gt; \n  filter(state != \"U.S.\" &  \n           !grepl(\"CD\", state) &  \n           enddate &gt;= \"2016-10-31\" &  \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt; \n  group_by(state) |&gt; \n  summarize(avg = mean(spread), sd = sd(spread), n = n()) |&gt; \n  mutate(state = as.character(state))"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-4",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-4",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nHere are the five closest races according to the polls:\n\n\nresults |&gt; arrange(abs(avg)) \n\n# A tibble: 47 × 4\n   state               avg     sd     n\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1 Florida         0.00356 0.0163     7\n 2 North Carolina -0.0073  0.0306     9\n 3 Ohio           -0.0104  0.0252     6\n 4 Nevada          0.0169  0.0441     7\n 5 Iowa           -0.0197  0.0437     3\n 6 Michigan        0.0209  0.0203     6\n 7 Arizona        -0.0326  0.0270     9\n 8 Pennsylvania    0.0353  0.0116     9\n 9 New Mexico      0.0389  0.0226     6\n10 Georgia        -0.0448  0.0238     4\n# ℹ 37 more rows"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-5",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-5",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nWe now introduce the command left_join that will let us easily add the number of electoral votes for each state from the dataset us_electoral_votes_2016.\nHere, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first:\n\n\nresults &lt;- left_join(results, results_us_election_2016, by = \"state\")"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-6",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-6",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nNotice that some states have no polls because the winner is pretty much known:\n\n\nresults_us_election_2016 |&gt; filter(!state %in% results$state) |&gt;  \n  pull(state) \n\n[1] \"Rhode Island\"         \"Alaska\"               \"Wyoming\"             \n[4] \"District of Columbia\"\n\n\n\nNo polls were conducted in DC, Rhode Island, Alaska, and Wyoming because Democrats are sure to win in the first two and Republicans in the last two."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-7",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-7",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nBecause we can’t estimate the standard deviation for states with just one poll, we will estimate it as the median of the standard deviations estimated for states with more than one poll:\n\n\nresults &lt;- results |&gt; \n  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-8",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-8",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nTo make probabilistic arguments, we will use a Monte Carlo simulation.\nFor each state, we apply the Bayesian approach to generate an election day \\(\\mu\\).\nWe could construct the priors for each state based on recent history."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-9",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-9",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nHowever, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen.\nGiven that results from a specific state don’t vary significantly from election year to election year, we will assign a standard deviation of 2% or \\(\\tau=0.02\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-10",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-10",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nFor now, we will assume, incorrectly, that the poll results from each state are independent.\nThe code for the Bayesian calculation under these assumptions looks like this:\n\n\nmu &lt;- 0 \ntau &lt;- 0.02 \nresults |&gt; mutate(sigma = sd/sqrt(n),  \n                   B = sigma^2/(sigma^2 + tau^2), \n                   posterior_mean = B*mu + (1 - B)*avg, \n                   posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2))) \n\n# A tibble: 47 × 12\n   state          avg      sd     n electoral_votes clinton trump others   sigma\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama   -0.149   2.53e-2     3               9    34.4  62.1    3.6 1.46e-2\n 2 Arizona   -0.0326  2.70e-2     9              11    45.1  48.7    6.2 8.98e-3\n 3 Arkansas  -0.151   9.90e-4     2               6    33.7  60.6    5.8 7.00e-4\n 4 Californ…  0.260   3.87e-2     5              55    61.7  31.6    6.7 1.73e-2\n 5 Colorado   0.0452  2.95e-2     7               9    48.2  43.3    8.6 1.11e-2\n 6 Connecti…  0.0780  2.11e-2     3               7    54.6  40.9    4.5 1.22e-2\n 7 Delaware   0.132   3.35e-2     2               3    53.4  41.9    4.7 2.37e-2\n 8 Florida    0.00356 1.63e-2     7              29    47.8  49      3.2 6.18e-3\n 9 Georgia   -0.0448  2.38e-2     4              16    45.9  51      3.1 1.19e-2\n10 Hawaii     0.186   2.10e-2     1               4    62.2  30      7.7 2.10e-2\n# ℹ 37 more rows\n# ℹ 3 more variables: B &lt;dbl&gt;, posterior_mean &lt;dbl&gt;, posterior_se &lt;dbl&gt;"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-11",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-11",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-12",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-12",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nNow, we repeat this 10,000 times and generate an outcome from the posterior.\nIn each iteration, we track the total number of electoral votes for Clinton.\nRemember that Trump gets 270 votes minus the ones for Clinton.\nAlso, note that the reason we add 7 in the code is to account for Rhode Island and D.C."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-13",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-13",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nB &lt;- 10000 \nmu &lt;- 0 \ntau &lt;- 0.02 \nclinton_EV &lt;- replicate(B, { \n  results |&gt; mutate(sigma = sd/sqrt(n),  \n                   B = ifelse(n &gt;= 5, sigma^2 / (sigma^2 + tau^2), 0), \n                   posterior_mean = B*mu + (1 - B)*avg, \n                   posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)), \n                   result = rnorm(length(posterior_mean),  \n                                  posterior_mean, posterior_se), \n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) |&gt;  \n    summarize(clinton = sum(clinton)) |&gt;  \n    pull(clinton) + 7 \n}) \nmean(clinton_EV &gt; 269) \n\n[1] 0.9982"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-14",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-14",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nThis model gives Clinton over 99% chance of winning.\nA similar prediction was made by the Princeton Election Consortium.\nWe now know it was quite off.\nWhat happened?\nThe model above ignores the general bias and assumes the results from different states are independent.\nAfter the election, we realized that the general bias in 2016 was not that big: it was between 2% and 3%."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-15",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-15",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nBut because the election was close in several big states and these states had a large number of polls, pollsters that ignored the general bias greatly underestimated the standard error.\nUsing the notation we introduced, they assumed the standard error was \\(\\sqrt{\\sigma^2/N}\\).\nWith large \\(N\\), this estimate is substiantially closer to 0 than the more accurate estimate \\(\\sqrt{\\sigma^2/N + \\sigma_b^2}\\).\nFiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-16",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-16",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nWe can simulate the results now with a bias term.\nFor the state level, the general bias can be larger so we set it at \\(\\sigma_b = 0.035\\):"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-17",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-17",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\ntau &lt;- 0.02 \nbias_sd &lt;- 0.035 \nclinton_EV_2 &lt;- replicate(10000, { \n  results |&gt; mutate(sigma = sd/sqrt(n), \n                    #few polls means not in play so don't shrink \n                    B = ifelse(n &gt;= 5, sigma^2 / (sigma^2 + tau^2), 0), \n                    posterior_mean = B*mu + (1 - B)*avg, \n                    posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)), \n                    result = rnorm(length(posterior_mean),  \n                                  posterior_mean, sqrt(posterior_se^2 + bias_sd^2)), \n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) |&gt;  \n    summarize(clinton = sum(clinton) + 7) |&gt;  \n    pull(clinton) \n}) \nmean(clinton_EV_2 &gt; 269) \n\n[1] 0.8878\n\n\n\nThis gives us a much more sensible estimate."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-18",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-18",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nLooking at the outcomes of the simulation, we see how the bias term adds variability to the final results."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting",
    "href": "slides/inference/27-hierarchical-models.html#forecasting",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nForecasters like to make predictions well before the election.\nThe predictions are adapted as new poll results are released.\nHowever, an important question forecasters must ask is: How informative are polls taken several weeks before the election about the actual election? Here, we study the variability of poll results across time."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-1",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-1",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\nTo make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:\n\none_pollster &lt;- polls_us_election_2016 |&gt;  \n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt;  \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-2",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-2",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nSince there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation.\nWe compute both here:\n\n\nse &lt;- one_pollster |&gt;  \n  summarize(empirical = sd(spread),  \n            theoretical = 2*sqrt(mean(spread)*(1 - mean(spread))/min(samplesize))) \nse \n\n   empirical theoretical\n1 0.04025194  0.03256719"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-3",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-3",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nBut the empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-4",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-4",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nThe models we have described include pollster-to-pollster variability and sampling error.\nBut this plot is for one pollster and the variability we see is certainly not explained by sampling error.\nWhere is the extra variability coming from?"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-5",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-5",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\nThe following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes \\(p\\) is fixed:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-6",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-6",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nSome of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see that the peaks and valleys are consistent across several pollsters:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-7",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-7",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nThis implies that if we are going to forecast, we must include a term to accounts for the time effect.\nWe can add a bias term for time and denote as \\(b_t\\).\nThe standard deviation of \\(b_t\\) would depend on \\(t\\) since the closer we get to election day, the closer to 0 this bias term should be.\nPollsters also try to estimate trends from these data and incorporate them into their predictions.\nWe can model the time trend \\(b_t\\) with a smooth function."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-8",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-8",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nWe usually see the trend estimate not for the difference, but for the actual percentages for each candidate like this:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-9",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-9",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nWe usually see the trend estimate not for the difference, but for the actual percentages for each candidate like this:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-10",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-10",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#forecasting-11",
    "href": "slides/inference/27-hierarchical-models.html#forecasting-11",
    "title": "Hierarchical Models",
    "section": "Forecasting",
    "text": "Forecasting\n\nOnce a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions.\nThere is a variety of methods for estimating trends which we discuss in the section on Machine Learning."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias-3",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias-3",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias-4",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias-4",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias\n\nThere is no agreed upon explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%; then in the following election, they favor Republicans by 1%; then in the next election there is no bias; then in the following one Republicans are favored by 3%, …\nThere appears to be regional effect, not included in our analysis."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#the-general-bias-5",
    "href": "slides/inference/27-hierarchical-models.html#the-general-bias-5",
    "title": "Hierarchical Models",
    "section": "The general bias",
    "text": "The general bias\n\nIn 2016, the polls were biased in favor of the Democrats by 2-3%.\nAlthough we know this bias term affects our polls, we have no way of knowing what this bias is until election night.\nSo we can’t correct our polls accordingly.\nWhat we can do is include a term in our model that accounts for the variability."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nSuppose we are collecting data from one pollster and we assume there is no general bias.\nThe pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\).\nSuppose the real proportion for Hillary is \\(p\\) and the difference is \\(\\mu\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-1",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-1",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nThe urn model theory tells us that:\n\n\\[\nX_j \\sim \\mbox{N}\\left(\\mu, 2\\sqrt{p(1-p)/N}\\right)\n\\]\n\nWe use the index \\(j\\) to represent the different polls conducted by this pollster."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-2",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-2",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nHere is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:\n\n\nset.seed(3) \nJ &lt;- 6 \nN &lt;- 2000 \nmu &lt;- .021 \np &lt;- (mu + 1)/2 \nX &lt;- rnorm(J, mu, 2*sqrt(p*(1 - p)/N))"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-3",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-3",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nNow, suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters.\nFor simplicity, let’s say all polls had the same sample size \\(N\\).\nThe urn model tell us the distribution is the same for all pollsters, so to simulate data, we use the same model for each:\n\n\nI &lt;- 5 \nJ &lt;- 6 \nN &lt;- 2000 \nX &lt;- sapply(1:I, function(i){ \n  rnorm(J, mu, 2*sqrt(p*(1 - p)/N)) \n})"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-4",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-4",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-5",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-5",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes.\nWe use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-6",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-6",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nThe model is now augmented to include pollster effects \\(h_i\\), referred to as “house effects” by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):\n\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nX_{i,j} \\mid h_i &\\sim \\mbox{N}\\left(\\mu + h_i, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-7",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-7",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\), and then generate individual poll data after adding this effect.\nHere is how we would do it for one specific pollster (we assume \\(\\sigma_h\\) is 0.025):\n\n\nI &lt;- 5 \nJ &lt;- 6 \nN &lt;- 2000 \nmu &lt;- .021 \np &lt;- (mu + 1)/2 \nh &lt;- rnorm(I, 0, 0.025) \nX &lt;- sapply(1:I, function(i){ \n  mu + h[i] + rnorm(J, 0, 2*sqrt(p*(1 - p)/N)) \n})"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-8",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-8",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nThe simulated data now looks more like the actual data:"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-9",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-9",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nNote that \\(h_i\\) is common to all the observed spreads from a specific pollster.\nDifferent pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in this model, we assume the average pollster effect is 0.\nWe think that for every pollster biased in favor of our party, there is another one in favor of the other, and assume the standard deviation is \\(\\sigma_h\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-10",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-10",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nBut, historically, we see that every election has a general bias affecting all polls.\nWe can observe this with the 2016 data.\nIf we collect historical data, we see that the average of polls misses by more than models like the one above predict.\nTo see this, we would take the average of polls for each election year and compare it to the actual value.\nIf we did this, we would see a difference with a standard deviation of between 2-3%."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-11",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-11",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nTo account for this variability we can add another level to the model as follows:\n\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nX_{i,j} | \\, h_j, b &\\sim \\mbox{N}\\left(\\mu + h_j, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-12",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-12",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\nThis model accounts for three levels of variability:\n\nVariability in the bias observed from election to election, quantified by \\(\\sigma_b\\).\nPollster-to-pollster or house effect variability, quantified by \\(\\sigma_h\\).\nPoll sampling variability, which we can derive to be \\(\\sqrt(p(1-p)/N)\\)."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-13",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-13",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nNote that not including a term like \\(b\\) in the models is what led many forecasters to be overconfident.\nThis random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within one election.\nThis implies that we can’t estimate \\(\\sigma_h\\) with data from just one election.\nIt also implies that the random variables \\(X_{i,j}\\) for a fixed election year share the same \\(b\\) and are therefore correlated."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#hierarchical-model-14",
    "href": "slides/inference/27-hierarchical-models.html#hierarchical-model-14",
    "title": "Hierarchical Models",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nOne way to interpret \\(b\\) is as the difference between the average of all polls from all pollsters and the actual result of the election.\nSince we don’t know the actual result until after the election, we can’t estimate \\(b\\) until then."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-19",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-19",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nFiveThirtyEight includes many other features we do not include here.\nOne is that they model variability with distributions that have high probabilities for extreme events compared to the normal.\nOne way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution.\nFiveThirtyEight predicted a probability of 71%."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nNow, let’s fit the model above to data:\n\n\npolls &lt;- polls_us_election_2016 |&gt;  \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\", \"B\"))) |&gt;  \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-1",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-1",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nHere, we have just one poll per pollster, so we will drop the \\(j\\) index and represent the data as before with \\(X_1, \\dots, X_I\\).\n\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt;  \n  filter(enddate == max(enddate)) |&gt; \n  ungroup()"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-2",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-2",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nAs a reminder, we have data from \\(I=15\\) pollsters.\nBased on the model assumptions described above, we can mathematically show that the average \\(\\bar{X}\\):\n\n\nx_bar &lt;- mean(one_poll_per_pollster$spread) \n\n\nthis has expected value \\(\\mu\\).\nBut how precise is this estimate?"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-3",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-3",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nBecause the \\(X_i\\) are correlated, estimating the standard error is more complex than what we have described up to now.\nSpecifically, using advanced statistical calculations not shown here, we can show that the typical variance (standard error squared) estimate:\n\n\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2/length(spread)) \n\n\nwill consistently underestimate the true standard error by about \\(\\sigma_b^2\\).\nTo estimate \\(\\sigma_b\\), we need data from several elections."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-4",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-4",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nBy collecting and analyzing polling data from several elections, we estimates this variability with \\(\\sigma_b \\approx 0.025\\).\nWe can therefore greatly improve our standard error estimate by adding this quantity:\n\n\nsigma_b &lt;- 0.025 \nse &lt;- sqrt(s2 + sigma_b^2)"
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#computing-a-posterior-5",
    "href": "slides/inference/27-hierarchical-models.html#computing-a-posterior-5",
    "title": "Hierarchical Models",
    "section": "Computing a posterior",
    "text": "Computing a posterior\n\nIf we redo the Bayesian calculation taking this variability into account, we obtain a result much closer to FiveThirtyEight’s:\n\n\nmu &lt;- 0 \ntau &lt;- 0.035 \nB &lt;- se^2/(se^2 + tau^2) \nposterior_mean &lt;- B*mu + (1 - B)*x_bar \nposterior_se &lt;- sqrt(1/(1/se^2 + 1/tau^2)) \n1 - pnorm(0, posterior_mean, posterior_se) \n\n[1] 0.8414884\n\n\n\nBy accounting for the general bias term, we produce a posterior probability similar to that reported by FiveThirtyEight."
  },
  {
    "objectID": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-20",
    "href": "slides/inference/27-hierarchical-models.html#predicting-the-electoral-college-20",
    "title": "Hierarchical Models",
    "section": "Predicting the electoral college",
    "text": "Predicting the electoral college\n\nFiveThirtyEight includes many other features we do not include here.\nOne is that they model variability with distributions that have high probabilities for extreme events compared to the normal.\nOne way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution.\nFiveThirtyEight predicted a probability of 71%."
  },
  {
    "objectID": "slides/inference/26-bayes.html",
    "href": "slides/inference/26-bayes.html",
    "title": "Bayesian Models",
    "section": "",
    "text": "In 2016 FiveThirtyEight showed this diagram:\n\n\n\n\n\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model."
  },
  {
    "objectID": "slides/inference/25-models.html",
    "href": "slides/inference/25-models.html",
    "title": "Data-driven models",
    "section": "",
    "text": "“All models are wrong, but some are useful.” –George E.P. Box."
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final project you will choose one of these topics:\n\nCOVID-19 pandemic in the US\nExcess mortality in Puerto Rico after Hurricane María.\nTopic of your choice (needs instructor approval).\nElection results dashboard.\n\nYou will submit your project using Git. Your project should be completely reproducible, meaning all the code and data needed to render your report from scratch should be in the repository.\nYou can work alone or in groups of at most three people.\nIf you select option 4 you can skip to @dashboard.\n\n\nFor the first three options, you will prepare a comprehensive report following the style of an academic paper. This report will be divided into the following five structured sections, with approximate word counts to help you reach a target of 2,500 to 3,000 words, up to four figures and up to two tables.\n\n\n\nPurpose: The abstract provides a concise summary of your project, including its objectives, key findings, and significance. Write this section last, after completing all other sections, to accurately reflect your project’s focus and main results.\nGuidelines: Limit this section to 150-200 words. Briefly outline the purpose of your study, the approach you used, and the primary results and conclusions. The abstract should be clear, succinct, and give readers an immediate understanding of what your project entails.\n\n\n\n\n\nPurpose: The introduction sets the stage for your project, presenting the background and rationale for your analysis. Explain why the topic is significant.\nGuidelines: Start with a broad overview of the topic, gradually narrowing down to your specific focus. Conclude with a clear statement of your research questions, hypotheses, or objectives. Use 2-3 paragraphs to establish a solid foundation for the rest of the paper.\n\n\n\n\n\nPurpose: This section details the data sources, methods, and analytical techniques you used to conduct your analysis. It should be specific enough that someone else could replicate your study using the same resources and approach.\nGuidelines: Describe the dataset(s) you used, including information about data collection (e.g., sources, time frame). Outline your approach for cleaning and analyzing the data, including any statistical or computational methods applied. Clearly explain any assumptions or limitations in your approach.\n\n\n\n\n\nPurpose: The results section presents the main findings of your analysis without interpretation. Organize the data logically to highlight key insights, using tables, figures, and charts to illustrate trends and comparisons.\nGuidelines: For each result, briefly describe it and refer to relevant visuals or tables where appropriate. Do not provide explanations or discuss implications in this section; focus only on presenting the findings clearly and accurately.\n\n\n\n\n\nPurpose: In the discussion, interpret the significance of your findings, explore potential implications, and relate the results back to your initial research questions or hypotheses. This section allows you to discuss any patterns, unexpected findings, or limitations and suggest possible future research.\nGuidelines: Analyze your results in the context of your research question, linking them back to the background information from the introduction. Consider what your findings reveal, any limitations they may have, and how they might impact future work or policy. End with a brief conclusion summarizing your main insights.\n\nYour final report should be professionally formatted, with each section clearly labeled and referenced. Aim for clarity, precision, and a well-organized presentation of your analysis.\nTotal Word Count: Approximately 2,500-3,000 words.\n\n\n\n\nYou can include a separate document titled Supplementary Methods.\n\nPurpose: Share any mathematical derivations, data visualizations, or tables needed to justify the choices described in the Methods Section. You can also provide further support for the claims made in the Results Section. You can refer to this document in the main report.\nGuidelines: There is no limits in the length of this section nor on the number of figures and tables. However, be careful not to drown the graders with too much information.\n\n\n\n\nHere are two examples or papers related to the two first topics:\n\nA Flexible Statistical Framework for Estimating Excess Mortality\nExcess deaths associated with covid-19 pandemic in 2020: age and sex disaggregated time series analysis in 29 high income countries\n\n\n\n\nWe recommend your repository include:\n\nDirectories code, data, and docs.\nAt least one script for wrangling in the code directory.\nThere should be one file called final-project.qmd that can be rendered to produce the final report. This can be in the code or home directories as long as it renders.\nYou should include a README file explaining how to reproduce all the results.\nIf you need to share raw data include it in the raw-data directory. Alternatively, you can include code that downloads the necessary data from the internet.\n\nWe expect to see at least five commits by each person.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#sections",
    "href": "final-project.html#sections",
    "title": "Final Project",
    "section": "",
    "text": "For the first three options, you will prepare a comprehensive report following the style of an academic paper. This report will be divided into the following five structured sections, with approximate word counts to help you reach a target of 2,500 to 3,000 words, up to four figures and up to two tables.\n\n\n\nPurpose: The abstract provides a concise summary of your project, including its objectives, key findings, and significance. Write this section last, after completing all other sections, to accurately reflect your project’s focus and main results.\nGuidelines: Limit this section to 150-200 words. Briefly outline the purpose of your study, the approach you used, and the primary results and conclusions. The abstract should be clear, succinct, and give readers an immediate understanding of what your project entails.\n\n\n\n\n\nPurpose: The introduction sets the stage for your project, presenting the background and rationale for your analysis. Explain why the topic is significant.\nGuidelines: Start with a broad overview of the topic, gradually narrowing down to your specific focus. Conclude with a clear statement of your research questions, hypotheses, or objectives. Use 2-3 paragraphs to establish a solid foundation for the rest of the paper.\n\n\n\n\n\nPurpose: This section details the data sources, methods, and analytical techniques you used to conduct your analysis. It should be specific enough that someone else could replicate your study using the same resources and approach.\nGuidelines: Describe the dataset(s) you used, including information about data collection (e.g., sources, time frame). Outline your approach for cleaning and analyzing the data, including any statistical or computational methods applied. Clearly explain any assumptions or limitations in your approach.\n\n\n\n\n\nPurpose: The results section presents the main findings of your analysis without interpretation. Organize the data logically to highlight key insights, using tables, figures, and charts to illustrate trends and comparisons.\nGuidelines: For each result, briefly describe it and refer to relevant visuals or tables where appropriate. Do not provide explanations or discuss implications in this section; focus only on presenting the findings clearly and accurately.\n\n\n\n\n\nPurpose: In the discussion, interpret the significance of your findings, explore potential implications, and relate the results back to your initial research questions or hypotheses. This section allows you to discuss any patterns, unexpected findings, or limitations and suggest possible future research.\nGuidelines: Analyze your results in the context of your research question, linking them back to the background information from the introduction. Consider what your findings reveal, any limitations they may have, and how they might impact future work or policy. End with a brief conclusion summarizing your main insights.\n\nYour final report should be professionally formatted, with each section clearly labeled and referenced. Aim for clarity, precision, and a well-organized presentation of your analysis.\nTotal Word Count: Approximately 2,500-3,000 words.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#supplementary-methods-no-limit",
    "href": "final-project.html#supplementary-methods-no-limit",
    "title": "Final Project",
    "section": "",
    "text": "You can include a separate document titled Supplementary Methods.\n\nPurpose: Share any mathematical derivations, data visualizations, or tables needed to justify the choices described in the Methods Section. You can also provide further support for the claims made in the Results Section. You can refer to this document in the main report.\nGuidelines: There is no limits in the length of this section nor on the number of figures and tables. However, be careful not to drown the graders with too much information.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#examples",
    "href": "final-project.html#examples",
    "title": "Final Project",
    "section": "",
    "text": "Here are two examples or papers related to the two first topics:\n\nA Flexible Statistical Framework for Estimating Excess Mortality\nExcess deaths associated with covid-19 pandemic in 2020: age and sex disaggregated time series analysis in 29 high income countries",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#githut-repository",
    "href": "final-project.html#githut-repository",
    "title": "Final Project",
    "section": "",
    "text": "We recommend your repository include:\n\nDirectories code, data, and docs.\nAt least one script for wrangling in the code directory.\nThere should be one file called final-project.qmd that can be rendered to produce the final report. This can be in the code or home directories as long as it renders.\nYou should include a README file explaining how to reproduce all the results.\nIf you need to share raw data include it in the raw-data directory. Alternatively, you can include code that downloads the necessary data from the internet.\n\nWe expect to see at least five commits by each person.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#data",
    "href": "final-project.html#data",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nYou can use the data we downloaded in problem set 4.\nWe want you to examine the entire pandemic, until at least December 1, 2024 which means you will need to obtain population estimates for 2023 and 2024.\nFor those working in groups we want you to obtain daily or weekly mortality data for each state.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#tasks-and-questions",
    "href": "final-project.html#tasks-and-questions",
    "title": "Final Project",
    "section": "Tasks and questions",
    "text": "Tasks and questions\n\nDivide the pandemic period, January 2020 to December 2024 into waves. Justify your choice with data visualization.\nFor each period compute the deaths rates by state. Describe which states did better or worse during the different periods.\nDescribe if COVID-19 became less or more virulent across the different periods.\nFor those working in groups: Estimate excess mortality for each week for each state. Do COVID-19 deaths explain the excess mortality?\nFor those working in groups: Repeat 2 but for excess mortality instead of COVID-19 deaths.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#data-1",
    "href": "final-project.html#data-1",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nYou can use puerto_rico_counts in the excessmort package.\nFor those working in groups we want you to wrangle the data from the pdf report shared by the New York Times. https://github.com/c2-d2/pr_mort_official/raw/master/data/Mortalidad-RegDem-2015-17-NYT-part1.pdf",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#tasks-and-questions-1",
    "href": "final-project.html#tasks-and-questions-1",
    "title": "Final Project",
    "section": "Tasks and questions",
    "text": "Tasks and questions\n\nExamine the population sizes by age group and sex. Describe any interesting patterns.\nUse data from before 2017 to estimate expected mortality and a standard deviation for each week. Do this by age group and sex. Describe tendencies you observe. You can combine data into bigger age groups if the data show they have similar death rates.\nExplore the data to see if there are periods during or before 2017 that appear to have excess mortality. If so, explain and recompute expected death rates removing these periods.\nEstimate excess deaths for each week of 2017-2018. Make sure you define the weeks so that one of the weeks starts the day María made landfall. Comment on excess mortality. Which age groups were affected? Were men and women affected differently?\nFor those working in groups: Extract the data from the PDF shared with NY Times. Comment on how it matches with excessmort package data.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#data-2",
    "href": "final-project.html#data-2",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nObtain pollster data for 2016, 2018, 2020, 2022 elections. We already provided data from the 2024 election.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#tasks-and-questions-2",
    "href": "final-project.html#tasks-and-questions-2",
    "title": "Final Project",
    "section": "Tasks and questions",
    "text": "Tasks and questions\n\nWe want you to build a shiny dashboard to be ready to predict the 2026 senate election. To do this you will prepare a dashboard for the 2024 senate election.\nThe dashboard should show your prediction for each senate race based on a Bayesian analysis.\nThe dashboard should also show a prediction for the number of seats the democrats will hold. It should display a histogram with your estimate of the probability distribution for each possible outcome.\nUse data from previous years to motivate the choice of priors.\nFor those working in groups: Use the previous 4 elections to estimate correlations between states in the polling bias. Use it to improve your model.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final-project.html#github",
    "href": "final-project.html#github",
    "title": "Final Project",
    "section": "GitHub",
    "text": "GitHub\nAll the code needed to construct the shiny app should be included in a GitHub repository, including whatever wrangling you performed for your data analysis.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nTo understand the concepts of correlation and simple regression, we actually use the dataset from which regression was born.\nThe example is from genetics.\nFrancis Galton studied the variation and heredity of human traits.\nAmong many other traits, Galton collected and studied height data from families to try to understand heredity."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-1",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-1",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nWhile doing this, he developed the concepts of correlation and regression.\nAt the time this data was collected, our knowledge of genetics was limited compared to what we know today.\nA very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height?"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-2",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-2",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nWe have access to Galton’s family height data through the HistData package.\nThis data contains heights on several dozen families: mothers, fathers, daughters, and sons."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-3",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-3",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nTo imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:\n\n\nlibrary(tidyverse) \nlibrary(HistData) \nset.seed(1983) \ngalton_heights &lt;- GaltonFamilies |&gt; \n  filter(gender == \"male\") |&gt; \n  group_by(family) |&gt; \n  sample_n(1) |&gt; \n  ungroup() |&gt; \n  select(father, childHeight) |&gt; \n  rename(son = childHeight)"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-4",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-4",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nSuppose we were asked to summarize the father and son data.\nSince both distributions are well approximated by the normal distribution, we could use:\n\n\ngalton_heights |&gt;  \n  summarize(mean(father), sd(father), mean(son), sd(son)) \n\n# A tibble: 1 × 4\n  `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1           69.1         2.55        69.2      2.71"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-5",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-5",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nThis summary fails to describe an important characteristic of the data:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-6",
    "href": "slides/linear-models/29-regression.html#case-study-is-height-hereditary-6",
    "title": "Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\n\nThe correlation coefficient is an informative summary of how two variables move together.\nIt’s defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as:\n\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\n\nwith \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\n\\(\\rho\\) is the greek letter for \\(r\\).\nIt is not a coincidence that \\(r\\) is the first letter in regression.\nWe learn about the connection between correlation and regression."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-1",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-1",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nWe can represent the formula above with R code using:\n\n\nrho &lt;- mean(scale(x) * scale(y))"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-2",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-2",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nTo understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average.\nSimilarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\).\nIf \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-3",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-3",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nThis correlation is the average and therefore unrelated variables will have 0 correlation.\nIf instead the quantities vary together, then we are averaging mostly positive products (\\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation.\nIf they vary in opposite directions, we get a negative correlation."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-4",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-4",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nThe correlation coefficient is always between -1 and 1.\nWe can show this mathematically."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-5",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-5",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nThe correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\n\ngalton_heights |&gt; summarize(r = cor(father, son)) |&gt; pull(r) \n\n[1] 0.4334102"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-correlation-coefficient-6",
    "href": "slides/linear-models/29-regression.html#the-correlation-coefficient-6",
    "title": "Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nCorrelations ranging from -0.9 to 0.99:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data science applications, we observe data that includes random variation.\nFor example, in many cases, we do not observe data for the entire population of interest, but rather for a random sample.\nAs with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-1",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-1",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nThis implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-2",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-2",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nA less fortunate geneticist can only afford measurements from a random sample of 25 pairs.\nThe sample correlation can be computed with:\n\n\nR &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt;  \n  summarize(r = cor(father, son)) |&gt; pull(r) \n\n\nR is a random variable."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-3",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-3",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nWe can run a Monte Carlo simulation to see its distribution:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-4",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-4",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nWe see that the expected value of R is the population correlation:\n\n\nmean(R) \n\n[1] 0.4307393\n\n\n\nand that it has a relatively high standard error relative to the range of values R can take:\n\n\nsd(R) \n\n[1] 0.1609393"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-5",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-5",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies.\nTherefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-6",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-a-random-variable-6",
    "title": "Regression",
    "section": "Sample correlation is a random variable",
    "text": "Sample correlation is a random variable\n\nThe standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary",
    "href": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary",
    "title": "Regression",
    "section": "Correlation is not always a useful summary",
    "text": "Correlation is not always a useful summary\n\nCorrelation is not always a good summary of the relationship between two variables.\nThe following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary-1",
    "href": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary-1",
    "title": "Regression",
    "section": "Correlation is not always a useful summary",
    "text": "Correlation is not always a useful summary\n\nAll these pairs have a correlation of 0.82:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary-2",
    "href": "slides/linear-models/29-regression.html#correlation-is-not-always-a-useful-summary-2",
    "title": "Regression",
    "section": "Correlation is not always a useful summary",
    "text": "Correlation is not always a useful summary\n\nCorrelation is only meaningful in a particular context.\nTo help us understand when correlation is meaningful as a summary statistic, we return to the example of predicting a son’s height using his father’s height."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations",
    "href": "slides/linear-models/29-regression.html#conditional-expectations",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height.\nBecause the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error.\nBut what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-1",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-1",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nIt turns out that, if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed.\nThis implies that the average of the distribution computed on this subset would be our best prediction.\nWe call this approach conditioning.\nThe general idea is that we stratify a population into groups and compute summaries in each group."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-2",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-2",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nTo provide a mathematical description of conditioning, consider that we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England.\nIn the previous chapter, we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mbox{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\).\nHowever, we are no longer interested in the general population."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-3",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-3",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nInstead, we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example.\nThis subset of the population is also a population, and thus, the same principles and properties we have learned apply.\nThe \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-4",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-4",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nIn our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches.\nThe statistical notation for the conditional expectation is:\n\n\\[\n\\mbox{E}(Y \\mid X = x)\n\\]\n\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-5",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-5",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nSimilarly, we denote the standard deviation of the strata with:\n\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\n\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-6",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-6",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity.\nThe conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall.\nWe want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-7",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-7",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nWe previously learned that the sample average is the preferred approach to estimating the population average.\nA challenge when using this approach is that we don’t have many data points matching exactly one value in our sample.\nFor example, we have only:\n\n\nsum(galton_heights$father == 72) \n\n[1] 8\n\n\n\nfathers that are exactly 72 inches."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-8",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-8",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nIf we change the number to 72.5, we get even fewer data points:\n\n\nsum(galton_heights$father == 72.5) \n\n[1] 1"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-9",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-9",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nA practical way to improve these estimates of the conditional expectations is to define strata of observations with similar value of \\(x\\).\nIn our example, we can round father heights to the nearest inch and assume that they are all 72 inches.\nIf we do this, we end up with the following prediction for the son of a father that is 72 inches tall:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-10",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-10",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nconditional_avg &lt;- galton_heights |&gt;  \n  filter(round(father) == 72) |&gt; \n  summarize(avg = mean(son)) |&gt;  \n  pull(avg) \nconditional_avg \n\n[1] 70.5\n\n\n\nNote that a 72 inch father is taller than average, specifically (72.0 - 69.1)/2.5 = 1.1 standard deviations taller than the average father."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-11",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-11",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nOur prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son.\nThe sons of 72 inch fathers have regressed some to the average height.\nWe notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. This is not a coincidence.\nIf we want to make a prediction of any height, not just 72 inches, we could apply the same approach to each strata."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-12",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-12",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nStratification followed by boxplots lets us see the distribution of each group:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-13",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-13",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nNot surprisingly, the centers of the groups are increasing with height.\nFurthermore, these centers appear to follow a linear relationship.\nBelow, we plot the averages of each group.\nIf we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-14",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-14",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line",
    "href": "slides/linear-models/29-regression.html#the-regression-line",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{Y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-1",
    "href": "slides/linear-models/29-regression.html#the-regression-line-1",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nThe formula for the regression is therefore:\n\n\\[\n\\left( \\frac{\\hat{Y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-2",
    "href": "slides/linear-models/29-regression.html#the-regression-line-2",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nWe can rewrite it like this:\n\n\\[\n\\hat{Y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\n\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs.\nIf there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\).\nFor values between 0 and 1, the prediction is somewhere in between.\nIf the correlation is negative, we predict a reduction instead of an increase."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-3",
    "href": "slides/linear-models/29-regression.html#the-regression-line-3",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s.\nThis is why we call it regression: the son regresses to the average height.\nIn fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-4",
    "href": "slides/linear-models/29-regression.html#the-regression-line-4",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nTo add regression lines to plots, we will need the above formula in the form:\n\n\\[\n\\hat{Y} = b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-5",
    "href": "slides/linear-models/29-regression.html#the-regression-line-5",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nHere we add the regression line to the original data:\n\n\nmu_x &lt;- mean(galton_heights$father) \nmu_y &lt;- mean(galton_heights$son) \ns_x &lt;- sd(galton_heights$father) \ns_y &lt;- sd(galton_heights$son) \nr &lt;- cor(galton_heights$father, galton_heights$son) \ngalton_heights |&gt;  \n  ggplot(aes(father, son)) +  \n  geom_point(alpha = 0.5) + \n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-6",
    "href": "slides/linear-models/29-regression.html#the-regression-line-6",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-7",
    "href": "slides/linear-models/29-regression.html#the-regression-line-7",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision-1",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision-1",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nB &lt;- 1000 \nN &lt;- 50 \nset.seed(1983) \nconditional_avg &lt;- replicate(B, { \n  dat &lt;- sample_n(galton_heights, N) \n  dat |&gt; filter(round(father) == 72) |&gt;  \n    summarize(avg = mean(son)) |&gt;  \n    pull(avg) \n  }) \nregression_prediction &lt;- replicate(B, { \n  dat &lt;- sample_n(galton_heights, N) \n  mu_x &lt;- mean(dat$father) \n  mu_y &lt;- mean(dat$son) \n  s_x &lt;- sd(dat$father) \n  s_y &lt;- sd(dat$son) \n  r &lt;- cor(dat$father, dat$son) \n  mu_y + r*(72 - mu_x)/s_x*s_y \n})"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision-2",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision-2",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nAlthough the expected value of these two random variables is about the same:\n\n\nmean(conditional_avg, na.rm = TRUE) \n\n[1] 70.49368\n\nmean(regression_prediction) \n\n[1] 70.50941\n\n\n\nThe standard error for the regression prediction is substantially smaller:\n\n\nsd(conditional_avg, na.rm = TRUE) \n\n[1] 0.9635814\n\nsd(regression_prediction) \n\n[1] 0.4520833"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision-3",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision-3",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nThe prediction obtained with the regression line is therefore much more stable than the prediction obtained using the conditional mean.\nThere is an intuitive reason for this."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision-4",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision-4",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nThe conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall.\nIn fact, in some of the permutations we have no data, which is why we use na.rm=TRUE.\nThe regression line is estimated using all the data.\nSo why not always use the regression line for prediction? Because it is not always appropriate."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#regression-improves-precision-5",
    "href": "slides/linear-models/29-regression.html#regression-improves-precision-5",
    "title": "Regression",
    "section": "Regression improves precision",
    "text": "Regression improves precision\n\nFor example, Anscombe provided cases for which the data does not have a linear relationship.\nSo are we justified in using the regression line to predict? Galton answered this in the positive for height data.\nThe justification, which we include in the next section, is somewhat more advanced than the rest of the chapter.\nCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nAnscombe’s examples provide over-simplified cases in which the correlation is not a useful summary.\nBut there are many real-life examples.\nThe main way we motivate appropriate use of correlation as a summary, involves the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-1",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-1",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nAs we saw in earlier they can be thin (high correlation) or circle-shaped (no correlation).\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-2",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-2",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nWhen three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-3",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-3",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-4",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-4",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-5",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-5",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-6",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-6",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nNow we come back to defining correlation.\nGalton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-7",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-7",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nWe don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\n\\[\n\\mbox{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#variance-explained",
    "href": "slides/linear-models/29-regression.html#variance-explained",
    "title": "Regression",
    "section": "Variance explained",
    "text": "Variance explained\n\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}  \n\\]\n\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons.\nBut once we condition, we are only looking at the variability of the sons with a tall, 72 inch father."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#variance-explained-1",
    "href": "slides/linear-models/29-regression.html#variance-explained-1",
    "title": "Regression",
    "section": "Variance explained",
    "text": "Variance explained\n\nThis group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#variance-explained-2",
    "href": "slides/linear-models/29-regression.html#variance-explained-2",
    "title": "Regression",
    "section": "Variance explained",
    "text": "Variance explained\n\nWe could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers.\nIn this case, this percent actually refers to the variance (the SD squared)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#variance-explained-3",
    "href": "slides/linear-models/29-regression.html#variance-explained-3",
    "title": "Regression",
    "section": "Variance explained",
    "text": "Variance explained\n\nSo if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#there-are-two-regression-lines",
    "href": "slides/linear-models/29-regression.html#there-are-two-regression-lines",
    "title": "Regression",
    "section": "There are two regression lines",
    "text": "There are two regression lines\n\nWe computed a regression line to predict the son’s height from father’s height.\nWe used these calculations:\n\n\nmu_x &lt;- mean(galton_heights$father) \nmu_y &lt;- mean(galton_heights$son) \ns_x &lt;- sd(galton_heights$father) \ns_y &lt;- sd(galton_heights$son) \nr &lt;- cor(galton_heights$father, galton_heights$son) \nm_1 &lt;-  r * s_y / s_x \nb_1 &lt;- mu_y - m_1*mu_x \n\n\nwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#there-are-two-regression-lines-1",
    "href": "slides/linear-models/29-regression.html#there-are-two-regression-lines-1",
    "title": "Regression",
    "section": "There are two regression lines",
    "text": "There are two regression lines\n\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\).\nSince the data is approximately bivariate normal, the theory described earlier tells us that this conditional expectation will follow a line with slope and intercept:\n\n\nm_2 &lt;-  r * s_x / s_y \nb_2 &lt;- mu_x - m_2 * mu_y"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#there-are-two-regression-lines-2",
    "href": "slides/linear-models/29-regression.html#there-are-two-regression-lines-2",
    "title": "Regression",
    "section": "There are two regression lines",
    "text": "There are two regression lines\n\nSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y.\nAgain, we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nNext we show plot showing the two regression lines, with blue for the predicting son heights with father heights, and red for predicting father heights with son heights:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models",
    "href": "slides/linear-models/29-regression.html#linear-models",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nWe are now ready to understand the connection between regression and linear models.\nWe have described how, if data is bivariate normal, then the conditional expectations follow the regression line.\nThe fact that the conditional expectation is a line is not an extra assumption, but rather a derived result."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-1",
    "href": "slides/linear-models/29-regression.html#linear-models-1",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nHowever, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nWe note that linear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-2",
    "href": "slides/linear-models/29-regression.html#linear-models-2",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nIn mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables.\nFor example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-3",
    "href": "slides/linear-models/29-regression.html#linear-models-3",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nWe can also add a constant so \\(2 + 3x - 4y + 5z\\) is also a linear combination of \\(x\\), \\(y\\), and \\(z\\).\nWe previously described how if \\(X\\) and \\(Y\\) are bivariate normal, then if we look at only the pairs with \\(X=x\\), then \\(Y \\mid X=x\\) follows a normal distribution with expected value \\(\\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\\), which is a linear function of \\(x\\), and standard deviation \\(\\sigma_Y \\sqrt{1-\\rho^2}\\) that does not depend on \\(x\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-4",
    "href": "slides/linear-models/29-regression.html#linear-models-4",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nNote that if we write:\n\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\n\nIf we assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us: it follows a normal distribution, the expected value is a linear function \\(x\\), and the standard deviation does not depend on \\(x\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-5",
    "href": "slides/linear-models/29-regression.html#linear-models-5",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nIf we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.  \n\\]\n\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-6",
    "href": "slides/linear-models/29-regression.html#linear-models-6",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nWe can further assume that \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\nIn the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\).\nWe estimate these from the data."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-7",
    "href": "slides/linear-models/29-regression.html#linear-models-7",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nOnce we do this, we can predict son’s heights for any father’s height \\(x\\).\nAlthough this model is exactly the same one we derived earlier by assuming bivariate normal data, a somewhat nuanced difference is that, in the first approach, we assumed the data was bivariate normal and the linear model was derived, not assumed."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-8",
    "href": "slides/linear-models/29-regression.html#linear-models-8",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nIn practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified.\nNevertheless, if your data is bivariate normal, the above linear model holds.\nIf your data is not bivariate normal, then you will need to have other ways of justifying the model."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-9",
    "href": "slides/linear-models/29-regression.html#linear-models-9",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nOne reason linear models are popular is that they are interpretable.\nIn the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\).\nBecause not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-10",
    "href": "slides/linear-models/29-regression.html#linear-models-10",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nThis remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable, as it is the predicted height of a son with a father with no height.\nDue to regression to the mean, the prediction will usually be a bit larger than 0."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-11",
    "href": "slides/linear-models/29-regression.html#linear-models-11",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nTo make the slope parameter more interpretable, we can rewrite the model slightly as:\n\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N  \n\\]\n\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\).\nIn this case, \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#linear-models-12",
    "href": "slides/linear-models/29-regression.html#linear-models-12",
    "title": "Regression",
    "section": "Linear models",
    "text": "Linear models\n\nLater, we will see how the linear model representation permits us to use the same mathematical frameworks in other contexts and to achieve more complicated goals than predict one variable from another."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nFor linear models to be useful, we have to estimate the unknown \\(\\beta\\)s.\nThe standard approach is to find the values that minimize the distance of the fitted model to the data.\nSpecifically, we find the \\(\\beta\\)s that minize the least squares (LS) equation show below."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-1",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-1",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nFor Galton’s data, the LS equation looks like this:\n\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2  \n\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-2",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-2",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nThe quantity we try to minimize is called the residual sum of squares (RSS).\nOnce we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them by placing a hat over the parameters."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-3",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-3",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nIn our example we use \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nWe will demonstrate how we find these values using the previously defined galton_heights dataset."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-4",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-4",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nLet’s start by writing a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\nrss &lt;- function(beta0, beta1, data){ \n  resid &lt;- galton_heights$son - (beta0 + beta1*galton_heights$father) \n  return(sum(resid^2)) \n} \n\n\nSo for any pair of values, we get an RSS."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-5",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-5",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nHere is a plot of the RSS as a function of \\(\\beta_1\\), when we keep the \\(\\beta_0\\) fixed at 25."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-6",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-6",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65.\nHowever, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked.\nWe don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-7",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-7",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nWe could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus.\nSpecifically, we take the partial derivatives, set them to 0, and solve for \\(\\beta_1\\) and \\(\\beta_2\\).\nOf course, if we have many parameters, these equations can get rather complex."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#least-squares-estimates-8",
    "href": "slides/linear-models/29-regression.html#least-squares-estimates-8",
    "title": "Regression",
    "section": "Least Squares Estimates",
    "text": "Least Squares Estimates\n\nThere are functions in R that do these calculations for us.\nTo learn the mathematics behind this, you can consult a book on linear models."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-lm-function",
    "href": "slides/linear-models/29-regression.html#the-lm-function",
    "title": "Regression",
    "section": "The lm function",
    "text": "The lm function\n\nIn R, we can obtain the least squares estimates using the lm function.\nTo fit the model:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\n\nwith \\(Y_i\\) being the son’s height and \\(x_i\\) being the father’s height, we can use this code to obtain the least squares estimates."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-lm-function-1",
    "href": "slides/linear-models/29-regression.html#the-lm-function-1",
    "title": "Regression",
    "section": "The lm function",
    "text": "The lm function\n\nfit &lt;- lm(son ~ father, data = galton_heights) \nfit$coefficients \n\n(Intercept)      father \n  37.287605    0.461392 \n\n\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~).\nThe intercept is added automatically to the model that will be fit."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-lm-function-2",
    "href": "slides/linear-models/29-regression.html#the-lm-function-2",
    "title": "Regression",
    "section": "The lm function",
    "text": "The lm function\n\nThe object fit includes more information.\nWe can use the function summary to extract more of this information:\n\n\nsummary(fit) \n\n\nCall:\nlm(formula = son ~ father, data = galton_heights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3543 -1.5657 -0.0078  1.7263  9.4150 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28761    4.98618   7.478 3.37e-12 ***\nfather       0.46139    0.07211   6.398 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 177 degrees of freedom\nMultiple R-squared:  0.1878,    Adjusted R-squared:  0.1833 \nF-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-lm-function-3",
    "href": "slides/linear-models/29-regression.html#the-lm-function-3",
    "title": "Regression",
    "section": "The lm function",
    "text": "The lm function\n\nTo understand some of the information included in this summary, we need to remember that the LSE are random variables.\nMathematical statistics gives us some ideas of the distribution of these random variables."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\).\nThis implies that our estimates are random variables."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-1",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-1",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nTo see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\n\n\nB &lt;- 1000 \nN &lt;- 50 \nlse &lt;- replicate(B, { \n  sample_n(galton_heights, N, replace = TRUE) |&gt;  \n    lm(son ~ father, data = _) |&gt;  \n    coef() \n}) \nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-2",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-2",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nWe can see the variability of the estimates by plotting their distributions:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-3",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-3",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nThe standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-4",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-4",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nThe function summary shows us the standard error estimates:\n\n\nsample_n(galton_heights, N, replace = TRUE) |&gt;  \n  lm(son ~ father, data = _) |&gt;  \n  summary() |&gt;  \n  coef() \n\n              Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 24.5722201 11.0734543 2.219020 0.0312467290\nfather       0.6479396  0.1602234 4.043975 0.0001896386"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-5",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-5",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nYou can see that the standard errors estimates reported above are close to the standard errors from the simulation:\n\n\nlse |&gt; summarize(se_0 = sd(beta_0), se_1 = sd(beta_1)) \n\n      se_0      se_1\n1 8.669471 0.1256726\n\n\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)).\nThe t-statistic is not actually based on the central limit theorem, but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-6",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-6",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nUnder this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model.\nIn our example \\(p=2\\), and the two p-values are obtained from testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described earlier, for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#lse-are-random-variables-7",
    "href": "slides/linear-models/29-regression.html#lse-are-random-variables-7",
    "title": "Regression",
    "section": "LSE are random variables",
    "text": "LSE are random variables\n\nAlso, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy.\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”.\nHowever, several assumptions have to hold for these statements to be true."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#predicted-values-are-random-variables",
    "href": "slides/linear-models/29-regression.html#predicted-values-are-random-variables",
    "title": "Regression",
    "section": "Predicted values are random variables",
    "text": "Predicted values are random variables\n\nOnce we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model.\nFor example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-1",
    "href": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-1",
    "title": "Regression",
    "section": "Predicted values are random variables",
    "text": "Predicted values are random variables\n\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-2",
    "href": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-2",
    "title": "Regression",
    "section": "Predicted values are random variables",
    "text": "Predicted values are random variables\n\nIf we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well.\nIn fact, the ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\n\ngalton_heights |&gt; ggplot(aes(son, father)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-3",
    "href": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-3",
    "title": "Regression",
    "section": "Predicted values are random variables",
    "text": "Predicted values are random variables"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nWhen the linear model is assumed, rather than derived, all interpretations depend on the usefulness of the model.\nThe lm function will fit the model and return summaries even when the model is wrong and not useful.\nVisually inspecting residuals, defined as the difference between observed values and predicted values:\n\n\\[\nr = Y - \\hat{Y} = Y - \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\]"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-1",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-1",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nand summaries of the residuals, is a powerful way to diagnose if the model is useful.\nNote that the residuals can be thought of estimates of the errors since:\n\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\]\n\nIn fact residuals are often denoted as \\(\\hat{\\varepsilon}\\).\nThis motivates several diagnostic plots."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-2",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-2",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nBecause we observe \\(r\\), but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{Y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribution, a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-3",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-3",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-4",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-4",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nWe prefer plots rather than summaries based on correlation because correlation is not always the best summary of association.\nThe function plot applied to an lm object automatically plots these."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-5",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-5",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-6",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-6",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nplot(fit, which = 1:3)"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-7",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-7",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see.\nYou can learn more by reading the plot.lm help file.\nHowever, some of the plots are based on more advanced concepts beyond the scope of this book.\nTo learn more, we recommend an advanced book on regression analysis.\nWe later introduce data analysis challenges in which we may decide to not to include certain variables in the model."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-8",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-8",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nIn these cases, an important diagnostic test to add checks if the residuals are related to variables not included in the model."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort.\n\nIt is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-1",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-1",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best.\nThe sophomore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year.\nFor example, this Fox Sports article[^linear-models-1] asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-2",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-2",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nhttps://web.archive.org/web20160815063904/http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715\nDoes the data confirm the existence of a sophomore slump? Let’s take a look.\nExamining the data for a widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-3",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-3",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nIn fact, the proportion of players that have a lower batting average during their sophomore year is 0.7090909.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all the players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-4",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-4",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not.\nThere is no such thing as a sophomore slump."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-5",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-5",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThis is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-6",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-6",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-7",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-7",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\n\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014.\nIt’s not a jinx; it’s just due to chance."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-8",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-8",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThe ROY are selected from the top values of \\(X\\), so it is expected that \\(Y\\) will regress to the mean."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-9",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-9",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\n\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014.\nIt’s not a jinx; it’s just due to chance."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-fallacy-10",
    "href": "slides/linear-models/29-regression.html#the-regression-fallacy-10",
    "title": "Regression",
    "section": "The regression fallacy",
    "text": "The regression fallacy\n\nThe ROY are selected from the top values of \\(X\\), so it is expected that \\(Y\\) will regress to the mean."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most applications, we observe data that includes random variation.\nFor example, in many cases, we do not observe data for the entire population of interest, but rather for a random sample.\nAs with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-1",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-1",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nThis implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-2",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-2",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nA less fortunate geneticist can only afford measurements from a random sample of 25 pairs.\nThe sample correlation can be computed with:\n\n\nr &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt;  \n  summarize(r = cor(father, son)) |&gt; pull(r) \n\n\nr is a random variable."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-3",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-3",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nWe can run a Monte Carlo simulation to see its distribution:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-4",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-4",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nWe see that the expected value of r is the population correlation:\n\n\nmean(r) \n\n[1] 0.4307393\n\n\n\nand that it has a relatively high standard error relative to the range of values r can take:\n\n\nsd(r) \n\n[1] 0.1609393"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-5",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-5",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies.\nTherefore, for large enough \\(N\\), the distribution of r is approximately normal with expected value \\(\\rho\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-6",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-6",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\nThe standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\)."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#sample-correlation-is-random-7",
    "href": "slides/linear-models/29-regression.html#sample-correlation-is-random-7",
    "title": "Regression",
    "section": "Sample correlation is random",
    "text": "Sample correlation is random\n\n\\(N=25\\) does not seem to be large enough:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-not-always-useful",
    "href": "slides/linear-models/29-regression.html#correlation-not-always-useful",
    "title": "Regression",
    "section": "Correlation not always useful",
    "text": "Correlation not always useful\n\nCorrelation is not always a good summary of the relationship between two variables.\nThe following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-not-always-useful-1",
    "href": "slides/linear-models/29-regression.html#correlation-not-always-useful-1",
    "title": "Regression",
    "section": "Correlation not always useful",
    "text": "Correlation not always useful\n\nAll these pairs have a correlation of 0.82:"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#correlation-not-always-useful-2",
    "href": "slides/linear-models/29-regression.html#correlation-not-always-useful-2",
    "title": "Regression",
    "section": "Correlation not always useful",
    "text": "Correlation not always useful\n\nCorrelation is only meaningful in a particular context.\nTo help us understand when correlation is meaningful as a summary statistic, we return to the example of predicting a son’s height using his father’s height."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#conditional-expectations-15",
    "href": "slides/linear-models/29-regression.html#conditional-expectations-15",
    "title": "Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nThe fact that these conditional averages follow a line is not a coincidence.\nThe line these averages follow is what we call the regression line.\nHowever, it is not always appropriate to estimate conditional expectations with the regression line, so we also describe Galton’s theoretical justification for using the regression line."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#the-regression-line-8",
    "href": "slides/linear-models/29-regression.html#the-regression-line-8",
    "title": "Regression",
    "section": "The regression line",
    "text": "The regression line\n\nYou can make same plot, but using standard units like this:\n\n\ngalton_heights |&gt;  \n  ggplot(aes(scale(father), scale(son))) +  \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = 0, slope = r)"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-8",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-8",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\nThis is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mbox{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\n\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability.\nTherefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#bivariate-normal-distribution-9",
    "href": "slides/linear-models/29-regression.html#bivariate-normal-distribution-9",
    "title": "Regression",
    "section": "Bivariate normal distribution",
    "text": "Bivariate normal distribution\n\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line."
  },
  {
    "objectID": "slides/linear-models/29-regression.html#there-are-two-regression-lines-3",
    "href": "slides/linear-models/29-regression.html#there-are-two-regression-lines-3",
    "title": "Regression",
    "section": "There are two regression lines",
    "text": "There are two regression lines"
  },
  {
    "objectID": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-4",
    "href": "slides/linear-models/29-regression.html#predicted-values-are-random-variables-4",
    "title": "Regression",
    "section": "Predicted values are random variables",
    "text": "Predicted values are random variables\n\nThe R function predict takes an lm object as input and returns the prediction.\nIf requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\n\nfit &lt;- galton_heights |&gt; lm(son ~ father, data = _)  \ny_hat &lt;- predict(fit, se.fit = TRUE) \nnames(y_hat) \n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\""
  },
  {
    "objectID": "slides/linear-models/29-regression.html#diagnostic-plots-9",
    "href": "slides/linear-models/29-regression.html#diagnostic-plots-9",
    "title": "Regression",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\nIn these cases, an important diagnostic test to add checks if the residuals are related to variables not included in the model."
  },
  {
    "objectID": "slides/linear-models/28-intro-to-linear-models.html#linear-models",
    "href": "slides/linear-models/28-intro-to-linear-models.html#linear-models",
    "title": "Introduction",
    "section": "Linear Models",
    "text": "Linear Models\n\nWe have focused mainly datasets consisting of a single variable.\nIn data analyses challenges, it is very common to be interested in the relationship between two or more variables.\nWe now introduce linear models, a general framework that unifies approaches used for analyzing association between variables."
  },
  {
    "objectID": "slides/linear-models/28-intro-to-linear-models.html#linear-models-1",
    "href": "slides/linear-models/28-intro-to-linear-models.html#linear-models-1",
    "title": "Introduction",
    "section": "Linear Models",
    "text": "Linear Models\nLinear models are useful for:\n\nsimple and multivariate regression,\nmeasurement error models,\ntreatment effect models, and\nassociation tests."
  },
  {
    "objectID": "psets/pset-08-linear-models.html",
    "href": "psets/pset-08-linear-models.html",
    "title": "Problem set 8",
    "section": "",
    "text": "Load the HistData package. Create a galton_height data with the father’s height and one randomly selected daughter from each family. Exclude families with no female children. Set the seed at 2007 and use the function sample_n to select the random child. You should end up with a heights dataset with two columns: father and daughter.\n\n\nlibrary(HistData)\nnames(GaltonFamilies)\nset.seed(2007)\nheights &lt;- GaltonFamilies |&gt; ## your code here\n\n\nEstimate the intercept and slope of the regression line for predicting daughter height \\(Y\\) using father height \\(X\\). Use the following regression line formula:\n\n\\[\n\\frac{\\hat{Y} - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x - \\mu_x}{\\sigma_x}\n\\]\n\n## your code here\n\n\nMake a plot to confirm the regression line goes through the data.\n\n\nheights |&gt; ggplot(aes(father, daughter)) + ## your code here\n\n\nRecompute the slope and intercept coefficients, this time using lm and confirm you get the same answer as with the formula used in problem 2.\n\n\n## your code here\n\n\nNote that the interpretation of the intercept is: the height prediction for the daughter whose father is 0 inches tall. This is not a very useful interpretation. Re-run the regression but instead of father height use inches above average for each father: instead of using the \\(x_i\\)s use \\(x_i - \\bar{x}\\). What is the interpretation of the intercept now? Does the slope estimate change?\n\n\n##your code here\n\n\nWhen using the centered father heights as a predictor, is the intercept the same as the average daughter height? Check if this is the case with the values you computed and then show that mathematically this has to be the case.\n\n\n##your code here\n\nFor the next exercises install the excessmort package. For the latest version use\n\nlibrary(devtools)\ninstall_github(\"rafalab/excessmort\")\n\n\nDefine an object counts by wrangling puerto_rico_counts to 1) include data only from 2002-2017 and counts for people 60 or over. We will focus in this older subset throughout the rest of the problem set.\n\n\nlibrary(excessmort) \n\n\nUse R to determine what day of the week María made landfall in PR (September 20, 2017).\n\n\n##your code here\n\n\nRedefine the date column to be the start of the week that date is part of: in other words, round the date down to the nearest week. Use the day of the week María made landfall as the first day. So, for example, 2017-09-20, 2017-09-21, 2017-09-22 should all be rounded down to 2017-09-20, while 2017-09-19 should be rounded down to 2017-09-13. Save the resulting table in weekly_counts.\n\n\n##your code here\n\n\nNow collapse the weekly_count data frame to store only one mortality value for each week, for each sex and agegroup. To this by by redefining outcome to have the total deaths that week for each sex and agegroup. Remove weeks that have less the 7 days of data. Finally, add a column with the MMWR week. Name the resulting data frame weekly_counts.\n\n\n##your code here\n\n\nComparing mortality totals is often unfair because the two groups begin compared have different population sizes. It is particularly important we consider rates rather than totals in this dataset because the demographics in Puerto Rico changed dramatically in the last 20 years. To see this use puerto_rico_counts to plot the population sizes by age group and gender. Provide a two sentence description of what you see.\n\n\npuerto_rico_counts |&gt; ## your code here\n\n\nMake a boxplot for each MMWR week’s mortality rate based on the 2002-2016 data. Each week has 15 data points, one for each year. Then add the 2017 data as red points.\n\n\n###your code here\n\n\nNote twp things: 1) there is a strong week effect and 2) 2017 is lower than expected. Plot the yearly rates (per 1,000) for 2002-2016:\n\n\nweekly_counts |&gt; \n  filter(year(date) &lt; 2017) |&gt;\n ## your code here\n\n\nThe plot made in 14 explains why 2017 is below what is expected: there appears to be a general decrease in mortality with time. A possible explanation is that medical care is improving and people are living more healthy lives.\n\nFit a linear model to the weekly data for the 65 and older to the 2002-2016 data that accounts for:\n\nA changing population.\nThe trend observed in 12.\nThe week effect.\nAge effect.\nA sex effect.\n\nUse rate as the outcome in the model.\n\n##your code here\n\n\nNow obtain expected counts for the entire dataset, including 2017. Compute the difference between the observed count and expected count and plot the total excess death for each week. Construct a confidence interval for the excess mortality estimate for each week. Hint: use the predict function.\n\n\n##your code here\n\n\nFinally, plot the observed rates and predicted rates from the model for each agegroup and sex. Comment on how well the model fits and what you might do differently.\n\n\n##your code here"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariate-regression",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariate-regression",
    "title": "Multivariate Regression",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nSince Galton’s original development, regression has become one of the most widely used tools in data analysis.\nOne reason is the fact that an adaptation of the original regression approach, based on linear models, permits us to find relationships between two variables taking into account the effects of other variables that affect both.\nThis has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariate-regression-1",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariate-regression-1",
    "title": "Multivariate Regression",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nWhen we are unable to randomly assign each individual to a treatment or control group, confounding becomes particularly prevalent.\nFor instance, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction.\nFast food consumers are more likely to be smokers, drinkers, and have lower incomes.\nConsequently, a naive regression model may lead to an overestimate of the negative health effects of fast food."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariate-regression-2",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariate-regression-2",
    "title": "Multivariate Regression",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nSo, how do we account for confounding in practice?\nToday we learn how multivariate regression can help with such situations and can be used to describe how one or more variables affect an outcome variable.\nWe illustrate with a real-world example in which data was used to help pick underappreciated players to improve a resource limited sports team."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#case-study-moneyball",
    "href": "slides/linear-models/30-multivariate-regression.html#case-study-moneyball",
    "title": "Multivariate Regression",
    "section": "Case study: Moneyball",
    "text": "Case study: Moneyball"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#moneyball",
    "href": "slides/linear-models/30-multivariate-regression.html#moneyball",
    "title": "Multivariate Regression",
    "section": "Moneyball",
    "text": "Moneyball"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#moneyball-1",
    "href": "slides/linear-models/30-multivariate-regression.html#moneyball-1",
    "title": "Multivariate Regression",
    "section": "Moneyball",
    "text": "Moneyball"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#case-study-moneyball-1",
    "href": "slides/linear-models/30-multivariate-regression.html#case-study-moneyball-1",
    "title": "Multivariate Regression",
    "section": "Case study: Moneyball",
    "text": "Case study: Moneyball"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#examples",
    "href": "slides/linear-models/30-multivariate-regression.html#examples",
    "title": "Multivariate Regression",
    "section": "Examples",
    "text": "Examples\n\nHere is a video showing a success: https://www.youtube.com/watch?v=HL-XjMCPfio.\nAnd here is one showing a failure: https://www.youtube.com/watch?v=NeloljCx-1g."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#baseball-basics",
    "href": "slides/linear-models/30-multivariate-regression.html#baseball-basics",
    "title": "Multivariate Regression",
    "section": "Baseball basics",
    "text": "Baseball basics"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#baseball-basics-1",
    "href": "slides/linear-models/30-multivariate-regression.html#baseball-basics-1",
    "title": "Multivariate Regression",
    "section": "Baseball basics",
    "text": "Baseball basics\n\nBases on balls (BB): The pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.\nSingle: The batter hits the ball and gets to first base.\nDouble (2B): The batter hits the ball and gets to second base.\nTriple (3B): The batter hits the ball and gets to third base.\nHome Run (HR): The batter hits the ball and goes all the way home and scores a run."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#baseball-basics-2",
    "href": "slides/linear-models/30-multivariate-regression.html#baseball-basics-2",
    "title": "Multivariate Regression",
    "section": "Baseball basics",
    "text": "Baseball basics\n\nHere is an example of a HR: https://www.youtube.com/watch?v=xYxSZJ9GZ-w."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#baseball-basics-3",
    "href": "slides/linear-models/30-multivariate-regression.html#baseball-basics-3",
    "title": "Multivariate Regression",
    "section": "Baseball basics",
    "text": "Baseball basics\n\nHere is an example of a stolen base: https://www.youtube.com/watch?v=JSE5kfxkzfk.\nAll these events are tracked throughout the season and are available to us through the Lahman package.\nNow, we can begin discussing how data analysis can help us determine how to use these statistics to evaluate players."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#baseball-basics-4",
    "href": "slides/linear-models/30-multivariate-regression.html#baseball-basics-4",
    "title": "Multivariate Regression",
    "section": "Baseball basics",
    "text": "Baseball basics\n\nHere is an example of a stolen base: https://www.youtube.com/watch?v=JSE5kfxkzfk.\nAll these events are tracked throughout the season and are available to us through the Lahman package.\nNow, we can begin discussing how data analysis can help us determine how to use these statistics to evaluate players."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#no-awards-for-base-on-balls",
    "href": "slides/linear-models/30-multivariate-regression.html#no-awards-for-base-on-balls",
    "title": "Multivariate Regression",
    "section": "No awards for base on balls",
    "text": "No awards for base on balls"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases",
    "href": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases",
    "title": "Multivariate Regression",
    "section": "Base on balls or stolen bases?",
    "text": "Base on balls or stolen bases?\n\nlibrary(tidyverse) \nlibrary(Lahman) \ndat &lt;- Teams |&gt; filter(yearID %in% 1962:2002) |&gt; \n  mutate(team = teamID, year = yearID, r = R/G,  \n         singles = (H - X2B - X3B - HR)/G,  \n         doubles = X2B/G, triples = X3B/G, hr = HR/G, \n         sb = SB/G, bb = BB/G) |&gt; \n  select(team, year, r, singles, doubles, triples, hr, sb, bb)"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-1",
    "href": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-1",
    "title": "Multivariate Regression",
    "section": "Base on balls or stolen bases?",
    "text": "Base on balls or stolen bases?\n\nNow let’s start with a obvious question: do teams that hit more home runs score more runs? When exploring the relationship between two variables, The visualization of choice is a scatterplot:"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-2",
    "href": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-2",
    "title": "Multivariate Regression",
    "section": "Base on balls or stolen bases?",
    "text": "Base on balls or stolen bases?"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-3",
    "href": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-3",
    "title": "Multivariate Regression",
    "section": "Base on balls or stolen bases?",
    "text": "Base on balls or stolen bases?"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-4",
    "href": "slides/linear-models/30-multivariate-regression.html#base-on-balls-or-stolen-bases-4",
    "title": "Multivariate Regression",
    "section": "Base on balls or stolen bases?",
    "text": "Base on balls or stolen bases?\n\nBut does this mean that increasing a team’s BBs causes an increase in runs?"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball",
    "href": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball",
    "title": "Multivariate Regression",
    "section": "Regression applied to baseball",
    "text": "Regression applied to baseball\n\nCan we use regression with these data?"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball-1",
    "href": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball-1",
    "title": "Multivariate Regression",
    "section": "Regression applied to baseball",
    "text": "Regression applied to baseball\n\nNow we are ready to use linear regression to predict the number of runs a team will score, if we know how many home runs the team hits using regression:\n\n\nhr_fit  &lt;- lm(r ~ hr, data = dat)$coef \np + geom_abline(intercept = hr_fit[[1]], slope = hr_fit[[2]])"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball-2",
    "href": "slides/linear-models/30-multivariate-regression.html#regression-applied-to-baseball-2",
    "title": "Multivariate Regression",
    "section": "Regression applied to baseball",
    "text": "Regression applied to baseball\nIn ggplot we can do this:\n\np + geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#the-broom-package",
    "href": "slides/linear-models/30-multivariate-regression.html#the-broom-package",
    "title": "Multivariate Regression",
    "section": "The broom package",
    "text": "The broom package\n\nlibrary(broom) \nfit &lt;- lm(r ~ bb, data = dat) \ntidy(fit, conf.int = TRUE) \n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.93     0.116       16.7 1.91e-55    1.70      2.15 \n2 bb             0.739    0.0348      21.2 1.90e-83    0.671     0.807\n\n\n\nThe other functions provided by broom, glance and augment, relate to model-specific and observation-specific outcomes, respectively."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nPreviously, we noted a strong relationship between Runs and BB.\nIf we find the regression line for predicting runs from bases on balls, we a get slope of:\n\n\nbb_slope &lt;- lm(r ~ bb, data = dat)$coef[2] \nbb_slope  \n\n       bb \n0.7388725"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-1",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-1",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nThe data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game.\nBut this does not mean that BB are the cause."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-2",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-2",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nNote that, if we compute the regression line slope for singles, we get:\n\n\nlm(r ~ singles, data = dat)$coef[2] \n\n  singles \n0.4324101"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-3",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-3",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nHere we show the correlation between HR, BB, and singles:\n\n\ndat |&gt; summarize(cor(bb, hr), cor(singles, hr), cor(bb, singles)) \n\n  cor(bb, hr) cor(singles, hr) cor(bb, singles)\n1   0.4064585       -0.1862848      -0.05126617"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-4",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-4",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nA first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs.\n\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt;  \n  filter(hr_strata &gt;= 0.4 & hr_strata &lt;= 1.2) |&gt; \n  ggplot(aes(bb, r)) +   \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(~hr_strata)"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#understanding-confounding-through-stratification",
    "href": "slides/linear-models/30-multivariate-regression.html#understanding-confounding-through-stratification",
    "title": "Multivariate Regression",
    "section": "Understanding confounding through stratification",
    "text": "Understanding confounding through stratification"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#understanding-confounding-through-stratification-1",
    "href": "slides/linear-models/30-multivariate-regression.html#understanding-confounding-through-stratification-1",
    "title": "Multivariate Regression",
    "section": "Understanding confounding through stratification",
    "text": "Understanding confounding through stratification\n\nRemember that the regression slope for predicting runs with BB was 0.7.\nOnce we stratify by HR, these slopes are substantially reduced:\n\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt;  \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;   \n  group_by(hr_strata) |&gt; \n  reframe(tidy(lm(r ~ bb))) |&gt; \n  filter(term == \"bb\") \n\n# A tibble: 8 × 6\n  hr_strata term  estimate std.error statistic  p.value\n      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1       0.5 bb       0.566    0.110       5.14 3.02e- 6\n2       0.6 bb       0.405    0.0984      4.12 7.46e- 5\n3       0.7 bb       0.284    0.0717      3.96 1.13e- 4\n4       0.8 bb       0.378    0.0638      5.92 1.75e- 8\n5       0.9 bb       0.254    0.0762      3.33 1.08e- 3\n6       1   bb       0.506    0.0720      7.02 9.46e-11\n7       1.1 bb       0.444    0.0878      5.06 2.77e- 6\n8       1.2 bb       0.469    0.0804      5.84 2.91e- 7"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#counfounding",
    "href": "slides/linear-models/30-multivariate-regression.html#counfounding",
    "title": "Multivariate Regression",
    "section": "Counfounding",
    "text": "Counfounding\n\ndat |&gt; mutate(bb_strata = round(bb, 1)) |&gt;  \n  filter(bb_strata &gt;= 3 & bb_strata &lt;= 4) |&gt;   \n  group_by(bb_strata) |&gt; \n  reframe(tidy(lm(r ~ hr))) |&gt; \n  filter(term == \"hr\") \n\n# A tibble: 11 × 6\n   bb_strata term  estimate std.error statistic  p.value\n       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1       3   hr        1.51     0.182      8.31 1.47e-12\n 2       3.1 hr        1.49     0.168      8.87 3.10e-14\n 3       3.2 hr        1.61     0.150     10.8  6.96e-18\n 4       3.3 hr        1.57     0.167      9.39 5.73e-15\n 5       3.4 hr        1.55     0.153     10.1  3.77e-16\n 6       3.5 hr        1.64     0.174      9.43 4.96e-14\n 7       3.6 hr        1.78     0.197      8.99 6.68e-13\n 8       3.7 hr        1.47     0.196      7.50 6.37e-10\n 9       3.8 hr        1.66     0.202      8.24 9.64e-11\n10       3.9 hr        1.29     0.289      4.45 1.15e- 4\n11       4   hr        1.36     0.218      6.27 4.38e- 7"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nIt is somewhat complex to be computing regression lines for each strata.\nWe are essentially fitting models like this:\n\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\n\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and vice versa.\nBut is there an easier approach?"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-1",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-1",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nIf we take random variability into account, the slopes in the strata don’t appear to change much.\nIf these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants.\nThis, in turn, implies that the expectation of runs conditioned on HR and BB can be written as follows:\n\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-2",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-2",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nThis model suggests that, if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\).\nOur exploratory data analysis suggested that this is the case.\nThe model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1\\).\nIn this analysis, referred to as multivariable regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-3",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-3",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nBecause the data is approximately normal and conditional distributions were also normal, we are justified in using a linear model:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\n\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) walks per game, and \\(x_{i,2}\\)."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-4",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-4",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nTo use lm here, we need to let the function know we have two predictor variables.\nWe therefore use the + symbol as follows:\n\n\ntidy(lm(r ~ bb + hr, data = dat), conf.int = TRUE)  \n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.74     0.0820      21.2 3.38e- 83    1.58      1.90 \n2 bb             0.387    0.0269      14.4 8.41e- 43    0.334     0.440\n3 hr             1.57     0.0488      32.1 1.39e-157    1.47      1.66"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-5",
    "href": "slides/linear-models/30-multivariate-regression.html#multivariable-regression-5",
    "title": "Multivariate Regression",
    "section": "Multivariable regression",
    "text": "Multivariable regression\n\nWhen we fit the model with only one variable, the estimated slopes were 0.74 and 1.85 for BB and HR, respectively.\nNote that when fitting the multivariable model both go down, with the BB effect decreasing much more."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i\n\\]\n\nwith \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\) representing BB, singles, doubles, triples, and HR respectively."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-1",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-1",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nUsing lm, we can quickly find the LSE for the parameters using:\n\n\nfit &lt;- dat |&gt;  filter(year &lt;= 2001) |&gt;  \n  lm(r ~ bb + singles + doubles + triples + hr, data = _)"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-2",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-2",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nWe can see the coefficients using tidy:\n\n\ntidy(fit, conf.int = TRUE) |&gt; filter(term != \"(Intercept)\") \n\n# A tibble: 5 × 7\n  term    estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 bb         0.370    0.0119      31.2 1.00e-149    0.347     0.393\n2 singles    0.517    0.0128      40.5 5.29e-213    0.492     0.543\n3 doubles    0.775    0.0229      33.8 7.09e-168    0.730     0.820\n4 triples    1.24     0.0778      15.9 4.62e- 51    1.09      1.39 \n5 hr         1.44     0.0248      58.1 1.98e-323    1.39      1.49"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-3",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-3",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nTo see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot:\n\n\ndat |&gt; mutate(r_hat = predict(fit, newdata = dat)) |&gt; \n  filter(year == 2002) %&gt;% \n  ggplot(aes(r_hat, r, label = team)) +  \n  geom_point() + \n  geom_text(nudge_x = 0.1, cex = 2) +  \n  geom_abline()"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-4",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-4",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-5",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-5",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nThe formula would look like this: -2.76 + 0.37 \\(\\times\\) BB + 0.52 \\(\\times\\) singles + 0.78 \\(\\times\\) doubles + 1.24 \\(\\times\\) triples + 1.44 \\(\\times\\) HR.\nTo define a player-specific metric, we have a bit more work to do."
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-6",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-6",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\npa_per_game &lt;- Batting |&gt; filter(yearID == 2002) |&gt;  \n  group_by(teamID) |&gt; \n  summarize(pa_per_game = sum(AB + BB)/162) |&gt;  \n  pull(pa_per_game) |&gt;  \n  mean()"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-7",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-7",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nplayers &lt;- Batting |&gt;  \n  filter(yearID %in% 1997:2001) |&gt;  \n  group_by(playerID) |&gt; \n  mutate(pa = BB + AB) |&gt; \n  summarize(g = sum(pa)/pa_per_game, \n    bb = sum(BB)/g, \n    singles = sum(H - X2B - X3B - HR)/g, \n    doubles = sum(X2B)/g,  \n    triples = sum(X3B)/g,  \n    hr = sum(HR)/g, \n    avg = sum(H)/sum(AB), \n    pa = sum(pa)) |&gt; \n  filter(pa &gt;= 1000) |&gt; \n  select(-g) \nplayers$r_hat = predict(fit, newdata = players)"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-8",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-8",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nhist(players$r_hat, main = \"Predicted runs per game\")"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-9",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-9",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-10",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-10",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nplayers &lt;- Salaries |&gt;  \n  filter(yearID == 2002) |&gt; \n  select(playerID, salary) |&gt; \n  right_join(players, by = \"playerID\")"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-11",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-11",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nposition_names &lt;-  \n  paste0(\"G_\", c(\"p\",\"c\",\"1b\",\"2b\",\"3b\",\"ss\",\"lf\",\"cf\",\"rf\", \"dh\")) \ntmp &lt;- Appearances |&gt;  \n  filter(yearID == 2002) |&gt;  \n  group_by(playerID) |&gt; \n  summarize_at(position_names, sum) |&gt; \n  ungroup() \npos &lt;- tmp |&gt; \n  select(all_of(position_names)) |&gt; \n  apply(X = _, 1, which.max)  \nplayers &lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) |&gt; \n  mutate(POS = str_to_upper(str_remove(POS, \"G_\"))) |&gt; \n  filter(POS != \"P\") |&gt; \n  right_join(players, by = \"playerID\") |&gt; \n  filter(!is.na(POS)  & !is.na(salary))"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-12",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-12",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nplayers &lt;- People |&gt; \n  select(playerID, nameFirst, nameLast, debut) |&gt; \n  mutate(debut = as.Date(debut)) |&gt; \n  right_join(players, by = \"playerID\")"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-13",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-13",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\nIf you are a baseball fan, you will recognize the top 10 players:\n\n\nplayers |&gt; select(nameFirst, nameLast, POS, salary, r_hat) |&gt; arrange(desc(r_hat)) |&gt; head(10)  \n\n   nameFirst nameLast POS   salary    r_hat\n1      Barry    Bonds  LF 15000000 8.052460\n2      Larry   Walker  RF 12666667 7.960583\n3       Todd   Helton  1B  5000000 7.403074\n4      Manny  Ramirez  LF 15462727 7.352475\n5      Sammy     Sosa  RF 15000000 7.201670\n6       Jeff  Bagwell  1B 11000000 7.053805\n7       Mike   Piazza   C 10571429 6.993616\n8      Jason   Giambi  1B 10428571 6.916405\n9      Edgar Martinez  DH  7086668 6.912145\n10       Jim    Thome  1B  8000000 6.885270"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-14",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-14",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\n\n\n\n\nnameFirst\nnameLast\nPOS\nsalary\nr_hat\n\n\n\n\nTodd\nHelton\n1B\n5000000\n7.403073\n\n\nMike\nPiazza\nC\n10571429\n6.993616\n\n\nEdgar\nMartinez\nDH\n7086668\n6.912145\n\n\nJim\nEdmonds\nCF\n7333333\n6.231373\n\n\nJeff\nKent\n2B\n6000000\n6.079064\n\n\nPhil\nNevin\n3B\n2600000\n5.857409\n\n\nMatt\nStairs\nRF\n500000\n5.758631\n\n\nHenry\nRodriguez\nLF\n300000\n5.640563\n\n\nJohn\nValentin\nSS\n550000\n5.000417"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-15",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-15",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\n\n\n\n\nnameLast\nbb\nsingles\ndoubles\ntriples\nhr\navg\nr_hat\n\n\n\n\nHelton\n0.91\n-0.21\n2.65\n-0.31\n1.52\n2.67\n2.54\n\n\nPiazza\n0.33\n0.42\n0.20\n-1.42\n1.83\n2.20\n2.09\n\n\nMartinez\n2.14\n-0.01\n1.26\n-1.22\n0.81\n2.20\n2.00\n\n\nEdmonds\n1.07\n-0.56\n0.79\n-1.15\n0.97\n0.85\n1.26\n\n\nKent\n0.23\n-0.73\n2.01\n0.45\n0.77\n0.79\n1.09\n\n\nNevin\n0.31\n-0.91\n0.48\n-1.19\n1.19\n0.10\n0.85\n\n\nStairs\n1.10\n-1.51\n-0.05\n-1.13\n1.12\n-0.56\n0.74\n\n\nRodriguez\n0.20\n-1.60\n0.33\n-0.78\n1.32\n-0.67\n0.61\n\n\nValentin\n0.18\n-0.93\n1.79\n-0.43\n-0.05\n-0.47\n-0.09"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-16",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-16",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team\n\n\n\n\n\nnameFirst\nnameLast\nPOS\nsalary\nr_hat\n\n\n\n\nTodd\nHelton\n1B\n5000000\n7.403073\n\n\nMike\nPiazza\nC\n10571429\n6.993616\n\n\nEdgar\nMartinez\nDH\n7086668\n6.912145\n\n\nJim\nEdmonds\nCF\n7333333\n6.231373\n\n\nJeff\nKent\n2B\n6000000\n6.079064\n\n\nPhil\nNevin\n3B\n2600000\n5.857409\n\n\nMatt\nStairs\nRF\n500000\n5.758631\n\n\nHenry\nRodriguez\nLF\n300000\n5.640563\n\n\nJohn\nValentin\nSS\n550000\n5.000417"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-17",
    "href": "slides/linear-models/30-multivariate-regression.html#building-a-baseball-team-17",
    "title": "Multivariate Regression",
    "section": "Building a baseball team",
    "text": "Building a baseball team"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-5",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-5",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "slides/linear-models/30-multivariate-regression.html#confounding-6",
    "href": "slides/linear-models/30-multivariate-regression.html#confounding-6",
    "title": "Multivariate Regression",
    "section": "Confounding",
    "text": "Confounding\n\nOnce we stratify by HR, these slopes are substantially reduced:\n\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt;  \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;   \n  group_by(hr_strata) |&gt; \n  reframe(tidy(lm(r ~ bb))) |&gt; \n  filter(term == \"bb\") \n\n# A tibble: 8 × 6\n  hr_strata term  estimate std.error statistic  p.value\n      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1       0.5 bb       0.566    0.110       5.14 3.02e- 6\n2       0.6 bb       0.405    0.0984      4.12 7.46e- 5\n3       0.7 bb       0.284    0.0717      3.96 1.13e- 4\n4       0.8 bb       0.378    0.0638      5.92 1.75e- 8\n5       0.9 bb       0.254    0.0762      3.33 1.08e- 3\n6       1   bb       0.506    0.0720      7.02 9.46e-11\n7       1.1 bb       0.444    0.0878      5.06 2.77e- 6\n8       1.2 bb       0.469    0.0804      5.84 2.91e- 7"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nUp to now, all our linear models have been applied to two or more continuous random variables.\nWe assume the random variables are multivariate normal and use this to motivate a linear model.\nThis approach covers many real-life examples of linear regression."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-1",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nLinear models have many other applications.\nOne of the most popular is to quantify treatment effects in randomized and controlled experiments.\nOne of the first applications was in agriculture, where different plots of lands were treated with different combinations of fertilizers.\nThe use of \\(Y\\) for the outcome in statistics is due to the mathematical theory being developed for crop yield as the outcome."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-2",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nThe same ideas have been applied in other areas.\nIn randomized trials developed to determine if drugs cure or prevent diseases or if policies have an effect on social or educational outcomes.\nWe think of the policy intervention as a treatment so we can follow the same mathematical descriptions.\nThe analyses used in A/B testing are based on treatment effects models."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-3",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nThese models have also been applied in observational studies as well.\nWe use linear models to estimate effects of interest while accounting for potential confounders.\nFor example, to estimate the effect of a diet high in fruits and vegetables on blood pressure, we would have to account for factors such as age, sex, and smoking status."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-4",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nA boxplot shows that the high fat diet mice are, on average, heavier."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-5",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-5",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nHowever, given that we divided the mice randomly, is it possible that the observed difference is simply due to chance? Here, we can compute the sample average and standard deviation of each group and perform statistical inference on the difference of these means."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-6",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-6",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nHowever, given that we divided the mice randomly, is it possible that the observed difference is simply due to chance? Here, we can compute the sample average and standard deviation of each group and perform statistical inference on the difference of these means."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nIs it possible that the observed difference is simply due to chance?\nWe can apply the ideas we learned in the statistical inference lectures.\nDefine \\(\\mu_1\\) and \\(\\mu_0\\) the population averagees for treatment and control.\nWe want to know if \\(\\mu_1 - \\mu_0 &gt;0\\)."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-1",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\nWe know that the \\(\\bar{X}_1 - \\bar{X}_0\\)\n\nmice_weights |&gt; group_by(diet) |&gt; summarize(average = mean(body_weight)) \n\n# A tibble: 2 × 2\n  diet  average\n  &lt;fct&gt;   &lt;dbl&gt;\n1 chow     31.5\n2 hf       36.7\n\n\nfollows a normal distribution, with expected value \\(\\mu_1-\\mu_0\\) and standard error:\n\\[\n\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}\n\\]"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-2",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nIf we define the null hypothesis as the high-fat diet having no effect, or \\(\\mu_1 - \\mu_0 = 0\\), this implies that.\n\n\\[\n\\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}}\n\\]\n\nhas expected value 0 and standard error 1 and therefore approximately follows a standard normal distribution."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-3",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nNote that we can’t compute this quantity in practice because the \\(\\sigma_1\\) and \\(\\sigma_0\\) are unknown.\nHowever, if we estimate them with the sample standard deviations the CLT still holds and:\n\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}}\n\\]\n\nfollows a standard normal distribution when the null hypothesis is true."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-4",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nThis implies that we can easily compute the probability of observing a value as large as the one we obtained:\n\n\nstats &lt;- mice_weights |&gt;  \n  group_by(diet) |&gt;  \n  summarize(xbar = mean(body_weight), s = sd(body_weight), n = n())  \nt_stat &lt;- with(stats, (xbar[2] - xbar[1])/sqrt(s[2]^2/n[2] + s[1]^2/n[1])) \nt_stat \n\n[1] 9.339096\n\n\n\nHere \\(t\\) is well over 3, so we don’t really need to compute the p-value 1-pnorm(t_stat) as we know it will be very small."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-5",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-5",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nNote that when \\(N\\) is not large enough, then the CLT does not apply and we use the t-distribution:\nSo the calculation of the p-value is the same except that we use pt instead of pnorm.\nSpecifically, we use\n\n\n1 - pt(t_stat, with(stats, n[2]+n[1]-2))\n\n[1] 0"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-6",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-6",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nNote that when \\(N\\) is not large enough, then the CLT does not apply and we use the t-distribution:\nSo the calculation of the p-value is the same except that we use pt instead of pnorm.\nSpecifically, we use\n\n\n1 - pt(t_stat, with(stats, n[2]+n[1]-2))\n\n[1] 0"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-7",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-7",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nDifferences in means are commonly examined in the scientific studies.\nAs a result this t-statistic is one of the most widely reported summaries.\nWhen used to determine if an observed difference is statistically significant, we refer to the procedure as “performing a t test”.\n:::{.callout-warning}.\nIn the computation above, we computed the probability of t being as large as what we observed."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nAlthough the t-test is useful for cases in which we compare two treatments, it is common to have other variables affect our outcomes.\nFor example, what if more male mice received the high-fat diet?\nWhat if their is cage effect?\nLinear models permit hypothesis testing in these more general situations."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-1",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nIf we assume that the weight distributions for both chow and high-fat diets are normally distributed, we can write:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\n\nwith \\(X_i\\) 1, if the \\(i\\)-th mice was fed the high-fat diet, and 0 otherwise, and the errors \\(\\varepsilon_i\\) IID normal with expected value 0 and standard deviation \\(\\sigma\\)."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-2",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nNote that this mathematical formula looks exactly like the model we wrote out for the father-son heights.\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\n\n\\(x_i\\) being 0 or 1 rather than a continuous variable, allows us to use it in this different context.\nNow \\(\\beta_0\\) represents the population average height of the mice on the chow diet and\n\\(\\beta_0 + \\beta_1\\) represents the population average for the weight of the mice on the high-fat diet."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-3",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nA nice feature of this model is that \\(\\beta_1\\) represents the treatment effect of receiving the high-fat diet.\nThe null hypothesis that the high-fat diet has no effect can be quantified as \\(\\beta_1 = 0\\)."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-4",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nA powerful characteristic of linear models is that we can estimate the \\(\\beta\\)s and their standard errors with the same LSE machinery:\n\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-5",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-5",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nBecause diet is a factor with two entries, the lm function knows to fit the linear model above with a \\(x_i\\) a indicator variable.\nThe summary function shows us the resulting estimate, standard error, and p-value:\n\n\ncoefficients(summary(fit)) \n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 31.537005  0.3858192 81.740369 0.000000e+00\ndiethf       5.136078  0.5484506  9.364705 8.021959e-20"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-6",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-6",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nUsing broom, we can write:\n\n\nlibrary(broom) \ntidy(fit, conf.int = TRUE) |&gt; filter(term == \"diethf\") \n\n# A tibble: 1 × 7\n  term   estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 diethf     5.14     0.548      9.36 8.02e-20     4.06      6.21"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-7",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-7",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nThe statistic computed here is the estimate divided by its standard error:\n\n\\[\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1)\\]\n\nIn the case of the simple one-factor model, we can show that this statistic is almost equivalent to the t-statistics computed in the previous section:\n\n\nc(coefficients(summary(fit))[2,3], t_stat) \n\n[1] 9.364705 9.339096"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-8",
    "href": "slides/linear-models/31-treatment-effect-models.html#one-factor-design-8",
    "title": "Treatment Effect Models",
    "section": "One factor design",
    "text": "One factor design\n\nThe statistic computed here is the estimate divided by its standard error:\n\n\\[\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1)\\]\n\nIn the case of the simple one-factor model, we can show that this statistic is almost equivalent to the t-statistics computed in the previous section:\n\n\nc(coefficients(summary(fit))[2,3], t_stat) \n\n[1] 9.364705 9.339096"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nNote that this experiment included male and female mice, and male mice are known to be heavier.\nThis explains why the residuals depend on the sex variable:\n\n\nboxplot(fit$residuals ~ mice_weights$sex)"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-1",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-2",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nThis misspecification can have real implications; for instance, if more male mice received the high-fat diet, then this could explain the increase.\nConversely, if fewer received it, we might underestimate the diet effect.\nSex could be a confounder, indicating that our model can certainly be improved."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-3",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nWe can examine the data:\n\n\nmice_weights |&gt; ggplot(aes(diet, log2(body_weight), fill = sex)) + geom_boxplot()"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-4",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-5",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-5",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nA linear model that permits a different expected value for the following four groups, 1) female on chow diet, 2) females on high-fat diet, 3) male on chow diet, and 4) males on high-fat diet, can be written like this:\n\n\\[\nY_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}  + \\beta_3 x_{i,3}  + \\beta_4 x_{i,4}  + \\varepsilon_i\n\\]\n\nwith \\(x_{i,1},\\dots,x_{i,4}\\) indicator variables for each of the four groups."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-6",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-6",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nNote that with this representation we allow the diet effect to be different for males and females.\nHowever, with this representation, none of the \\(\\beta\\)s represent the effect of interest: the diet effect.\nA powerful feature of linear models is that we can rewrite the model so that the expected value for each group remains the same, but the parameters represent the effects we are interested in."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-7",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-7",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nSo, for example, in the representation.\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}  + \\beta_3 x_{i,1} x_{i,2}  + \\varepsilon_i\n\\]\n\nwith \\(x_{i,1}\\) an indicator that is 1 if individual \\(i\\) is on the high-fat diet \\(x_{i,2}\\) an indicator that is 1 if you are male, the \\(\\beta_1\\) is interpreted as the diet effect for females, \\(\\beta_2\\) as the average difference between males and females, and \\(\\beta_3\\) the difference in the diet effect between males and females."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-8",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-8",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nIn statistics, \\(\\beta_3\\) is referred to as an interaction effect.\nThe \\(\\beta_0\\) is considered the baseline value, which is the average weight of females on the chow diet."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-9",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-9",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nStatistical textbooks describe several other ways in which the model can be rewritten to obtain other types of interpretations.\nFor example, we might want \\(\\beta_2\\) to represent the overall diet effect (the average between female and male effect) rather than the diet effect on females."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-10",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-10",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nThis is achieved by defining what contrasts we are interested in.\nIn R, we can specific the linear model above using the following:\n\n\nfit &lt;- lm(body_weight ~ diet*sex, data = mice_weights) \n\n\nThe * implies that the term that multiplies \\(x_{i,1}\\) and \\(x_{i,2}\\) should be included, along with the \\(x_{i,1}\\) and \\(x_{i,2}\\) terms."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-11",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-11",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nHere are the estimates:\n\n\ntidy(fit, conf.int = TRUE) |&gt; filter(!str_detect(term, \"Intercept\")) \n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 diethf          3.88     0.624      6.22 8.02e-10    2.66       5.10\n2 sexM            7.53     0.627     12.0  1.27e-30    6.30       8.76\n3 diethf:sexM     2.66     0.891      2.99 2.91e- 3    0.912      4.41\n\n\n\nNote that the male effect is larger that the diet effect, and the diet effect is statistically significant for both sexes, with diet affecting males more by between 1 and 4.5 grams."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-12",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-12",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nA common approach applied when more than one factor is thought to affect the measurement is to simply include an additive effect for each factor, like this:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}   + \\varepsilon_i\n\\]\n\nIn this model, the \\(\\beta_1\\) is a general diet effect that applies regardless of sex."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-13",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-13",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nIn R, we use the following code, employing a + instead of *:\n\n\nfit &lt;- lm(body_weight ~ diet + sex, data = mice_weights) \n\n\nNote that this model does not account for the difference in diet effect between males and females."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-14",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-14",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nDiagnostic plots would reveal this deficiency by showing that the residuals are biased: they are, on average, negative for females on the diet and positive for males on the diet, rather than being centered around 0.\n\n\nplot(fit, which = 1)"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nIn the examples we have examined, each treatment had only two groups: diet had chow/high-fat, and sex had female/male.\nHowever, variables of interest often have more than one level.\nFor example, we might have tested a third diet on the mice.\nIn statistics textbooks, these variables are referred to as a factor, and the groups in each factor are called its levels."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-1",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nWhen a factor is included in the formula, the default behavior for lm is to define the intercept term as the expected value for the first level, and the other coefficient are to represent the difference, or contrast, between the other levels and first.\nWe can see when we estimate the sex effect with lm like this:\n\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights) \ncoefficients(fit) \n\n(Intercept)        sexM \n  29.758844    8.820658"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-2",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nTo recover the expected mean for males, we can simply add the two coefficients:\n\n\nsum(fit$coefficients[1:2]) \n\n[1] 38.5795\n\n\n\nThe package emmeans simplifies the calculation and also calculates standard errors:\n\n\nlibrary(emmeans) \nemmeans(fit, ~sex) \n\n sex emmean    SE  df lower.CL upper.CL\n F     29.8 0.339 778     29.1     30.4\n M     38.6 0.346 778     37.9     39.3\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-3",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nNow, what if we really didn’t want to define a reference level? What if we wanted a parameter to represent the difference from each group to the overall mean? Can we write a model like this:\n\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\n\nwith \\(x_{i,1} = 1\\), if observation \\(i\\) is female and 0 otherwise, and \\(x_{i,2}=1\\), if observation \\(i\\) is male and 0 otherwise?"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-4",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nUnfortunately, this representation has a problem.\nNote that the mean for females and males are represented by \\(\\beta_0 + \\beta_1\\) and \\(\\beta_0 + \\beta_2\\), respectively.\nThis is a problem because the expected value for each group is just one number, say \\(\\mu_f\\) and \\(\\mu_m\\), and there is an infinite number of ways \\(\\beta_0 + \\beta_1 = \\mu_f\\) and \\(\\beta_0 +\\beta_2 = \\mu_m\\) (three unknowns with two equations).\nThis implies that we can’t obtain a unique least squares estimates.\nThe model, or parameters, are unidentifiable."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-5",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-5",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nThe default behavior in R solves this problem by requiring \\(\\beta_1 = 0\\), forcing \\(\\beta_0 = \\mu_m\\), which permits us to solve the system of equations.\nKeep in mind that this is not the only constraint that permits estimation of the parameters."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-6",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-6",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nAny linear constraint will do as it adds a third equation to our system.\nA widely used constraint is to require \\(\\beta_1 + \\beta_2 = 0\\).\nTo achieve this in R, we can use the argument contrast in the following way:\n\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights,  \n          contrasts = list(sex = contr.sum)) \ncoefficients(fit) \n\n(Intercept)        sex1 \n  34.169173   -4.410329"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-7",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-7",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nWe see that the intercept is now larger, reflecting the overall mean rather than just the mean for females.\nThe other coefficient, \\(\\beta_1\\), represents the contrast between females and the overall mean in our model.\nThe coefficient for men is not shown because it is redundant: \\(\\beta_1= -\\beta_2\\)."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-8",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-8",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nIf we want to see all the estimates, the emmeans package also makes the calculations for us:\n\n\ncontrast(emmeans(fit, ~sex)) \n\n contrast estimate    SE  df t.ratio p.value\n F effect    -4.41 0.242 778 -18.203  &lt;.0001\n M effect     4.41 0.242 778  18.203  &lt;.0001\n\nP value adjustment: fdr method for 2 tests"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-9",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-9",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nThe use of this alternative constraint is more practical when a factor has more than one level, and choosing a baseline becomes less convenient.\nFurthermore, we might be more interested in the variance of the coefficients rather than the contrasts between groups and the reference level."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova",
    "href": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova",
    "title": "Treatment Effect Models",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nWhen a factor has more than one level, it is common to want to determine if there is significant variability across the levels rather than specific difference between any given pair of levels.\nAnalysis of variances (ANOVA) provides tools to do this.\nANOVA provides an estimate of \\(\\sigma^2_{\\text{gen}}\\) and a statistical test for the null hypothesis that the factor contributes no variability: \\(\\sigma^2_{\\text{gen}} =0\\).\nOnce a linear model is fit using one or more factors, the aov function can be used to perform ANOVA."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-1",
    "title": "Treatment Effect Models",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nSpecifically, the estimate of the factor variability is computed along with a statistic that can be used for hypothesis testing:\n\n\nsummary(aov(fit)) \n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\ngen           4    294   73.50   1.128  0.342\nResiduals   775  50479   65.13"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-2",
    "title": "Treatment Effect Models",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nKeep in mind that if given a model formula, aov will fit the model:\n\n\nsummary(aov(body_weight ~ gen, data = mice_weights)) \n\n\nWe do not need to specify the constraint because ANOVA needs to constrain the sum to be 0 for the results to be interpretable.\nThis analysis indicates that generation is not statistically significant."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#analysis-of-variance-anova-3",
    "title": "Treatment Effect Models",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nThose interested in learning more about these topics can consult one of these textbooks.\n:::"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#multiple-factors",
    "href": "slides/linear-models/31-treatment-effect-models.html#multiple-factors",
    "title": "Treatment Effect Models",
    "section": "Multiple factors",
    "text": "Multiple factors\n\nANOVA was developed to analyze agricultural data, which included several factors.\nWe can perform ANOVA with multiple factors:\n\n\nsummary(aov(body_weight ~ sex + diet + gen,  data = mice_weights)) \n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nsex           1  15165   15165 389.799 &lt;2e-16 ***\ndiet          1   5238    5238 134.642 &lt;2e-16 ***\ngen           4    295      74   1.895  0.109    \nResiduals   773  30074      39                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThis analysis suggests that sex is the biggest source of variability, which is consistent with previously made exploratory plots."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#multiple-factors-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#multiple-factors-1",
    "title": "Treatment Effect Models",
    "section": "Multiple factors",
    "text": "Multiple factors\n\nOne of the key aspects of ANOVA (Analysis of Variance) is its ability to decompose the total variance in the data, represented by \\(\\sum_{i=1}^n Y_i^2\\), into individual contributions attributable to each factor in the study.\nHowever, for the mathematical underpinnings of ANOVA to be valid, the experimental design must be balanced.\nThis means that for every level of any given factor, there must be an equal representation of the levels of all other factors."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#multiple-factors-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#multiple-factors-2",
    "title": "Treatment Effect Models",
    "section": "Multiple factors",
    "text": "Multiple factors\n\nIn our study involving mice, the design is unbalanced, requiring a cautious approach in the interpretation of the ANOVA results.\n:::"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#array-representation",
    "href": "slides/linear-models/31-treatment-effect-models.html#array-representation",
    "title": "Treatment Effect Models",
    "section": "Array representation",
    "text": "Array representation\n\nWhen the model includes more than one factor, writing down linear models can become cumbersome."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#array-representation-1",
    "href": "slides/linear-models/31-treatment-effect-models.html#array-representation-1",
    "title": "Treatment Effect Models",
    "section": "Array representation",
    "text": "Array representation\n\nFor example, in our two factor model, we include indicator variables for both factors:\n\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\sum_{k=1}^K \\beta_{J+k} x_{i,J+k} + \\varepsilon_i \\\\\n\\mbox{ with }\\sum_{j=1}^J \\beta_j=0 \\mbox{ and } \\sum_{k=1}^K \\beta_{J+k} = 0,\n\\]\n\nthe \\(x\\)s are indicator functions for the different levels."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#array-representation-2",
    "href": "slides/linear-models/31-treatment-effect-models.html#array-representation-2",
    "title": "Treatment Effect Models",
    "section": "Array representation",
    "text": "Array representation\nSpecifically, in\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\sum_{k=1}^K \\beta_{J+k} x_{i,J+k} + \\varepsilon_i\n\\]\nthe \\(x_{i,1},\\dots,x_{i,J}\\) indicator functions for the \\(J\\) levels in the first factor and \\(x_{i,J+1},\\dots,x_{i,J+K}\\) indicator functions for the \\(K\\) levels in the second factor."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-7",
    "href": "slides/linear-models/31-treatment-effect-models.html#treatment-effect-models-7",
    "title": "Treatment Effect Models",
    "section": "Treatment effect models",
    "text": "Treatment effect models\n\nHowever, given that we divided the mice randomly, is it possible that the observed difference is simply due to chance? Here, we can compute the sample average and standard deviation of each group and perform statistical inference on the difference of these means."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-8",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-8",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nHowever, when our interest spans both directions, for example, either an increase or decrease in weight, we need to compute the probability of t being as extreme as what we observe.\nThe formula simply changes to using the absolute value: 1 - pnorm(abs(t-test)) or 1-pt(abs(t_stat), with(stats, n[2]+n[1]-2).\n:::"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-9",
    "href": "slides/linear-models/31-treatment-effect-models.html#comparing-group-means-9",
    "title": "Treatment Effect Models",
    "section": "Comparing group means",
    "text": "Comparing group means\n\nHowever, when our interest spans both directions, for example, either an increase or decrease in weight, we need to compute the probability of t being as extreme as what we observe.\nThe formula simply changes to using the absolute value: 1 - pnorm(abs(t-test)) or 1-pt(abs(t_stat), with(stats, n[2]+n[1]-2).\n:::"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#case-study",
    "href": "slides/linear-models/31-treatment-effect-models.html#case-study",
    "title": "Treatment Effect Models",
    "section": "Case study",
    "text": "Case study\n\nMice were randomly selected and divided into two groups: one group receiving a high-fat diet while the other group served as the control.\nThe data is included in the dslabs package:\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \ntable(mice_weights$diet) \n\n\nchow   hf \n 394  386"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-15",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-15",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-16",
    "href": "slides/linear-models/31-treatment-effect-models.html#two-factor-designs-16",
    "title": "Treatment Effect Models",
    "section": "Two factor designs",
    "text": "Two factor designs\n\nScientific studies, particularly within epidemiology and social sciences, frequently omit interaction terms from models due to the high number of variables.\nAdding interactions necessitates numerous parameters, which in extreme cases may prevent the model from fitting.\nHowever, this approach assumes that the interaction terms are zero, and if incorrect, it can skew the interpretation of the results.\nConversely, when this assumption is valid, models excluding interactions are simpler to interpret, as parameters are typically viewed as the extent to which the outcome increases with the assigned treatment.\n\n\n\n\n\n\n\nTip\n\n\n\nLinear models are highly flexible and applicable in many contexts.\nFor example, we can include many more factors than just 2.\nWe have only just scratched the surface of how linear models can be used to estimate treatment effects.\nWe highly recommend learning more about this by exploring linear model textbooks and R manuals that cover the use of functions such as lm, contrasts, and model.matrix."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-10",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-10",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nAs an example, consider that the mice in our dataset are actually from several generations:\n\n\ntable(mice_weights$gen) \n\n\n  4   7   8   9  11 \n 97 195 193  97 198"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-11",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-11",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nTo estimate the variability due to the different generations, a convenient model is:\n\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\varepsilon_i\n\\]\n\nwith \\(x_{i,j}\\) indicator variables: \\(x_{i,j}=1\\) if mouse \\(i\\) is in level \\(j\\) and 0 otherwise, \\(J\\) representing the number of levels, in our example 5 generations, and the level effects constrained with.\n\n\\[\n\\frac{1}{J} \\sum_{j=1}^J \\beta_j = 0 \\implies \\sum_{j=1}^J \\beta_j = 0.  \n\\]"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-12",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-12",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nThe constraint makes the model identifiable and also allows us to quantify the variability due to generations with:\n\n\\[\n\\sigma^2_{\\text{gen}} \\equiv \\frac{1}{J}\\sum_{j=1}^J \\beta_j^2\n\\]"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#contrasts-13",
    "href": "slides/linear-models/31-treatment-effect-models.html#contrasts-13",
    "title": "Treatment Effect Models",
    "section": "Contrasts",
    "text": "Contrasts\n\nWe can see the estimated coefficients using the following:\n\n\nfit &lt;- lm(body_weight ~ gen,  data = mice_weights,  \n          contrasts = list(gen = contr.sum)) \ncontrast(emmeans(fit, ~gen))  \n\n contrast     estimate    SE  df t.ratio p.value\n gen4 effect    -0.122 0.705 775  -0.174  0.8621\n gen7 effect    -0.812 0.542 775  -1.497  0.3372\n gen8 effect    -0.113 0.544 775  -0.207  0.8621\n gen9 effect     0.149 0.705 775   0.212  0.8621\n gen11 effect    0.897 0.540 775   1.663  0.3372\n\nP value adjustment: fdr method for 5 tests"
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#array-representation-3",
    "href": "slides/linear-models/31-treatment-effect-models.html#array-representation-3",
    "title": "Treatment Effect Models",
    "section": "Array representation",
    "text": "Array representation\n\nAn alternative approach widely used in ANOVA to avoid indicators variables, is to save the data in an array, using different Greek letters to denote factors and indices to denote levels:\n\n\\[\nY_{i,j,k} = \\mu + \\alpha_j + \\beta_k + \\varepsilon_{i,j,k}\n\\]\n\nwith \\(\\mu\\) the overall mean, \\(\\alpha_j\\) the effect of level \\(j\\) in the first factor, and \\(\\beta_k\\) the effect of level \\(k\\) in the second factor."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#array-representation-4",
    "href": "slides/linear-models/31-treatment-effect-models.html#array-representation-4",
    "title": "Treatment Effect Models",
    "section": "Array representation",
    "text": "Array representation\n\nThe constraint can now be written as:\n\n\\[\n\\sum_{j=1}^J \\alpha_j = 0 \\text{ and } \\sum_{k=1}^K \\beta_k = 0\n\\]\n\nThis notation lends itself to estimating the effects by computing means across dimensions of the array."
  },
  {
    "objectID": "psets/pset-09-matrices.html",
    "href": "psets/pset-09-matrices.html",
    "title": "Problem set 9",
    "section": "",
    "text": "You are not allowed to load any package or use for-loop. For exercises 1 and 3-6 you only get to write one line of code for the solution.\nFor better preparation for midterm, we recommend not using chatGPT for this homework.\n\nCreate a 100 by 10 matrix of randomly generated normal numbers. Put the result in x.\nApply the three R functions that give you the dimension of x, the number of rows of x, and the number of columns of x, respectively.\nAdd the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x.\nAdd the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix x. Hint: Use sweep with FUN = \"+\".\nCompute the average of each row of x.\nCompute the average of each column of x.\nFor each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make a boxplot by digit class. Hint: Use logical operators and rowMeans.\nUse the function solve to solve the following system of equations. Hint: use the function solve.\n\n\\[\n\\begin{align}\nx+2y−2z &=−15\\\\\n2x+y−5z&=−21\\\\\nx−4y+z&=18\n\\end{align}\n\\]\n\nUse matrix multiplication to compute the average of each column of x and store in a single row matrix. Hint define a \\(1\\times n\\) matrix \\((1/n, \\dots, 1/n)\\) with \\(n\\) the nrow(x).\nUse matrix multiplication and other matrix operations to compute the standard deviation of each column. Do not use sweep or apply."
  },
  {
    "objectID": "slides/linear-models/31-treatment-effect-models.html#definitions",
    "href": "slides/linear-models/31-treatment-effect-models.html#definitions",
    "title": "Treatment Effect Models",
    "section": "Definitions",
    "text": "Definitions\n\nFactors: Categorical variables used to define subgroups.\nLevels: The distinct categories or groups within each factor.\nTreatment: The factor we are interested in. Levels are often experimental and control."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#applied-linear-algebra",
    "href": "slides/highdim/32-linear-algebra-intro.html#applied-linear-algebra",
    "title": "Introduction to Linear Algebra",
    "section": "Applied Linear Algebra",
    "text": "Applied Linear Algebra\n\nLinear algebra is the main mathematical technique used to describe and motivate statistical methods and machine learning approaches.\nWe introduce some of the mathematical concepts needed to understand these techniques and demonstrate how to work with matrices in R.\nTo learn the mathematical details of statistical and ML theory you will need to learn linear algebra in more detail."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#applied-linear-algebra-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#applied-linear-algebra-1",
    "title": "Intro to linear algebra",
    "section": "Applied Linear Algebra",
    "text": "Applied Linear Algebra\n\nWe use these concepts and techniques throughout the remainder of the book.\nWe start the chapter with a motivating example."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nA commonly used operation in data analysis is matrix multiplication.\nLinear algebra originated from mathematicians developing systematic ways to solve systems of linear equations."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-1",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\\[\n\\begin{aligned}\nx_1 + x_2 + x_3 + x_4 + x_5 &= 15 \\\\\n2x_1 - x_2 + x_3 - x_4 + x_5 &= 10 \\\\\n-x_1 + 3x_2 - 2x_3 + x_4 - x_5 &= -5 \\\\\nx_1 + 4x_2 + x_3 + 2x_4 + 3x_5 &= 34 \\\\\n3x_1 - 2x_2 + x_3 - x_4 + 2x_5 &= 20\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-2",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nMathematicians figured out that by representing linear systems of equations using matrices and vectors, predefined algorithms could be designed to solve any system of linear equations.\nA basic linear algebra class will teach some of these algorithms, such as Gaussian elimination, the Gauss-Jordan elimination, and the LU and QR decompositions.\nThese methods are usually covered in detail in university level linear algebra courses."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-3",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nTo explain matrix multiplication, define two matrices:\n\n\\[\n\\mathbf{A} =  \n\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{2}&\\dots&a_{mn}\n\\end{pmatrix}, \\,\n\\mathbf{B} = \\begin{pmatrix}\nb_{11}&b_{12}&\\dots&b_{1p}\\\\\nb_{21}&b_{22}&\\dots&b_{2p}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nb_{n1}&b_{n2}&\\dots&b_{np}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-4",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-4",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nThe product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is the matrix \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) that has entries \\(c_{ij}\\) equal to the sum of the component-wise product of the \\(i\\)th row of \\(\\mathbf{A}\\) with the \\(j\\)th column of \\(\\mathbf{B}\\)."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-5",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-5",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nUsing R code, we can define \\(\\mathbf{C}= \\mathbf{A}\\mathbf{B}\\) as follows:\n\n\nm &lt;- nrow(A) \np &lt;- ncol(B) \nC &lt;- matrix(0, m, p) \nfor(i in 1:m){ \n  for(j in 1:p){ \n    C[i,j] &lt;- sum(A[i,] * B[,j]) \n  } \n} \n\n\nBecause this operation is so common, R includes a mathematical operator %*% for matrix multiplication:\n\n\nC &lt;- A %*% B"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-6",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-6",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nUsing mathematical notation \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) looks like this:\n\n\\[\n\\tiny\n\\begin{pmatrix}\na_{11}b_{11} + \\dots + a_{1n}b_{n1}&\na_{11}b_{12} + \\dots + a_{1n}b_{n2}&\n\\dots&\na_{11}b_{1p} + \\dots + a_{1n}b_{np}\\\\\na_{21}b_{11} + \\dots + a_{2n}b_{n1}&\na_{21}b_{n2} + \\dots + a_{2n}b_{n2}&\n\\dots&\na_{21}b_{1p} + \\dots + a_{2n}b_{np}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}b_{11} + \\dots +a_{mn}b_{n1}&\na_{m1}b_{n2} + \\dots + a_{mn}b_{n2}&\n\\dots&\na_{m1}b_{1p} + \\dots + a_{mn}b_{np}\\\\\n\\end{pmatrix}\n\\]\n\nNote this implies the number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\)."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-7",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-7",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nSo how does this definition of matrix multiplication help solve systems of equations?\nAny system of equations\n\n\\[\n\\begin{aligned}\na_{11} x_1 + a_{12} x_2 \\dots + a_{1n}x_n &= b_1\\\\\na_{21} x_1 + a_{22} x_2 \\dots + a_{2n}x_n &= b_2\\\\\n\\vdots\\\\\na_{n1} x_1 + a_{n2} x_2 \\dots + a_{nn}x_n &= b_n\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-8",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-8",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ncan be represented as matrix multiplication by defining the following matrices:\n\n\\[\n\\mathbf{A} =\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{22}&\\dots&a_{nn}\n\\end{pmatrix}\n,\\,\n\\mathbf{b} =  \n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n,\\, \\mbox{ and }\n\\mathbf{x} =  \n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-9",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-9",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nWe can rewrite the system of equations like this:\n\n\\[\n\\mathbf{A}\\mathbf{x} =  \\mathbf{b}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-10",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-10",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nThe linear algebra algorithms listed above, such as Gaussian elimination, provide a way to compute the inverse matrix \\(A^{-1}\\) that solves the equation for \\(\\mathbf{x}\\):\n\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} =   \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-11",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-11",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nTo solve the first equation we wrote out, we can use the function solve:\n\n\n# Define the coefficient matrix A\nA &lt;- matrix(c(1,  1,  1,  1,  1,\n              2, -1,  1, -1,  1,\n             -1,  3, -2,  1, -1,\n              1,  4,  1,  2,  3,\n              3, -2,  1, -1,  2), \n            nrow = 5, byrow = TRUE)\n\nb &lt;- c(15, 10, -5, 34, 20)\n\n# Solve the system of equations\nx &lt;- solve(A, b)"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-12",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-12",
    "title": "Introduction to Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\nTo solve the first equation we wrote out in R, we can use the function qr.solve:\n\n\n# Define the coefficient matrix A\nA &lt;- matrix(c(1,  1,  1,  1,  1,\n              2, -1,  1, -1,  1,\n             -1,  3, -2,  1, -1,\n              1,  4,  1,  2,  3,\n              3, -2,  1, -1,  2), \n            nrow = 5, byrow = TRUE)\n\nb &lt;- c(15, 10, -5, 34, 20)\n\n# Solve the system of equations\nx &lt;- solve(A, b)"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-13",
    "href": "slides/highdim/32-linear-algebra-intro.html#matrix-multiplication-13",
    "title": "Intro to linear algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\n:::"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix",
    "title": "Introduction to Linear Algebra",
    "section": "The identity matrix",
    "text": "The identity matrix\n\nThe identity matrix, represented with a bold \\(\\mathbf{I}\\), is like the number 1, but for matrices: if you multiply a matrix by the identity matrix, you get back the matrix.\n\n\\[\n\\mathbf{I}\\mathbf{X} = \\mathbf{X}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-1",
    "title": "Introduction to Linear Algebra",
    "section": "The identity matrix",
    "text": "The identity matrix\n\nIf you define \\(\\mathbf{I}\\) as matrix with the same number of rows and columns (referred to as square matrix) with 0s everywhere except the diagonal:\n\n\\[\n\\mathbf{I}=\\begin{pmatrix}\n1&0&\\dots&0\\\\\n0&1&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&1\n\\end{pmatrix}\n\\]\n\nyou will obtain the desired property."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-2",
    "title": "Introduction to Linear Algebra",
    "section": "The identity matrix",
    "text": "The identity matrix\n\nNote that the definition of an inverse matrix implies that:\n\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{1}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-identity-matrix-3",
    "title": "Introduction to Linear Algebra",
    "section": "The identity matrix",
    "text": "The identity matrix\n\nBecause the default for the second argument in solve is an identity matrix, if we simply type solve(A), we obtain the inverse \\(\\mathbf{A}^{-1}\\).\nThis means we can also obtain a solution to our system of equations with:\n\n\nsolve(A) %*% b"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nMany of the analyses we perform with high-dimensional data relate directly or indirectly to distance.\nFor example, most machine learning techniques rely on being able to define distances between observations, using features or predictors.\nClustering algorithms, for example, search of observations that are similar.\nBut what does this mean mathematically?\nTo define distance, we introduce another linear algebra concept: the norm."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-1",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nRecall that a point in two dimensions can be represented in polar coordinates as:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-2",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-3",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nwith \\(\\theta = \\arctan{\\frac{x2}{x1}}\\) and \\(r = \\sqrt{x_1^2 + x_2^2}\\).\nIf we think of the point as two dimensional column vector \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\).\nThe norm can be thought of as the size of the two-dimensional vector disregarding the direction: if we change the angle, the vector changes but the size does not.\nThe point of defining the norm is that we can extrapolate the concept of size to higher dimensions."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-4",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-4",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nSpecifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\n$$\n|||| = $$\n\nWe can use the linear algebra concepts we have learned to define the norm like this:\n\n$$ ||||^2 = ^\n$$"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-5",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-5",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nTo define distance, suppose we have two two-dimensional points: \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\).\nWe can define how similar they are by simply using euclidean distance:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-6",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-6",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-7",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-7",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nWe know that the distance is equal to the length of the hypotenuse:\n\n\\[\n\\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]\n\nThe reason we introduced the norm is because this distance is the size of the vector between the two points and this can be extrapolated to any dimension.\nThe distance between two points, regardless of the dimensions, is defined as the norm of the difference:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-8",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-8",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n$$\n|| _1 - _2||. $$\n\nIf we use the digit data, the distance between the first and second observation will compute distance using all 784 features:\n\n$$ || _1 - _2 || = \n$$"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-9",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-9",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nTo demonstrate, let’s pick the features for three digits:\n\n\nx_1 &lt;- x[6,] \nx_2 &lt;- x[17,] \nx_3 &lt;- x[16,] \n\n\nWe can compute the distances between each pair using the definitions we just learned:\n\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt() \n\n[1] 2319.867 2331.210 2518.969\n\n\n\nIn R, the function crossprod(x) is convenient for computing norms.\nIt multiplies t(x) by x:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-10",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-10",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt() \n\n[1] 2319.867 2331.210 2518.969\n\n\n\nNote crossprod takes a matrix as the first argument.\nAs a result, the vectors used here are being coerced into single column matrices.\nAlso, note that crossprod(x,y) multiples t(x) by y.\nWe can see that the distance is smaller between the first two.\nThis agrees with the fact that the first two are 2s and the third is a 7."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-11",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-11",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\ny[c(6, 17, 16)] \n\n[1] 2 2 7\n\n\n\nWe can also compute all the distances at once relatively quickly using the function dist, which computes the distance between each row and produces an object of class dist:\n\n\nd &lt;- dist(x[c(6,17,16),]) \nclass(d) \n\n[1] \"dist\"\n\n\n\nThere are several machine learning related functions in R that take objects of class dist as input.\nTo access the entries using row and column indices, we need to coerce it into a matrix.\nWe can see the distance we calculated above like this:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-12",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-12",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nd \n\n         1        2\n2 2319.867         \n3 2331.210 2518.969\n\n\n\nThe image function allows us to quickly see an image of distances between observations.\nAs an example, we compute the distance between each of the first 300 observations and then make an image:\n\n\nd &lt;- dist(x[1:300,]) \nimage(as.matrix(d)) \n\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#distance-13",
    "href": "slides/highdim/32-linear-algebra-intro.html#distance-13",
    "title": "Intro to linear algebra",
    "section": "Distance",
    "text": "Distance\n\nThis is because observations from the same digits tend to be closer than to different digits:\n\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#spaces",
    "href": "slides/highdim/32-linear-algebra-intro.html#spaces",
    "title": "Intro to linear algebra",
    "section": "Spaces",
    "text": "Spaces\n\nPredictor space is a concept that is often used to describe machine learning algorithms.\nThe term space refers to an advanced mathematical definition for which we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#spaces-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#spaces-1",
    "title": "Intro to linear algebra",
    "section": "Spaces",
    "text": "Spaces\n\nA space can be thought of as the collection of all possible points that should be considered for the data analysis in question.\nThis includes points we could see, but have not been observed yet.\nIn the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, \\, i = 1, \\dots, p\\) is between 0 and 255.\nSome Machine Learning algorithms also define subspaces."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#spaces-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#spaces-2",
    "title": "Intro to linear algebra",
    "section": "Spaces",
    "text": "Spaces\n\nA commonly defined subspace in machine learning are neighborhoods composed of points that are close to a predetermined center.\nWe do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy:\n\n$$\n|| - _0 || r. $$"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#spaces-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#spaces-3",
    "title": "Intro to linear algebra",
    "section": "Spaces",
    "text": "Spaces\n\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region.\nWe will learn about these in ?@sec-trees."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nMatrices are usually represented with bold upper case letters:\n\n\\[\n\\mathbf{X} =  \n\\begin{bmatrix}\nx_{1,1}&x_{1,2}&\\dots & x_{1,p}\\\\\nx_{2,1}&x_{2,2}&\\dots & x_{2,p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n,1}&x_{n,2}&\\dots&x_{n,p}\\\\\n\\end{bmatrix}\n\\]\n\nwith \\(x_{i,j}\\) representing the \\(j\\)-the feature for the \\(i\\)-th observation."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#creating-a-matrix",
    "href": "slides/highdim/32-linear-algebra-intro.html#creating-a-matrix",
    "title": "Introduction to Linear Algebra",
    "section": "Creating a matrix",
    "text": "Creating a matrix\n\nIn R, we can create a matrix using the matrix function.\n\n\nz &lt;- matrix(rnorm(100*2), 100, 2)"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-1",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nLinear Algebra books denote vectors with lower case bold letters and represent as column vectors.\n\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1\\\\\\\nx_2\\\\\\\n\\vdots\\\\\\\nx_p\n\\end{bmatrix}\n\\]\n\nWe use this name because they have one columm, not because they are columns in a matrix."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-2",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nR follows this convention:\n\n\nas.matrix(1:5)\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-3",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nTo distinguish between features associated with the observations \\(i=1,\\dots,n\\), we add an index:\n\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-4",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-4",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nTo distinguish between features associated with the observations \\(i=1,\\dots,n\\), we add an index:\n\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-transpose",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-transpose",
    "title": "Introduction to Linear Algebra",
    "section": "The transpose",
    "text": "The transpose\n\nA common operation when working with matrices is the transpose.\nWe use the transpose to understand several concepts, such as distance, using matrix notation.\nThis operation simply converts the rows of a matrix into columns.\nWe use the symbols \\(\\top\\) or \\('\\) next to the bold upper case letter to denote the transpose:"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-transpose-1",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-transpose-1",
    "title": "Introduction to Linear Algebra",
    "section": "The transpose",
    "text": "The transpose\n\\[\n\\tiny\n\\text{if } \\,\n\\mathbf{X} =  \n\\begin{bmatrix}\n  x_{1,1}&\\dots & x_{1,p} \\\\\n  x_{2,1}&\\dots & x_{2,p} \\\\\n  \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&\\dots & x_{n,p}  \n  \\end{bmatrix} \\text{ then }\\,\n\\mathbf{X}^\\top =  \n\\begin{bmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}  \n  \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-transpose-2",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-transpose-2",
    "title": "Introduction to Linear Algebra",
    "section": "The transpose",
    "text": "The transpose\n\nIn R we compute the transpose using the function t.\n\n\nx &lt;- matrix(1:6, 3, 2)\ndim(x) \n\n[1] 3 2\n\ndim(t(x)) \n\n[1] 2 3"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-transpose-3",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-transpose-3",
    "title": "Introduction to Linear Algebra",
    "section": "The transpose",
    "text": "The transpose\n\nOne use of the transpose is that we can write the matrix \\(\\mathbf{X}\\) as rows of the column vectors representing the features for each individual observation in the following way:\n\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x}_1^\\top\\\\\n\\mathbf{x}_2^\\top\\\\\n\\vdots\\\\\n\\mathbf{x}_n^\\top\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#the-transpose-4",
    "href": "slides/highdim/32-linear-algebra-intro.html#the-transpose-4",
    "title": "Introduction to Linear Algebra",
    "section": "The transpose",
    "text": "The transpose\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x}_1^\\top\\\\\n\\mathbf{x}_2^\\top\\\\\n\\vdots\\\\\n\\mathbf{x}_n^\\top\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#matrices-in-r",
    "href": "slides/highdim/33-matrices-in-R.html#matrices-in-r",
    "title": "Matrices In R",
    "section": "Matrices in R",
    "text": "Matrices in R\n\nWhen the number of variables is large and they can all be represented as a number, it is convenient to store them in a matrix and perform the analysis with linear algebra operations, rather than using tidyverse with data frames.\nVariables for each observation are stored in a row, resulting in a matrix with as many columns as variables.\nWe refer to values represented in the rows of the matrix as the covariates or predictors and, in machine learning, we refer to them as the features."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#matrices-in-r-1",
    "href": "slides/highdim/33-matrices-in-R.html#matrices-in-r-1",
    "title": "Matrices In R",
    "section": "Matrices in R",
    "text": "Matrices in R\n\nIn linear algebra, we have three types of objects: scalars, vectors, and matrices.\nWe have already learned about vectors in R, and, although there is no data type for scalars, we can represent them as vectors of length 1.\nToday we learn how to work with matrices in R and relate them to linear algebra notation and concepts."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nThe first step in handling mail received in the post office is to sort letters by zip code:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-1",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-1",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nSoon we will describe how we can build computer algorithms to read handwritten digits, which robots then use to sort the letters.\nTo do this, we first need to collect data, which in this case is a high-dimensional dataset and best stored in a matrix."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-2",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-2",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nThe MNIST dataset was generated by digitizing thousands of handwritten digits, already read and annotated by humans: http://yann.lecun.com/exdb/mnist/"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-3",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-3",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\nExamples:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-4",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-4",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nThe images are converted into \\(28 \\times 28 = 784\\) pixels:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-5",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-5",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nFor each digitized image, indexed by \\(i\\), we are provided with 784 variables and a categorical outcome, or label, representing the digit among \\(0, 1, 2, 3, 4, 5, 6, 7 , 8,\\) and \\(9\\) that the image is representing.\nLet’s load the data using the dslabs package:\n\n\nlibrary(tidyverse) \nlibrary(dslabs) \nmnist &lt;- read_mnist()"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#case-study-mnist-6",
    "href": "slides/highdim/33-matrices-in-R.html#case-study-mnist-6",
    "title": "Matrices In R",
    "section": "Case study: MNIST",
    "text": "Case study: MNIST\n\nIn these cases, the pixel intensities are saved in a matrix:\n\n\nclass(mnist$train$images) \n\n[1] \"matrix\" \"array\" \n\n\n\nThe labels associated with each image are included in a vector:\n\n\ntable(mnist$train$labels) \n\n\n   0    1    2    3    4    5    6    7    8    9 \n5923 6742 5958 6131 5842 5421 5918 6265 5851 5949"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#motivating-tasks",
    "href": "slides/highdim/33-matrices-in-R.html#motivating-tasks",
    "title": "Matrices In R",
    "section": "Motivating tasks",
    "text": "Motivating tasks\n\nVisualize the original image. The pixel intensities are provided as rows in a matrix.\nDo some digits require more ink to write than others?\nAre some pixels uninformative?\nCan we remove smudges?\nBinarize the data.\nStandardize the digits."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#motivating-tasks-1",
    "href": "slides/highdim/33-matrices-in-R.html#motivating-tasks-1",
    "title": "Matrices In R",
    "section": "Motivating tasks",
    "text": "Motivating tasks\n\nThe tidyverse or data.table are not developed to perform these types of mathematical operations.\nFor this task, it is convenient to use matrices.\nTo simplify the code below, we will rename these x and y respectively:\n\n\nx &lt;- mnist$train$images \ny &lt;- mnist$train$labels"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#dimensions-of-a-matrix",
    "href": "slides/highdim/33-matrices-in-R.html#dimensions-of-a-matrix",
    "title": "Matrices In R",
    "section": "Dimensions of a matrix",
    "text": "Dimensions of a matrix\n\nThe nrow function tells us how many rows that matrix has:\n\n\nnrow(x) \n\n[1] 60000\n\n\nand ncol tells us how many columns:\n\nncol(x) \n\n[1] 784"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#dimensions-of-a-matrix-1",
    "href": "slides/highdim/33-matrices-in-R.html#dimensions-of-a-matrix-1",
    "title": "Matrices In R",
    "section": "Dimensions of a matrix",
    "text": "Dimensions of a matrix\n\nWe learn that our dataset contains 60,000 observations (images) and 784 features (pixels).\nThe dim function returns the rows and columns:\n\n\ndim(x) \n\n[1] 60000   784"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#creating-a-matrix",
    "href": "slides/highdim/33-matrices-in-R.html#creating-a-matrix",
    "title": "Matrices In R",
    "section": "Creating a matrix",
    "text": "Creating a matrix\n\nWe saw that we can create a matrix using the matrix function.\n\n\nz &lt;- matrix(rnorm(100*2), 100, 2) \n\n\nNote that by default the matrix is filled in column by column:\n\n\nmatrix(1:15, 3, 5) \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#creating-a-matrix-1",
    "href": "slides/highdim/33-matrices-in-R.html#creating-a-matrix-1",
    "title": "Matrices In R",
    "section": "Creating a matrix",
    "text": "Creating a matrix\n\nTo fill the matrix row by row, we can use the byrow argument:\n\n\nmatrix(1:15, 3, 5, byrow = TRUE) \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#subsetting",
    "href": "slides/highdim/33-matrices-in-R.html#subsetting",
    "title": "Matrices In R",
    "section": "Subsetting",
    "text": "Subsetting\n\nTo extract a specific entry from a matrix, for example the 300th row of the 100th column, we write:\n\n\nx[300,100]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#subsetting-1",
    "href": "slides/highdim/33-matrices-in-R.html#subsetting-1",
    "title": "Matrices In R",
    "section": "Subsetting",
    "text": "Subsetting\n\nWe can extract subsets of the matrices by using vectors of indexes.\nFor example, we can extract the first 100 pixels from the first 300 observations like this:\n\n\nx[1:300,1:100]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#subsetting-2",
    "href": "slides/highdim/33-matrices-in-R.html#subsetting-2",
    "title": "Matrices In R",
    "section": "Subsetting",
    "text": "Subsetting\n\nTo extract an entire row or subset of rows, we leave the column dimension blank.\n\n\nx[1:300,]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#subsetting-3",
    "href": "slides/highdim/33-matrices-in-R.html#subsetting-3",
    "title": "Matrices In R",
    "section": "Subsetting",
    "text": "Subsetting\n\nSimilarly, we can subset any number of columns by keeping the first dimension blank.\nHere is the code to extract the first 100 pixels:\n\n\nx[,1:100]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices",
    "href": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices",
    "title": "Matrices In R",
    "section": "Vectorization for matrices",
    "text": "Vectorization for matrices\n\nIn R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-1",
    "href": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-1",
    "title": "Matrices In R",
    "section": "Vectorization for matrices",
    "text": "Vectorization for matrices\n\nUsing mathematical notation, we would write it as follows:\n\n\\[\n\\begin{bmatrix}\n  X_{1,1}&\\dots & X_{1,p} \\\\\n  X_{2,1}&\\dots & X_{2,p} \\\\\n   & \\vdots & \\\\\n  X_{n,1}&\\dots & X_{n,p}  \n  \\end{bmatrix}\n-\n\\begin{bmatrix}\na_1\\\\\\\na_2\\\\\\\n\\vdots\\\\\\\na_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  X_{1,1}-a_1&\\dots & X_{1,p} -a_1\\\\\n  X_{2,1}-a_2&\\dots & X_{2,p} -a_2\\\\\n   & \\vdots & \\\\\n  X_{n,1}-a_n&\\dots & X_{n,p} -a_n\n  \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-6-standardize-the-digits",
    "href": "slides/highdim/33-matrices-in-R.html#task-6-standardize-the-digits",
    "title": "Matrices In R",
    "section": "Task 6: Standardize the digits",
    "text": "Task 6: Standardize the digits\n\nThe way R vectorizes arithmetic operations implies that we can scale each row of a matrix as follows:\n\n\n(x - rowMeans(x))/rowSds(x) \n\n\nYet this approach does not work for columns.\nFor columns, we can sweep:\n\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x)) \n\n\nTo divide by the standard deviation, we change the default arithmetic operation to division as follows:\n\n\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-6-standardize-the-digits-1",
    "href": "slides/highdim/33-matrices-in-R.html#task-6-standardize-the-digits-1",
    "title": "Matrices In R",
    "section": "Task 6: Standardize the digits",
    "text": "Task 6: Standardize the digits\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise.\nFor example, if two matrices are stored in x and y, then:\n\n\nx*y \n\n\ndoes not result in matrix multiplication.\nInstead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entry in row \\(i\\) and column \\(j\\) of x and y, respectively."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#creating-a-matrix-2",
    "href": "slides/highdim/33-matrices-in-R.html#creating-a-matrix-2",
    "title": "Matrices In R",
    "section": "Creating a matrix",
    "text": "Creating a matrix\n\nThe function as.vector converts a matrix back into a vector:\n\n\nas.vector(matrix(1:15, 3, 5)) \n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image",
    "href": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image",
    "title": "Matrices In R",
    "section": "Visualize the original image",
    "text": "Visualize the original image\n\nLet’s try to visualize the third observation.\n\n\nmnist$train$label[3] \n\n[1] 4"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-1",
    "href": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-1",
    "title": "Matrices In R",
    "section": "Visualize the original image",
    "text": "Visualize the original image\n\nThe third row of the matrix x[3,] contains the 784 pixel intensities.\nWe can assume these were entered in order and convert them back to a \\(28 \\times 28\\) matrix using:\n\n\ngrid &lt;- matrix(x[3,], 28, 28)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-2",
    "href": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-2",
    "title": "Matrices In R",
    "section": "Visualize the original image",
    "text": "Visualize the original image\n\nTo visualize the data, we can use image in the followin way:\n\n\nimage(1:28, 1:28, grid)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-3",
    "href": "slides/highdim/33-matrices-in-R.html#visualize-the-original-image-3",
    "title": "Matrices In R",
    "section": "Visualize the original image",
    "text": "Visualize the original image\n\nTo flip it back we can use:\n\n\nimage(1:28, 1:28, grid[, 28:1])"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries",
    "href": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries",
    "title": "Matrices In R",
    "section": "Row and column summaries",
    "text": "Row and column summaries\n\nA common operation with matrices is to apply the same function to each row or to each column.\nFor example, we may want to compute row averages and standard deviations.\nThe apply function lets you do this.\nThe first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function to be applied."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-1",
    "href": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-1",
    "title": "Matrices In R",
    "section": "Row and column summaries",
    "text": "Row and column summaries\n\nSo, for example, to compute the averages and standard deviations of each row, we write:\n\n\navgs &lt;- apply(x, 1, mean) \nsds &lt;- apply(x, 1, sd) \n\n\nTo compute these for the columns, we simply change the 1 to a 2:\n\n\navgs &lt;- apply(x, 2, mean) \nsds &lt;- apply(x, 2, sd)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-2",
    "href": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-2",
    "title": "Matrices In R",
    "section": "Row and column summaries",
    "text": "Row and column summaries\n\nBecause these operations are so common, special functions are available to perform them.\nThe functions rowMeans computes the average of each row:\n\n\navg &lt;- rowMeans(x) \n\n\nand the matrixStats function rowSds computes the standard deviations for each row:\n\n\nlibrary(matrixStats) \nsds &lt;- rowSds(x)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others",
    "href": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others",
    "title": "Matrices In R",
    "section": "Task 2: Do some digits require more ink to write than others?",
    "text": "Task 2: Do some digits require more ink to write than others?\n\nFor the second task, related to total pixel darkness, we want to see the average use of ink plotted against digit.\nWe have already computed this average and can generate a boxplot to answer the question:\n\n\navg &lt;- rowMeans(x) \nboxplot(avg ~ y)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others-1",
    "href": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others-1",
    "title": "Matrices In R",
    "section": "Task 2: Do some digits require more ink to write than others?",
    "text": "Task 2: Do some digits require more ink to write than others?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others-2",
    "href": "slides/highdim/33-matrices-in-R.html#task-2-do-some-digits-require-more-ink-to-write-than-others-2",
    "title": "Matrices In R",
    "section": "Task 2: Do some digits require more ink to write than others?",
    "text": "Task 2: Do some digits require more ink to write than others?\n\nFrom this plot we see that, not surprisingly, 1s use less ink than other digits."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#conditional-filtering",
    "href": "slides/highdim/33-matrices-in-R.html#conditional-filtering",
    "title": "Matrices In R",
    "section": "Conditional filtering",
    "text": "Conditional filtering\n\nOne of the advantages of matrices operations over tidyverse operations is that we can easily select columns based on summaries of the columns.\nNote that logical filters can be used to subset matrices in a similar way in which they can be used to subset vectors."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#conditional-filtering-1",
    "href": "slides/highdim/33-matrices-in-R.html#conditional-filtering-1",
    "title": "Matrices In R",
    "section": "Conditional filtering",
    "text": "Conditional filtering\n\nHere is a simple example subsetting columns with logicals:\n\n\nmatrix(1:15, 3, 5)[,c(FALSE, TRUE, TRUE, FALSE, TRUE)] \n\n     [,1] [,2] [,3]\n[1,]    4    7   13\n[2,]    5    8   14\n[3,]    6    9   15\n\n\n\nThis implies that we can select rows with conditional expression."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?\n\nWe can use these ideas to remove columns associated with pixels that don’t change much and thus do not inform digit classification.\nWe will quantify the variation of each pixel with its standard deviation across all entries.\nSince each column represents a pixel, we use the colSds function from the matrixStats package:\n\n\nsds &lt;- colSds(x) \n\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-1",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-1",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-2",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-2",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?\n\nhist(sds, breaks = 30, main = \"SDs\")"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-3",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-3",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-4",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-4",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?\n\nThis makes sense since we don’t write in some parts of the box.\nHere is the variance plotted by location:\n\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1]) \n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-5",
    "href": "slides/highdim/33-matrices-in-R.html#task-3-are-some-pixels-uninformative-5",
    "title": "Matrices In R",
    "section": "Task 3: Are some pixels uninformative?",
    "text": "Task 3: Are some pixels uninformative?\n\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\n\nnew_x &lt;- x[,colSds(x) &gt; 60] \ndim(new_x) \n\n[1] 60000   322\n\n\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#indexing-with-matrices",
    "href": "slides/highdim/33-matrices-in-R.html#indexing-with-matrices",
    "title": "Matrices In R",
    "section": "Indexing with matrices",
    "text": "Indexing with matrices\n\nAn operation that facilitates efficient coding is that we can change entries of a matrix based on conditionals applied to that same matrix.\nHere is a simple example:\n\n\nmat &lt;- matrix(1:15, 3, 5) \nmat[mat &gt; 6 & mat &lt; 12] &lt;- 0 \nmat \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    0    0   13\n[2,]    2    5    0    0   14\n[3,]    3    6    0   12   15"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges",
    "href": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges",
    "title": "Matrices In R",
    "section": "Task 4: Can we remove smudges?",
    "text": "Task 4: Can we remove smudges?\n\nA histogram of all our predictor data:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-1",
    "href": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-1",
    "title": "Matrices In R",
    "section": "Task 4: Can we remove smudges?",
    "text": "Task 4: Can we remove smudges?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-2",
    "href": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-2",
    "title": "Matrices In R",
    "section": "Task 4: Can we remove smudges?",
    "text": "Task 4: Can we remove smudges?\n\nhist(as.vector(x), breaks = 30, main = \"Pixel intensities\")"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-3",
    "href": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-3",
    "title": "Matrices In R",
    "section": "Task 4: Can we remove smudges?",
    "text": "Task 4: Can we remove smudges?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-4",
    "href": "slides/highdim/33-matrices-in-R.html#task-4-can-we-remove-smudges-4",
    "title": "Matrices In R",
    "section": "Task 4: Can we remove smudges?",
    "text": "Task 4: Can we remove smudges?\n\nshows a clear dichotomy which is explained as parts of the image with ink and parts without.\nIf we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\n\nnew_x &lt;- x \nnew_x[new_x &lt; 50] &lt;- 0"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-5-binarizing-the-data",
    "href": "slides/highdim/33-matrices-in-R.html#task-5-binarizing-the-data",
    "title": "Matrices In R",
    "section": "Task 5: Binarizing the data",
    "text": "Task 5: Binarizing the data\n\nThe histogram above seems to suggest that this data is mostly binary.\nA pixel either has ink or does not.\nApplying what we have learned, we can binarize the data using just matrix operations:\n\n\nbin_x &lt;- x \nbin_x[bin_x &lt; 255/2] &lt;- 0  \nbin_x[bin_x &gt; 255/2] &lt;- 1 \n\n\nWe can also convert to a matrix of logicals and then coerce to numbers like this:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#task-5-binarizing-the-data-1",
    "href": "slides/highdim/33-matrices-in-R.html#task-5-binarizing-the-data-1",
    "title": "Matrices In R",
    "section": "Task 5: Binarizing the data",
    "text": "Task 5: Binarizing the data\n\nbin_X &lt;- (x &gt; 255/2)*1"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-2",
    "href": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-2",
    "title": "Matrices In R",
    "section": "Vectorization for matrices",
    "text": "Vectorization for matrices\n\nThe same holds true for other arithmetic operations.\nThe function sweep facilitates this type of operation.\nIt works similarly to apply.\nIt takes each entry of a vector and applies an arithmetic operation to the corresponding row."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-3",
    "href": "slides/highdim/33-matrices-in-R.html#vectorization-for-matrices-3",
    "title": "Matrices In R",
    "section": "Vectorization for matrices",
    "text": "Vectorization for matrices\n\nSubtraction is the default arithmetic operation.\nSo, for example, to center each row around the average, we can use:\n\n\nsweep(x, 1, rowMeans(x))"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-3",
    "href": "slides/highdim/33-matrices-in-R.html#row-and-column-summaries-3",
    "title": "Matrices In R",
    "section": "Row and column summaries",
    "text": "Row and column summaries\n\nThe functions colMeans and colSds provide the version for columns.\nFor more fast implementations consider the functions available in matrixStats."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#do-some-digits-require-more-ink-to-write-than-others",
    "href": "slides/highdim/33-matrices-in-R.html#do-some-digits-require-more-ink-to-write-than-others",
    "title": "Matrices In R",
    "section": "Do some digits require more ink to write than others?",
    "text": "Do some digits require more ink to write than others?\n\nFor the second task, related to total pixel darkness, we want to see the average use of ink plotted against digit.\nWe have already computed this average and can generate a boxplot to answer the question:\n\n\navg &lt;- rowMeans(x) \nboxplot(avg ~ y)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#do-some-digits-require-more-ink-to-write-than-others-1",
    "href": "slides/highdim/33-matrices-in-R.html#do-some-digits-require-more-ink-to-write-than-others-1",
    "title": "Matrices In R",
    "section": "Do some digits require more ink to write than others?",
    "text": "Do some digits require more ink to write than others?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#conditional-filtering-2",
    "href": "slides/highdim/33-matrices-in-R.html#conditional-filtering-2",
    "title": "Matrices In R",
    "section": "Conditional filtering",
    "text": "Conditional filtering\n\nIn the following example we remove all observations containing at least one NA:\n\n\nx[apply(!is.na(x), 1, all),] \n\n\nThis being a common operation, we have a matrixStats function to do it faster:\n\n\nx[!rowAnyNAs(x),]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nWe can use these ideas to remove columns associated with pixels that don’t change much and thus do not inform digit classification.\nWe will quantify the variation of each pixel with its standard deviation across all entries."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-1",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-1",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nSince each column represents a pixel, we use the colSds function from the matrixStats package:\n\n\nsds &lt;- colSds(x)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-2",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-2",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-3",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-3",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nHere is the variance plotted by location:\n\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-4",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-4",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nWe could remove features that have no variation since these can’t help us predict.\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\n\nnew_x &lt;- x[,colSds(x) &gt; 60] \ndim(new_x) \n\n[1] 60000   322\n\n\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-5",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-5",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nHere is the variance plotted by location:\n\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-6",
    "href": "slides/highdim/33-matrices-in-R.html#are-some-pixels-uninformative-6",
    "title": "Matrices In R",
    "section": "Are some pixels uninformative?",
    "text": "Are some pixels uninformative?\n\nWe could remove features that have no variation since these can’t help us predict.\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\n\nnew_x &lt;- x[,colSds(x) &gt; 60] \ndim(new_x) \n\n[1] 60000   322\n\n\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#indexing-with-matrices-1",
    "href": "slides/highdim/33-matrices-in-R.html#indexing-with-matrices-1",
    "title": "Matrices In R",
    "section": "Indexing with matrices",
    "text": "Indexing with matrices\n\nA useful application is that we can change all the NA entries of a matrix to something else:\n\n\nx[is.na(x)] &lt;- 0"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges",
    "href": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges",
    "title": "Matrices In R",
    "section": "Can we remove smudges?",
    "text": "Can we remove smudges?\n\nA histogram of all our predictor data:"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-1",
    "href": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-1",
    "title": "Matrices In R",
    "section": "Can we remove smudges?",
    "text": "Can we remove smudges?\n\nThe plot shows a clear dichotomy which is explained as parts of the image with ink and parts without.\nIf we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\n\nnew_x &lt;- x \nnew_x[new_x &lt; 50] &lt;- 0"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-2",
    "href": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-2",
    "title": "Matrices In R",
    "section": "Can we remove smudges?",
    "text": "Can we remove smudges?"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-3",
    "href": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-3",
    "title": "Matrices In R",
    "section": "Can we remove smudges?",
    "text": "Can we remove smudges?\n\nshows a clear dichotomy which is explained as parts of the image with ink and parts without.\nIf we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\n\nnew_x &lt;- x \nnew_x[new_x &lt; 50] &lt;- 0"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-4",
    "href": "slides/highdim/33-matrices-in-R.html#can-we-remove-smudges-4",
    "title": "Matrices In R",
    "section": "Can we remove smudges?",
    "text": "Can we remove smudges?\n\nshows a clear dichotomy which is explained as parts of the image with ink and parts without.\nIf we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\n\nnew_x &lt;- x \nnew_x[new_x &lt; 50] &lt;- 0"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#binarizing-the-data",
    "href": "slides/highdim/33-matrices-in-R.html#binarizing-the-data",
    "title": "Matrices In R",
    "section": "Binarizing the data",
    "text": "Binarizing the data\n\nThe previous histogram seems to suggest that this data is mostly binary.\nA pixel either has ink or does not."
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#binarizing-the-data-1",
    "href": "slides/highdim/33-matrices-in-R.html#binarizing-the-data-1",
    "title": "Matrices In R",
    "section": "Binarizing the data",
    "text": "Binarizing the data\n\nApplying what we have learned, we can binarize the data using just matrix operations:\n\n\nbin_x &lt;- x \nbin_x[bin_x &lt; 255/2] &lt;- 0  \nbin_x[bin_x &gt; 255/2] &lt;- 1"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#binarizing-the-data-2",
    "href": "slides/highdim/33-matrices-in-R.html#binarizing-the-data-2",
    "title": "Matrices In R",
    "section": "Binarizing the data",
    "text": "Binarizing the data\n\nWe can also convert to a matrix of logicals and then coerce to numbers like this:\n\n\nbin_X &lt;- (x &gt; 255/2)*1"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#standardize-the-digits",
    "href": "slides/highdim/33-matrices-in-R.html#standardize-the-digits",
    "title": "Matrices In R",
    "section": "Standardize the digits",
    "text": "Standardize the digits\n\nThe way R vectorizes arithmetic operations implies that we can scale each row of a matrix as follows:\n\n\n(x - rowMeans(x))/rowSds(x)"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-1",
    "href": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-1",
    "title": "Matrices In R",
    "section": "Standardize the digits",
    "text": "Standardize the digits\n\nYet this approach does not work for columns.\nFor columns, we can sweep:\n\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x))"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-2",
    "href": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-2",
    "title": "Matrices In R",
    "section": "Standardize the digits",
    "text": "Standardize the digits\n\nTo divide by the standard deviation, we change the default arithmetic operation to division as follows:\n\n\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-3",
    "href": "slides/highdim/33-matrices-in-R.html#standardize-the-digits-3",
    "title": "Matrices In R",
    "section": "Standardize the digits",
    "text": "Standardize the digits\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise.\nFor example, if two matrices are stored in x and y, then:\n\n\nx*y \n\n\ndoes not result in matrix multiplication.\nInstead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entry in row \\(i\\) and column \\(j\\) of x and y, respectively."
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#check-if-it-worked",
    "href": "slides/highdim/32-linear-algebra-intro.html#check-if-it-worked",
    "title": "Introduction to Linear Algebra",
    "section": "Check if it worked",
    "text": "Check if it worked\n\ncbind(A %*% x, b)\n\n         b\n[1,] 15 15\n[2,] 10 10\n[3,] -5 -5\n[4,] 34 34\n[5,] 20 20"
  },
  {
    "objectID": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-5",
    "href": "slides/highdim/32-linear-algebra-intro.html#mathematical-notation-5",
    "title": "Introduction to Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\n\nTo distinguish between features associated with the observations \\(i=1,\\dots,n\\), we add an index:\n\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/33-matrices-in-R.html#componentwise-multiplication",
    "href": "slides/highdim/33-matrices-in-R.html#componentwise-multiplication",
    "title": "Matrices In R",
    "section": "Componentwise multiplication",
    "text": "Componentwise multiplication\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise.\nFor example, if two matrices are stored in x and y, then:\n\n\nx*y \n\n\ndoes not result in matrix multiplication.\nInstead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entry in row \\(i\\) and column \\(j\\) of x and y, respectively."
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance",
    "href": "slides/highdim/34-distance.html#distance",
    "title": "Distance",
    "section": "",
    "text": "Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance.\nMany machine learning techniques rely on defining distances between observations.\nClustering algorithms search of observations that are similar.\nBut what does this mean mathematically?"
  },
  {
    "objectID": "slides/highdim/34-distance.html#the-norm",
    "href": "slides/highdim/34-distance.html#the-norm",
    "title": "Distance",
    "section": "The norm",
    "text": "The norm\n\nA point can be represented in polar coordinates:"
  },
  {
    "objectID": "slides/highdim/34-distance.html#the-norm-1",
    "href": "slides/highdim/34-distance.html#the-norm-1",
    "title": "Distance",
    "section": "The norm",
    "text": "The norm\n\nIf \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/highdim/34-distance.html#the-norm-2",
    "href": "slides/highdim/34-distance.html#the-norm-2",
    "title": "Distance",
    "section": "The norm",
    "text": "The norm\n\nThe point of defining the norm is to extrapolate the concept of size to higher dimensions.\nSpecifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\n\\[\n||\\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_p^2}\n\\]\n\nSometimes convenient to write like this:\n\n\\[\n||\\mathbf{x}||^2 = x_1^2 + x_2^2 + \\dots + x_p^2\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#the-norm-3",
    "href": "slides/highdim/34-distance.html#the-norm-3",
    "title": "Distance",
    "section": "The norm",
    "text": "The norm\n\nWe define the norm like this:\n\n\\[\n||\\mathbf{x}||^2 = \\mathbf{x}^\\top\\mathbf{x}\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-1",
    "href": "slides/highdim/34-distance.html#distance-1",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nDistance is the norm of the difference:"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-2",
    "href": "slides/highdim/34-distance.html#distance-2",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n-We can see this using the definition we know:\n\\[\n\\mbox{distance} = \\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-3",
    "href": "slides/highdim/34-distance.html#distance-3",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nUsing the norm definition can be extrapolated to any dimension:\n\n\\[\n\\mbox{distance} = || \\mathbf{x}_1 - \\mathbf{x}_2||\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-4",
    "href": "slides/highdim/34-distance.html#distance-4",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nFor example, the distance between the first and second observation will compute distance using all 784 features:\n\n\\[\n|| \\mathbf{x}_1 - \\mathbf{x}_2 ||^2 = \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-5",
    "href": "slides/highdim/34-distance.html#distance-5",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nDefine the features and labels:\n\n\nmnist &lt;- read_mnist()\nx &lt;- mnist$train$images  \ny &lt;- mnist$train$labels \n\n\nx_1 &lt;- x[6,] \nx_2 &lt;- x[17,] \nx_3 &lt;- x[16,] \n\n\nCompute the distances:\n\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt() \n\n[1] 2319.867 2331.210 2518.969\n\n\n\nChecks out:\n\n\ny[c(6,17,16)]\n\n[1] 2 2 7"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-6",
    "href": "slides/highdim/34-distance.html#distance-6",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nIn R, the function crossprod(x) is convenient for computing norms.\nIt multiplies t(x) by x:\n\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt() \n\n[1] 2319.867 2331.210 2518.969"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-7",
    "href": "slides/highdim/34-distance.html#distance-7",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nWe can also compute all the distances at once:\n\n\nd &lt;- dist(x[c(6,17,16),]) \nd\n\n         1        2\n2 2319.867         \n3 2331.210 2518.969\n\n\n\ndist produces an object of class dist\n\n\nclass(d) \n\n[1] \"dist\"\n\n\n\nThere are several machine learning related functions in R that take objects of class dist as input."
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-8",
    "href": "slides/highdim/34-distance.html#distance-8",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\ndist objects are similar but not equal to a matrices.\nTo access the entries using row and column indices, we need to coerce it into a matrix.\n\n\nas.matrix(d)[2,3]\n\n[1] 2518.969"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-9",
    "href": "slides/highdim/34-distance.html#distance-9",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nThe image function allows us to quickly see an image of distances between observations.\n\n\nd &lt;- dist(x[1:300,]) \nimage(as.matrix(d))"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-10",
    "href": "slides/highdim/34-distance.html#distance-10",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nIf we order distance by the labels:\n\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-11",
    "href": "slides/highdim/34-distance.html#distance-11",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nThe image function allows us to quickly see an image of distances between observations.\n\n\nd &lt;- dist(x[1:300,]) \nimage(as.matrix(d))"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-12",
    "href": "slides/highdim/34-distance.html#distance-12",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal.\n\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])"
  },
  {
    "objectID": "slides/highdim/34-distance.html#distance-13",
    "href": "slides/highdim/34-distance.html#distance-13",
    "title": "Distance",
    "section": "Distance",
    "text": "Distance\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal.\n\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])"
  },
  {
    "objectID": "slides/highdim/34-distance.html#spaces",
    "href": "slides/highdim/34-distance.html#spaces",
    "title": "Distance",
    "section": "Spaces",
    "text": "Spaces\n\nPredictor space is a concept that is often used to describe machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points.\nThe space is the collection of all possible points that should be considered for the data analysis in question, including points we have not observed yet.\nIn the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, \\, i = 1, \\dots, p\\) is between 0 and 255."
  },
  {
    "objectID": "slides/highdim/34-distance.html#spaces-1",
    "href": "slides/highdim/34-distance.html#spaces-1",
    "title": "Distance",
    "section": "Spaces",
    "text": "Spaces\n\nSome Machine Learning algorithms also define subspaces.\nA commonly defined subspace in machine learning are neighborhoods composed of points that are close to a predetermined center.\nWe do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy:\n\n\\[\n|| \\mathbf{x} - \\mathbf{x}_0 || \\leq r.\n\\]"
  },
  {
    "objectID": "slides/highdim/34-distance.html#spaces-2",
    "href": "slides/highdim/34-distance.html#spaces-2",
    "title": "Distance",
    "section": "Spaces",
    "text": "Spaces\n\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region."
  },
  {
    "objectID": "slides/highdim/34-distance.html#spaces-3",
    "href": "slides/highdim/34-distance.html#spaces-3",
    "title": "Distance",
    "section": "Spaces",
    "text": "Spaces\n\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#dimension-reduction",
    "href": "slides/highdim/35-dimension-reduction.html#dimension-reduction",
    "title": "Dimension Reduction",
    "section": "",
    "text": "A typical machine learning task involves working with a large number of predictors which can make data analysis challenging.\nFor example, to compare each of the 784 features in our predicting digits example, we would have to create 306,936 scatterplots.\nCreating one single scatterplot of the data is impossible due to the high dimensionality."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#dimension-reduction-1",
    "href": "slides/highdim/35-dimension-reduction.html#dimension-reduction-1",
    "title": "Dimension Reduction",
    "section": "Dimension reduction",
    "text": "Dimension reduction\n\nThe general idea of dimension reduction is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations.\nWith fewer dimensions, data analysis becomes more feasible.\nThe general technique behind it all, the singular value decomposition, is also useful in other contexts.\nWe will describe Principal Component Analysis (PCA)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nWe consider an example with twin heights.\nSome pairs are adults, the others are children.\nHere we simulate 100 two-dimensional points that represent the number of standard deviations each individual is from the mean height.\nEach point is a pair of twins."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-1",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-1",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nWe see correlation is high and two clusters of twins:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-2",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-2",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nOur features are \\(n\\) two-dimensional points.\nWe will pretend that visualizing two dimensions is too challenging and want to explore the data through one histogram.\nWe want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-3",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-3",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nStart by standardizing data\n\n\nlibrary(matrixStats) \nx &lt;- sweep(x, 2, colMeans(x)) \nx &lt;- sweep(x, 2, colSds(x), \"/\")"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-4",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-4",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nWe highlight the distance between observation 1 and 2 (blue), and observation 1 and 51 (red)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-5",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-5",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nWe can compute these distances using dist:\n\n\nd &lt;- dist(x) \nas.matrix(d)[1, 2] \n\n[1] 0.5949407\n\nas.matrix(d)[2, 51] \n\n[1] 1.388275\n\n\n\nNote that the blue line is shorter.\nWe want our one dimension summary to approximate these distances."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-6",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-6",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nNote the blue and red line are almost diagonal.\nAn intuition is that most the information about distance is in that direction.\nWe can rotate the points in a way that preserve the distance between points, while increasing the variability in one dimension and reducing it on the other.\nUsing this method, we keep more of the information about distances in the first dimension."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-7",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-7",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-8",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-8",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nThis one number summary does ok at preserving distances, but, can we pick a one-dimensional summary that improves the approximation?\nIf we look back at the scatterplot and visualize a line between any pair of points, the length of this line is the distance between the two points.\nThese lines tend to go along the direction of the diagonal.\nWe will learn that we can rotate the points in a way that preserve the distance between points, while increasing the variability in one dimension and reducing it on the other."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-9",
    "href": "slides/highdim/35-dimension-reduction.html#motivation-preserving-distance-9",
    "title": "Dimension Reduction",
    "section": "Motivation: preserving distance",
    "text": "Motivation: preserving distance\n\nUsing this method, we keep more of the information about distances in the first dimension.\nIn the next section, we describe a mathematical approach that permits us to find rotations that preserve distance between points.\nWe can then find the rotation that maximizes the variability in the first dimension."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations",
    "href": "slides/highdim/35-dimension-reduction.html#rotations",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nWe saw that any point \\((x_1, x_2)^\\top\\) can be written as the base and height of a triangle with a hypotenuse going from \\((0,0)^\\top\\) to \\((x_1, x_2)^\\top\\):\n\n\\[\nx_1 = r \\cos\\phi, \\,\\, x_2 = r \\sin\\phi\n\\]\n\nwith \\(r\\) the length of the hypotenuse and \\(\\phi\\) the angle between the hypotenuse and the x-axis."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-1",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-1",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nTo rotate the point \\((x_1, x_2)^\\top\\) around a circle with center \\((0,0)^\\top\\) and radius \\(r\\) by an angle \\(\\theta\\) we change the angle to \\(\\phi + \\theta\\):\n\n\\[\nz_1 = r \\cos(\\phi+ \\theta), \\,\\,\nz_2 = r \\sin(\\phi + \\theta)\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-2",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-2",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-3",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-3",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nWe can use trigonometric identities to rewrite \\((z_1, z_2)\\):\n\n\\[\n\\begin{aligned}\nz_1 &= r \\cos(\\phi + \\theta)\\\\\n&= r \\cos \\phi \\cos\\theta -  r \\sin\\phi \\sin\\theta\\\\\n&=  x_1 \\cos(\\theta) -  x_2 \\sin(\\theta)\\\\\nz_2 &= r \\sin(\\phi + \\theta)\\\\\n&=  r \\cos\\phi \\sin\\theta + r \\sin\\phi \\cos\\theta\\\\\n&=  x_1 \\sin(\\theta) + x_2 \\cos(\\theta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-4",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-4",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nHere we rotate all points by a \\(-45\\) degrees:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-5",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-5",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nThe variability of \\(x_1\\) and \\(x_2\\) are similar.\nThe variability of \\(z_1\\) is larger than that of \\(z_2\\).\nThe distances between points appear to be preserved.\nWe soon show, mathematically, that distance is preserved."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#rotations-6",
    "href": "slides/highdim/35-dimension-reduction.html#rotations-6",
    "title": "Dimension Reduction",
    "section": "Rotations",
    "text": "Rotations\n\nNote that while the variability of \\(x_1\\) and \\(x_2\\) are similar, the variability of \\(z_1\\) is much larger than the variability of \\(z_2\\).\nAlso, notice that the distances between points appear to be preserved.\nIn the next sections, we show mathematically that this in fact the case."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nAny time a matrix \\(\\mathbf{X}\\) is multiplied by another matrix \\(\\mathbf{A}\\), we refer to the product\n\n\\[\\mathbf{Z} = \\mathbf{X}\\mathbf{A}\\]\nas a linear transformation of \\(\\mathbf{X}\\).\n\nWe can show that the previously shown rotation is a linear transformation."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-1",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-1",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nTo see this, note that for any row \\(i\\), the first entry was:\n\n\\[\nz_{i,1} = a_{1,1} x_{i,1} + a_{2,1} x_{i,2}\n\\]\n\nwith \\(a_{1,1} = \\cos\\theta\\) and \\(a_{2,1} = -\\sin\\theta\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-2",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-2",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nThe second entry was also a linear transformation:\n\n\\[z_{i,2} = a_{1,2} x_{i,1} + a_{2,2} x_{i,2}\\]\n\nwith \\(a_{1,2} = \\sin\\theta\\) and \\(a_{2,2} = \\cos\\theta\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-3",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-3",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nWe can therefore write these trasformation using the folowing matrix notation:\n\n\\[\n\\begin{pmatrix}\nz_1\\\\z_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix}^\\top\n\\begin{pmatrix}\nx_1\\\\x_2\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-4",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-4",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nAn advantage of using linear algebra is that we can write the transformation for the entire dataset by saving all observations in a \\(N \\times 2\\) matrix:\n\n\\[\n\\mathbf{X} \\equiv  \n\\begin{bmatrix}\n\\mathbf{x_1}^\\top\\\\\n\\vdots\\\\\n\\mathbf{x_n}^\\top\n\\end{bmatrix} =  \n\\begin{bmatrix}\nx_{1,1}&x_{1,2}\\\\\n\\vdots&\\vdots\\\\\nx_{n,1}&x_{n,2}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-5",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-5",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nWe can then obtain the rotated values \\(\\mathbf{z}_i\\) for each row \\(i\\) by applying a linear transformation of \\(X\\):\n\n\\[\n\\mathbf{Z} = \\mathbf{X} \\mathbf{A}\n\\mbox{ with }\n\\mathbf{A} = \\,\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix} =  \n\\begin{pmatrix}\n\\cos \\theta&\\sin \\theta\\\\\n-\\sin \\theta&\\cos \\theta\n\\end{pmatrix}  \n.\n\\]\n\nThe columns of \\(\\mathbf{A}\\) are referred to as directions because if we draw a vector from \\((0,0)\\) to \\((a_{1,j}, a_{2,j})\\), it points in the direction of the line that will become the \\(j-th\\) dimension."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-6",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-6",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nIf we define:\n\n\ntheta &lt;- -45 * 2*pi/360 #convert to radians \nA &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2) \n\n\nWe can write code implementing a rotation by any angle \\(\\theta\\) using linear algebra:\n\n\nrotate &lt;- function(x, theta){ \n  theta &lt;- theta*2*pi/360 \n  A &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2) \n  x %*% A \n}"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-7",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-7",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nAnother advantage of linear algebra we can convert \\(\\mathbf{Z}\\) back to \\(\\mathbf{X}\\) by multiplying by the inverse $^{-1}.\n\n\\[\n\\mathbf{Z} \\mathbf{A}^\\top = \\mathbf{X} \\mathbf{A}\\mathbf{A}^\\top\\ = \\mathbf{X}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-8",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-8",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nIn this particular case, we can use trigonometry to show that:\n\n\\[\nx_{i,1} = b_{1,1} z_{i,1} + b_{2,1} z_{i,2}\\\\\nx_{i,2} = b_{1,2} z_{i,1} + b_{2,2} z_{i,2}\n\\]\n\nwith \\(b_{2,1} = \\cos\\theta\\), \\(b_{2,1} = \\sin\\theta\\), \\(b_{1,2} = -\\sin\\theta\\), and \\(b_{2,2} = \\cos\\theta\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-9",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-9",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nThis implies that:\n\n\\[\n\\mathbf{X} = \\mathbf{Z}  \n\\begin{pmatrix}\n\\cos \\theta&-\\sin \\theta\\\\\n\\sin \\theta&\\cos \\theta\n\\end{pmatrix}  \n\\]\n\nThis implies that all the information in \\(\\mathbf{X}\\) is included in the rotation \\(\\mathbf{Z}\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-10",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-10",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nNote that in this case\n\n\\[\n\\begin{pmatrix}\n\\cos \\theta&-\\sin \\theta\\\\\n\\sin \\theta&\\cos \\theta\n\\end{pmatrix} =\n\\mathbf{A}^\\top\n\\]\nwhich implies\n\\[\n\\mathbf{Z} \\mathbf{A}^\\top = \\mathbf{X} \\mathbf{A}\\mathbf{A}^\\top\\ = \\mathbf{X}\n\\]\nand therefore that \\(\\mathbf{A}^\\top\\) is the inverse of \\(\\mathbf{A}\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\nRemember that we represent the rows of a matrix as column vectors.\nThis explains why we use \\(\\mathbf{A}\\) when showing the multiplication for the matrix \\(\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\\), but transpose the operation when showing the transformation for just one observation: \\(\\mathbf{z}_i = \\mathbf{A}^\\top\\mathbf{x}_i\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-11",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-11",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nTo see that distance is preserved note that the distance between two points \\(\\mathbf{z}_h\\) and \\(\\mathbf{z}_i\\) is\n\n\\[\n\\begin{aligned}\n||\\mathbf{z}_h - \\mathbf{z}_i|| &= ||\\mathbf{A} \\mathbf{x}_h - \\mathbf{A} \\mathbf{x}_i||   \\\\\n&= || \\mathbf{A} (\\mathbf{x}_h - \\mathbf{x}_i) || \\\\\n&= (\\mathbf{x}_h - \\mathbf{x}_i)^{\\top} \\mathbf{A}^{\\top} \\mathbf{A} (\\mathbf{x}_h - \\mathbf{x}_i) \\\\\n&=(\\mathbf{x}_h - \\mathbf{x}_i)^{\\top} (\\mathbf{x}_h - \\mathbf{x}_i) \\\\\n&= || \\mathbf{x}_h - \\mathbf{x}_i||\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nWe refer to transformation with the property \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) as orthogonal transformations.\nThese are guaranteed to preserve the distance between any two points.\nWe previously demonstrated our rotation has this property.\nWe can confirm using R:\n\n\nA %*% t(A) \n\n             [,1]         [,2]\n[1,] 1.000000e+00 1.014654e-17\n[2,] 1.014654e-17 1.000000e+00"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-1",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-1",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\n\\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares (TSS) of \\(\\mathbf{X}\\), defined as \\(\\sum_{i=1}^n \\sum_{j=1}^p x_{i,j}^2\\) is equal to the total sum of squares of the rotation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}^\\top\\).\n\n\\[\n\\sum_{1=1}^n ||\\mathbf{z}_i||^2 = \\sum_{i=1}^n ||\\mathbf{A}^\\top\\mathbf{x}_i||^2 = \\sum_{i=1}^n \\mathbf{x}_i^\\top \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}_i = \\sum_{i=1}^n \\mathbf{x}_i^\\top\\mathbf{x}_i = \\sum_{i=1}^n||\\mathbf{x}_i||^2  \n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-2",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-2",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nWe can confirm using R:\n\n\ntheta &lt;- -45 \nz &lt;- rotate(x, theta) # works for any theta \nsum(x^2) \n\n[1] 198\n\nsum(z^2) \n\n[1] 198"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-3",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-3",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nThis can be interpreted as a consequence of the fact that an orthogonal transformation guarantees that all the information is preserved.\nHowever, although the total is preserved, the sum of squares for the individual columns changes."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-4",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-4",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nNotice that \\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares (TSS) of \\(\\mathbf{X}\\), defined as \\(\\sum_{i=1}^n \\sum_{j=1}^p x_{i,j}^2\\) is equal to the total sum of squares of the rotation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}^\\top\\).\nTo illustrate, observe that if we denote the rows of \\(\\mathbf{Z}\\) as \\(\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\), then sum of squares can be written as:\n\n\\[\n\\sum_{1=1}^n ||\\mathbf{z}_i||^2 = \\sum_{i=1}^n ||\\mathbf{A}^\\top\\mathbf{x}_i||^2 = \\sum_{i=1}^n \\mathbf{x}_i^\\top \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}_i = \\sum_{i=1}^n \\mathbf{x}_i^\\top\\mathbf{x}_i = \\sum_{i=1}^n||\\mathbf{x}_i||^2  \n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-5",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-5",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nWe can confirm using R:\n\n\ntheta &lt;- -45 \nz &lt;- rotate(x, theta) # works for any theta \nsum(x^2) \n\n[1] 198\n\nsum(z^2) \n\n[1] 198\n\n\n\nThis can be interpreted as a consequence of the fact that an orthogonal transformation guarantees that all the information is preserved.\nHowever, although the total is preserved, the sum of squares for the individual columns changes."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-6",
    "href": "slides/highdim/35-dimension-reduction.html#orthogonal-transformations-6",
    "title": "Dimension Reduction",
    "section": "Orthogonal transformations",
    "text": "Orthogonal transformations\n\nHere we compute the proportion of TSS attributed to each column, referred to as the variance explained or variance captured by each column, for \\(\\mathbf{X}\\):\n\n\ncolSums(x^2)/sum(x^2) \n\n[1] 0.5 0.5\n\n\n\nand \\(\\mathbf{Z}\\):\n\n\ncolSums(z^2)/sum(z^2) \n\n[1] 0.98477912 0.01522088\n\n\n\nIn the next section, we describe how this last mathematical result can be useful."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nWe have established that orthogonal transformations preserve the distance between observations and the total sum of squares.\nWe have also established that, while the TSS remains the same, the way this total is distributed across the columns can change.\nThe general idea behind Principal Component Analysis (PCA) is to try to find orthogonal transformations that concentrate the variance explained in the first few columns.\nWe can then focus on these few columns, effectively reducing the dimension of the problem."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-1",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-1",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nIn our specific example, we are looking for the rotation that maximizes the variance explained in the first column.\nThe following code performs a grid search across rotations from -90 to 0:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-2",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-2",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-3",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-3",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nangles &lt;- seq(0, -90) \nv &lt;- sapply(angles, function(angle) colSums(rotate(x, angle)^2)) \nvariance_explained &lt;- v[1,]/sum(x^2) \nplot(angles, variance_explained, type = \"l\")"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-4",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-4",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nWe find that a -45 degree rotation appears to achieve the maximum, with over 98% of the total variability explained by the first dimension.\nWe denote this rotation matrix with \\(\\mathbf{V}\\):\n\n\ntheta &lt;- 2*pi*-45/360 #convert to radians \nV &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2) \n\n\nWe can rotate the entire dataset using:\n\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-5",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-5",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nz &lt;- x %*% V \n\n\nThe following animation further illustrates how different rotations affect the variability explained by the dimensions of the rotated data:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-6",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-6",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-7",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-7",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-8",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-8",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nThe first dimension of z is referred to as the first principal component (PC).\nBecause almost all the variation is explained by this first PC, the distance between rows in x can be very well approximated by the distance calculated with just z[,1]."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-9",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-9",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-10",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-10",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nWe also notice that the two groups, adults and children, can be clearly observed with the one number summary, better than with any of the two original dimensions."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-11",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-11",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-12",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-12",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nhist(x[,1], breaks = seq(-4,4,0.5)) \nhist(x[,2], breaks = seq(-4,4,0.5)) \nhist(z[,1], breaks = seq(-4,4,0.5))"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-13",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-13",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-14",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-14",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nWe can visualize these to see how the first component summarizes the data.\nIn the plot below, red represents high values and blue negative values:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-15",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-15",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-16",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-16",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nThis idea generalizes to dimensions higher than 2.\nAs done in our two dimensional example, we start by finding the \\(p \\times 1\\) vector \\(\\mathbf{v}_1\\) with \\(||\\mathbf{v}_1||=1\\) that maximizes \\(||\\mathbf{X} \\mathbf{v}_1||\\).\nThe projection \\(\\mathbf{X} \\mathbf{v}_1\\) is the first PC.\nTo find the second PC, we subtract the variation explained by first PC from \\(\\mathbf{X}\\):\n\n\\[\n\\mathbf{r} = \\mathbf{X} - \\mathbf{X} \\mathbf{v}_1 \\mathbf{v}_1^\\top\n\\]"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-17",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-17",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nand then find the vector \\(\\mathbf{v}_2\\) with\\(||\\mathbf{v}_2||=1\\) that maximizes \\(||\\mathbf{r} \\mathbf{v}_2||\\).\nThe projection \\(\\mathbf{X} \\mathbf{v}_2\\) is the second PC.\nWe then subtract the variation explained by the first two PCs, and continue this process until we have the entire rotation matrix and matrix of principal components, respectively:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-18",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-18",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\\[\n\\mathbf{V} =\n\\begin{bmatrix}  \n\\mathbf{v}_1&\\dots&\\mathbf{v}_p\n\\end{bmatrix},\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\n\nThe ideas of distance preservation extends to higher dimensions."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-19",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-19",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nFor a multidimensional matrix with \\(p\\) columns, the \\(\\mathbf{A}\\) transformation preserves the distance between rows, but with the variance explained by the columns in decreasing order.If the variances of the columns \\(\\mathbf{Z}_j\\), \\(j&gt;k\\) are very small, these dimensions have little to contribute to the distance calculation and we can approximate the distance between any two points with just \\(k\\) dimensions.\nIf \\(k\\) is much smaller than \\(p\\), then we can achieve a very efficient summary of our data.\n\n\n\n\n\n\nWarning"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-20",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-20",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nNotice that the solution to this maximization problem is not unique because \\(||\\mathbf{X} \\mathbf{v}|| = ||-\\mathbf{X} \\mathbf{v}||\\).\nAlso, note that if we multiply a column of \\(\\mathbf{A}\\) by \\(-1\\), we still represent \\(\\mathbf{X}\\) as \\(\\mathbf{Z}\\mathbf{V}^\\top\\) as long as we also multiple the corresponding column of \\(\\mathbf{V}\\) by -1.\nThis implies that we can arbitrarily change the sign of each column of the rotation \\(\\mathbf{V}\\) and principal component matrix \\(\\mathbf{Z}\\).\n:::"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-21",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-21",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nIn R, we can find the principal components of any matrix with the function prcomp:\n\n\npca &lt;- prcomp(x, center = FALSE) \n\n\nKeep in mind that default behavior is to center the columns of x before computing the PCs, an operation we don’t need because our matrix is scaled.\nThe object pca includes the rotated data \\(Z\\) in pca$x and the rotation \\(\\mathbf{V}\\) in pca$rotation.\nWe can see that columns of the pca$rotation are indeed the rotation obtained with -45 (remember the sign is arbitrary):"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-22",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-22",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\npca$rotation \n\n            PC1        PC2\n[1,] -0.7071068  0.7071068\n[2,] -0.7071068 -0.7071068\n\n\n\nThe square root of the variation of each column is included in the pca$sdev component.\nThis implies we can compute the variance explained by each PC using:\n\n\npca$sdev^2/sum(pca$sdev^2) \n\n[1] 0.98477912 0.01522088\n\n\n\nThe function summary performs this calculation for us:\n\n\nsummary(pca) \n\nImportance of components:\n                          PC1     PC2\nStandard deviation     1.4034 0.17448\nProportion of Variance 0.9848 0.01522\nCumulative Proportion  0.9848 1.00000\n\n\n\nWe also see that we can rotate x (\\(\\mathbf{X}\\)) and pca$x (\\(\\mathbf{Z}\\)) as explained with the mathematical formulas above:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-23",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-pca-23",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nall.equal(pca$x, x %*% pca$rotation) \n\n[1] TRUE\n\nall.equal(x, pca$x %*% t(pca$rotation)) \n\n[1] TRUE"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#examples",
    "href": "slides/highdim/35-dimension-reduction.html#examples",
    "title": "Dimension Reduction",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nThe iris data is a widely used example.\nIt includes four botanical measurements related to three flower species:\n\n\nnames(iris) \n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nhead(iris$Species)\n\n[1] setosa setosa setosa setosa setosa setosa\nLevels: setosa versicolor virginica"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-1",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-1",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nIf we visualize the distances, we see the three species:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-2",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-2",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nOur features matrix has four dimensions\nThree are very correlated:\n\n\ncor(x) \n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-3",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-3",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nIf we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions.\nUsing the summary function, we can see the variability explained by each PC:\n\n\npca &lt;- prcomp(x) \nsummary(pca) \n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-4",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-4",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nWe are able to approximate the distances with two dimensions:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-5",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-5",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nA useful application is we can now visualize with a two-dimensional plot:\n\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt; \n  ggplot(aes(PC1, PC2, fill = Species)) + \n  geom_point(cex = 3, pch = 21) + \n  coord_fixed(ratio = 1)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-6",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-6",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-7",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-7",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\nWe learn that:\n\nthe first PC ia weighted average of sepal length, petal length, and petal width (red in first column), and subtracting a a quantity proportional to sepal width (blue in first column).\nThe second PC is a weighted average of petal length and petal width, minus a weighted average of sepal length and petal width."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-8",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-8",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nwe learn that the first PC is obtained by taking a weighted average of sepal length, petal length, and petal width (red in first column), and subtracting a a quantity proportional to sepal width (blue in first column)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nThe written digits example has 784 features.\nIs there any room for data reduction? We will use PCA to answer this.\nWe expect pixels close to each other on the grid to be correlated: dimension reduction should be possible."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-1",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-1",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nLet’s compute the PCs:\n\n\npca &lt;- prcomp(mnist$train$images)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-2",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-2",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\nAnd look at the variance explained:\n\nplot(pca$sdev^2/sum(pca$sdev^2), xlab = \"PC\", ylab = \"Variance explained\")"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-3",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-3",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nFirst two PCs for a random sample of 500 digits:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-4",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-4",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nWe can also see the rotation values on the 28 \\(\\times\\) 28 grid to get an idea of how pixels are being weighted in the transformations that result in the PCs."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-5",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-5",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nWe can also confirm that the lower variance PCs appear related to unimportant variability, mainly smudges in the corners."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-6",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-6",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-7",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-7",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nWe can clearly see that first PC appears to be separating the 1s (red) from the 0s (blue).\nWe can vaguely discern digits, or parts of digits, in the other three PCs as well.\nBy looking at the PCs stratified by digits, we get further insights.\nFor example, we see that the second PC separates 4s, 7s, and 9s from the rest:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-8",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-8",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-9",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-9",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example\n\nWe can also confirm that the lower variance PCs appear related to unimportant variability, mainly smudges in the corners:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-10",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-10",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-11",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-11",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#mnist-example-12",
    "href": "slides/highdim/35-dimension-reduction.html#mnist-example-12",
    "title": "Dimension Reduction",
    "section": "MNIST example",
    "text": "MNIST example"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#linear-transformations-12",
    "href": "slides/highdim/35-dimension-reduction.html#linear-transformations-12",
    "title": "Dimension Reduction",
    "section": "Linear transformations",
    "text": "Linear transformations\n\nHere is an example for a 30 degree rotation, although it works for any angle:\n\n\nall.equal(as.matrix(dist(rotate(x, 30))), as.matrix(dist(x))) \n\n[1] TRUE\n\n\n\nUsing linear algebra, we can rewrite the quantity above as:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#transformations",
    "href": "slides/highdim/35-dimension-reduction.html#transformations",
    "title": "Dimension Reduction",
    "section": "Transformations",
    "text": "Transformations\n\nHere we compute the proportion of TSS attributed to each column, referred to as the variance explained or variance captured by each column, for \\(\\mathbf{X}\\):\n\n\ncolSums(x^2)/sum(x^2) \n\n[1] 0.5 0.5\n\n\n\nand \\(\\mathbf{Z}\\):\n\n\ncolSums(z^2)/sum(z^2) \n\n[1] 0.98477912 0.01522088\n\n\n\nWe now explain how useful this property can be."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe have established that orthogonal transformations preserve the distance between observations and the total sum of squares.\nWe have also established that, while the TSS remains the same, the way this total is distributed across the columns can change.\nThe general idea behind Principal Component Analysis (PCA) is to try to find orthogonal transformations that concentrate the variance explained in the first few columns.\nWe can then focus on these few columns, effectively reducing the dimension of the problem."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-1",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-1",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nIn our specific example, we are looking for the rotation that maximizes the variance explained in the first column:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-2",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-2",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe find that a -45 degree rotation appears to achieve the maximum, with over 98% of the total variability explained by the first dimension.\nWe denote this rotation matrix with \\(\\mathbf{V}\\):\n\n\ntheta &lt;- 2*pi*-45/360 #convert to radians \nV &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-3",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-3",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe can rotate the entire dataset using:\n\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\n\nIn R:\n\n\nz &lt;- x %*% V"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-4",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-4",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-5",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-5",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-6",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-6",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThe first dimension of z is referred to as the first principal component (PC).\nBecause almost all the variation is explained by this first PC, the distance between rows in x can be very well approximated by the distance calculated with just z[,1]."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-7",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-7",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nrafalib::mypar() \nplot(dist(x), dist(z[,1])) \nabline(0,1, col = \"red\")"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-8",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-8",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThe two groups can be clearly observed with the one dimension:\n\n\n\n\n\n\n\n\n\n\n\nBetter than with any of the two original dimensions."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-9",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-9",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe can visualize these to see how the first component summarizes the data:"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-10",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-10",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThis idea generalizes to dimensions higher than 2.\nAs done in our two dimensional example, we start by finding the \\(p \\times 1\\) vector \\(\\mathbf{v}_1\\) with \\(||\\mathbf{v}_1||=1\\) that maximizes \\(||\\mathbf{X} \\mathbf{v}_1||\\).\nThe projection \\(\\mathbf{X} \\mathbf{v}_1\\) is the first PC."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-11",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-11",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nTo find the second PC, we subtract the variation explained by first PC from \\(\\mathbf{X}\\):\n\n\\[\n\\mathbf{r} = \\mathbf{X} - \\mathbf{X} \\mathbf{v}_1 \\mathbf{v}_1^\\top\n\\]\n\nand then find the vector \\(\\mathbf{v}_2\\) with\\(||\\mathbf{v}_2||=1\\) that maximizes \\(||\\mathbf{r} \\mathbf{v}_2||\\).\nThe projection \\(\\mathbf{X} \\mathbf{v}_2\\) is the second PC."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-12",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-12",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe then subtract the variation explained by the first two PCs, and continue this process until we have the entire rotation matrix and matrix of principal components, respectively:\n\n\\[\n\\mathbf{V} =\n\\begin{bmatrix}  \n\\mathbf{v}_1&\\dots&\\mathbf{v}_p\n\\end{bmatrix},\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\n\nThe ideas of distance preservation extends to higher dimensions."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-13",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-13",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nFor a multidimensional matrix with \\(p\\) columns, we can find an orthogonal transformation \\(\\mathbf{A}\\) that preserves the distance between rows, but with the variance explained by the columns in decreasing order.\nIf the variances of the columns \\(\\mathbf{Z}_j\\), \\(j&gt;k\\) are very small, these dimensions have little to contribute to the distance calculation and we can approximate the distance between any two points with just \\(k\\) dimensions.\nIf \\(k\\) is much smaller than \\(p\\), then we can achieve a very efficient summary of our data.\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nNotice that the solution to this maximization problem is not unique because \\(||\\mathbf{X} \\mathbf{v}|| = ||-\\mathbf{X} \\mathbf{v}||\\).\nAlso, note that if we multiply a column of \\(\\mathbf{A}\\) by \\(-1\\), we still represent \\(\\mathbf{X}\\) as \\(\\mathbf{Z}\\mathbf{V}^\\top\\) as long as we also multiple the corresponding column of \\(\\mathbf{V}\\) by -1.\nThis implies that we can arbitrarily change the sign of each column of the rotation \\(\\mathbf{V}\\) and principal component matrix \\(\\mathbf{Z}\\)."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-14",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-14",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nIn R, we can find the principal components of any matrix with the function prcomp:\n\n\npca &lt;- prcomp(x, center = FALSE) \n\n\nThe default behavior is to center the columns of x before computing the PCs, an operation we don’t currently need because our matrix is scaled."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-15",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-15",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThe object pca includes the rotated data \\(Z\\) in pca$x and the rotation \\(\\mathbf{V}\\) in pca$rotation.\nWe can see that columns of the pca$rotation are indeed the rotation obtained with -45 (remember the sign is arbitrary):\n\n\npca$rotation \n\n            PC1        PC2\n[1,] -0.7071068  0.7071068\n[2,] -0.7071068 -0.7071068"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-16",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-16",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThe square root of the variation of each column is included in the pca$sdev component.\nThis implies we can compute the variance explained by each PC using:\n\n\npca$sdev^2/sum(pca$sdev^2) \n\n[1] 0.98477912 0.01522088"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-17",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-17",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nThe function summary performs this calculation:\n\n\nsummary(pca) \n\nImportance of components:\n                          PC1     PC2\nStandard deviation     1.4034 0.17448\nProportion of Variance 0.9848 0.01522\nCumulative Proportion  0.9848 1.00000"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-18",
    "href": "slides/highdim/35-dimension-reduction.html#principal-component-analysis-18",
    "title": "Dimension Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nWe also see that we can rotate x (\\(\\mathbf{X}\\)) and pca$x (\\(\\mathbf{Z}\\)) as explained with the mathematical earlier:\n\n\nall.equal(pca$x, x %*% pca$rotation) \n\n[1] TRUE\n\nall.equal(x, pca$x %*% t(pca$rotation)) \n\n[1] TRUE"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#iris-example-9",
    "href": "slides/highdim/35-dimension-reduction.html#iris-example-9",
    "title": "Dimension Reduction",
    "section": "Iris example",
    "text": "Iris example\n\nThe second PC is a weighted average of petal length and petal width, minus a weighted average of sepal length and petal width."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#pca-visualized",
    "href": "slides/highdim/35-dimension-reduction.html#pca-visualized",
    "title": "Dimension Reduction",
    "section": "PCA visualized",
    "text": "PCA visualized"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#first-four-pcs",
    "href": "slides/highdim/35-dimension-reduction.html#first-four-pcs",
    "title": "Dimension Reduction",
    "section": "First four PCs",
    "text": "First four PCs"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#first-pc",
    "href": "slides/highdim/35-dimension-reduction.html#first-pc",
    "title": "Dimension Reduction",
    "section": "First PC",
    "text": "First PC"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#second-pc",
    "href": "slides/highdim/35-dimension-reduction.html#second-pc",
    "title": "Dimension Reduction",
    "section": "Second PC",
    "text": "Second PC"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#third-pc",
    "href": "slides/highdim/35-dimension-reduction.html#third-pc",
    "title": "Dimension Reduction",
    "section": "Third PC",
    "text": "Third PC"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#fourth-pc",
    "href": "slides/highdim/35-dimension-reduction.html#fourth-pc",
    "title": "Dimension Reduction",
    "section": "Fourth PC",
    "text": "Fourth PC"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#last-four-pcs",
    "href": "slides/highdim/35-dimension-reduction.html#last-four-pcs",
    "title": "Dimension Reduction",
    "section": "Last four PCs",
    "text": "Last four PCs"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#approximation-with-200-dimensions",
    "href": "slides/highdim/35-dimension-reduction.html#approximation-with-200-dimensions",
    "title": "Dimension Reduction",
    "section": "Approximation with 200 dimensions",
    "text": "Approximation with 200 dimensions\n\nimages_hat &lt;- pca$x[,1:200] %*% t(pca$rotation[,1:200])"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#approximation-with-200-dimensions-1",
    "href": "slides/highdim/35-dimension-reduction.html#approximation-with-200-dimensions-1",
    "title": "Dimension Reduction",
    "section": "Approximation with 200 dimensions",
    "text": "Approximation with 200 dimensions"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#approximation-with-36-dimensions",
    "href": "slides/highdim/35-dimension-reduction.html#approximation-with-36-dimensions",
    "title": "Dimension Reduction",
    "section": "Approximation with 36 dimensions",
    "text": "Approximation with 36 dimensions"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html",
    "href": "slides/highdim/35-dimension-reduction.html",
    "title": "Dimension Reduction",
    "section": "",
    "text": "A typical machine learning task involves working with a large number of predictors which can make data analysis challenging.\nFor example, to compare each of the 784 features in our predicting digits example, we would have to create 306,936 scatterplots.\nCreating one single scatterplot of the data is impossible due to the high dimensionality."
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#dimensions-approximation",
    "href": "slides/highdim/35-dimension-reduction.html#dimensions-approximation",
    "title": "Dimension Reduction",
    "section": "200 dimensions approximation",
    "text": "200 dimensions approximation\n\nimages_hat &lt;- pca$x[,1:200] %*% t(pca$rotation[,1:200])"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#dimensions-approximation-1",
    "href": "slides/highdim/35-dimension-reduction.html#dimensions-approximation-1",
    "title": "Dimension Reduction",
    "section": "200 dimensions approximation",
    "text": "200 dimensions approximation\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine() masks gridExtra::combine()\n✖ dplyr::count()   masks matrixStats::count()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::select()  masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "slides/highdim/35-dimension-reduction.html#dimensions-approximation-2",
    "href": "slides/highdim/35-dimension-reduction.html#dimensions-approximation-2",
    "title": "Dimension Reduction",
    "section": "36 dimensions approximation",
    "text": "36 dimensions approximation"
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#terminology",
    "href": "slides/ml/notation-and-terminology.html#terminology",
    "title": "Notation And Terminology",
    "section": "Terminology",
    "text": "Terminology\n\nOutcome - what we want to predict\nFeatures - what we use to predict the outcome.\nAlgorithms that take feature values as input and returns a prediction for the outcome.\nWe train an algorithm using a dataset for which we do know the outcome, and then apply algorithm when we don’t know the outcome."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#terminology-1",
    "href": "slides/ml/notation-and-terminology.html#terminology-1",
    "title": "Notation And Terminology",
    "section": "Terminology",
    "text": "Terminology\n\nPrediction problems can be divided into categorical and continuous outcomes."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#categorical",
    "href": "slides/ml/notation-and-terminology.html#categorical",
    "title": "Notation And Terminology",
    "section": "Categorical",
    "text": "Categorical\n\nThe number of classes can vary greatly across applications.\nFor example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9.\nIn speech recognition, the outcomes are all possible words or phrases we are trying to detect.\nSpam detection has two outcomes: spam or not spam."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#categorical-1",
    "href": "slides/ml/notation-and-terminology.html#categorical-1",
    "title": "Notation And Terminology",
    "section": "Categorical",
    "text": "Categorical\n\nWe denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\).\nHowever, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#continuous",
    "href": "slides/ml/notation-and-terminology.html#continuous",
    "title": "Notation And Terminology",
    "section": "Continuous",
    "text": "Continuous\nExamples of outcomes include:\n\nstock prices\nrealestate prices\ntemperature next week\nstudent perforamnce"
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#notation",
    "href": "slides/ml/notation-and-terminology.html#notation",
    "title": "Notation And Terminology",
    "section": "Notation",
    "text": "Notation\n\nWe use \\(y_i\\) to denote the i-th outcome\n\\(x_{i,1}, \\dots, x_{i,p}\\) the corresponding features.\nFeatures are sometimes referred to as predictors or covariates.\nWe use matrix notation \\(\\mathbf{x}_i = (x_{i,1}, \\dots, x_{i,p})^\\top\\) to denote the vector of predictors."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#notation-1",
    "href": "slides/ml/notation-and-terminology.html#notation-1",
    "title": "Notation And Terminology",
    "section": "Notation",
    "text": "Notation\n\nBecause, to motivated algorithms, we often use statistical models that assume the features and predictors are random variables, we use captial letters.\nIn this case, when referring to an arbitrary set of features rather than a specific observation, we drop the index \\(i\\) and use \\(Y\\) and \\(\\mathbf{X} = (X_{1}, \\dots, X_{p})\\).\nWe use lower case, for example \\(\\mathbf{X} = \\mathbf{x}\\), to denote observed values."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#notation-2",
    "href": "slides/ml/notation-and-terminology.html#notation-2",
    "title": "Notation And Terminology",
    "section": "Notation",
    "text": "Notation\n\nThe machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features:\n\n\\[\n\\hat{y} = f(x_1,x_2,\\dots,x_p)\n\\]\n\nWe will learn several approaches to building these algorithms."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge",
    "href": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge",
    "title": "Notation And Terminology",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nThe general setup is as follows.\nWe have a series of features and an unknown outcome we want to predict:\n\n\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature p\n\n\n\n\n?\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(\\dots\\)\n\\(X_p\\)"
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-1",
    "href": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-1",
    "title": "Notation And Terminology",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nTo build a model that provides a prediction for any set of observed values \\(X_1=x_1, X_2=x_2, \\dots X_p=x_p\\), we collect data for which we know the outcome:\n\n\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature 5\n\n\n\n\n\\(y_{1}\\)\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n\\(x_{1,3}\\)\n\\(\\dots\\)\n\\(x_{1,p}\\)\n\n\n\\(y_{2}\\)\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n\\(x_{2,3}\\)\n\\(\\dots\\)\n\\(x_{2,p}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(y_n\\)\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n\\(x_{n,3}\\)\n\\(\\dots\\)\n\\(x_{n,p}\\)"
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-2",
    "href": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-2",
    "title": "Notation And Terminology",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nWhen the output is continuous, we refer to the ML task as prediction.\nWe use the term actual outcome \\(y\\) to denote what we end up observing.\nWe want the prediction \\(\\hat{y}\\) to match the actual outcome \\(y\\) as best as possible.\nWe define error as the difference between the prediction and the actual outcome \\(y - \\hat{y}\\)."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-3",
    "href": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-3",
    "title": "Notation And Terminology",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nWhen the outcome is categorical, we refer to the machine learning task as classification\nThe main output of the model will be a decision rule which prescribes which of the \\(K\\) classes we should predict."
  },
  {
    "objectID": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-4",
    "href": "slides/ml/notation-and-terminology.html#the-machine-learning-challenge-4",
    "title": "Notation And Terminology",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nMost models provide functions for each class \\(k\\), \\(f_k(x_1, x_2, \\dots, x_p)\\), that are used to make this decision such as\n\n\\[\n\\mbox{When } f_1(x_1, x_2, \\dots, x_p) &gt; C, \\mbox{ predict category 1}\n\\]\n\nOur predictions will be either right or wrong."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#evaluation-metrics",
    "href": "slides/ml/evaluation-metrics.html#evaluation-metrics",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nHere we describe ways in which machine learning algorithms are evaluated.\nWe need to quantify what we mean when we say an algorithm performs better.\nWe demonstrate with a boring and simple example: how to predict sex using height."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#evaluation-metrics-1",
    "href": "slides/ml/evaluation-metrics.html#evaluation-metrics-1",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nWe introduce the caret package, which provides useful functions to facilitate machine learning in R.\nWe describe caret it in more detail later"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#evaluation-metrics-2",
    "href": "slides/ml/evaluation-metrics.html#evaluation-metrics-2",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nFor our first example, we use the height data provided by the dslabs package.\n\n\nlibrary(caret) \nlibrary(dslabs) \n\n\nWe start by defining the outcome and predictors.\n\n\ny &lt;- heights$sex \nx &lt;- heights$height"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#evaluation-metrics-3",
    "href": "slides/ml/evaluation-metrics.html#evaluation-metrics-3",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nBut can we do better than guessing?\nA machine learning algorithm is evaluated on how it performs in the real world with a new datasets.\nHowever, when developing an algorithm, we usually have a dataset for which we know the outcomes.\nTo mimic the ultimate evaluation process, we split the data into two parts and act as if we don’t know the outcome for one of these.\nWe stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#training-and-test-sets",
    "href": "slides/ml/evaluation-metrics.html#training-and-test-sets",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nWe refer to the group for which we know the outcome, and that we use to develop the algorithm, as the training set.\nWe refer to the group for which we pretend we don’t know the outcome as the test set.\nA standard way of generating the training and test sets is by randomly splitting the data."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#training-and-test-sets-1",
    "href": "slides/ml/evaluation-metrics.html#training-and-test-sets-1",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nset.seed(2007) \ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) \n\n\nWe can use the result of the createDataPartition function call to define the training and test sets as follows:\n\n\ntest_set &lt;- heights[test_index, ] \ntrain_set &lt;- heights[-test_index, ]"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#training-and-test-sets-2",
    "href": "slides/ml/evaluation-metrics.html#training-and-test-sets-2",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nWe will develop an algorithm using only the training set.\nOnce we are done developing the algorithm, we will freeze it and evaluate it using the test set.\nThe simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set.\nThis metric is usually referred to as overall accuracy."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) \n\n\nNote that we are completely ignoring the predictor and simply guessing the sex."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-1",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-1",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\ncaret works best wtih factors.\nSo convert y_hat to factors using the factor function:\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) |&gt; \n  factor(levels = levels(test_set$sex)) \n\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n\nmean(y_hat == test_set$sex) \n\n[1] 0.51"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-2",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-2",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nCan we do better?\nExploratory data analysis suggests we can because, on average, males are slightly taller than females:\n\n\nlibrary(tidyverse) \nheights |&gt; group_by(sex) |&gt; summarize(avg = mean(height), sd = sd(height)) \n\n# A tibble: 2 × 3\n  sex      avg    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Female  64.9  3.76\n2 Male    69.3  3.61\n\n\n\nHow do we make use of this insight?"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-3",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-3",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nLet’s try another simple approach: predict Male if height is within two standard deviations from the average male.\n\n\ny_hat &lt;- factor(ifelse(x &gt; 62, \"Male\", \"Female\"), levels(test_set$sex)) \n\n\nThe accuracy goes up from 0.50 to about 0.80:\n\n\nmean(y == y_hat) \n\n[1] 0.793\n\n\n\nBut can we do even better?"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-4",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-4",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nBut remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation.\nAlthough for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in dangerously over-optimistic assessments."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-5",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-5",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\n\ncutoff &lt;- seq(61, 70) \naccuracy &lt;- sapply(cutoff, function(x){ \n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels = levels(test_set$sex)) \n  mean(y_hat == train_set$sex) \n})"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-6",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-6",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-7",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-7",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe see that the maximum value is:\n\n\nmax(accuracy) \n\n[1] 0.85\n\n\n\nwhich is much higher than 0.5.\nThe cutoff resulting in this accuracy is:\n\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)] \nbest_cutoff \n\n[1] 64"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#overall-accuracy-8",
    "href": "slides/ml/evaluation-metrics.html#overall-accuracy-8",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \ny_hat &lt;- factor(y_hat) \nmean(y_hat == test_set$sex) \n\n[1] 0.804\n\n\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing.\nAnd by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#the-confusion-matrix",
    "href": "slides/ml/evaluation-metrics.html#the-confusion-matrix",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches.\nGiven that the average female is about 64 inches, this prediction rule seems wrong.\nWhat happened?\nIf a student is the height of the average female, shouldn’t we predict Female?\nGenerally speaking, overall accuracy can be a deceptive measure."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#the-confusion-matrix-1",
    "href": "slides/ml/evaluation-metrics.html#the-confusion-matrix-1",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex) \ncm$table \n\n          Reference\nPrediction Female Male\n    Female     48   32\n    Male       71  374\n\n\n\nIf we study this table closely, it reveals a problem."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#the-confusion-matrix-2",
    "href": "slides/ml/evaluation-metrics.html#the-confusion-matrix-2",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nIf we compute the accuracy separately for each sex, we get:\n\n\ncm$byClass[c(\"Sensitivity\", \"Specificity\")] \n\nSensitivity Specificity \n      0.403       0.921 \n\n\n\nWe notice an imbalance: too many females are predicted to be male.\nWe are calling almost half of the females male! How can our overall accuracy be so high then?\nThis is because the prevalence of males in this dataset is high."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#the-confusion-matrix-3",
    "href": "slides/ml/evaluation-metrics.html#the-confusion-matrix-3",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThese heights were collected from three data sciences courses, two of which had higher male enrollment:\n\n\ncm$byClass[\"Prevalence\"] \n\nPrevalence \n     0.227 \n\n\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men.\nThis type of bias can actually be a big problem in practice.\nIf your training data is biased in some way, you are likely to develop algorithms that are biased as well."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#the-confusion-matrix-4",
    "href": "slides/ml/evaluation-metrics.html#the-confusion-matrix-4",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThe fact that we used a test set does not matter because it is also derived from the original biased dataset.\nThis is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix.\nA general improvement to using overall accuracy is to study sensitivity and specificity separately."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nTo define sensitivity and specificity, we need a binary outcome.\nWhen the outcomes are categorical, we can define these terms for a specific category.\nIn the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit.\nOnce we specify a category of interest, then we can talk about positive outcomes, \\(Y=1\\), and negative outcomes, \\(Y=0\\)."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-1",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-1",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nIn general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\).\nBecause an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm.\nFor this reason, we also examine specificity, which is generally defined as the ability of an algorithm to not predict a positive \\(\\hat{Y}=0\\) when the actual outcome is not a positive \\(Y=0\\).\nWe can summarize in the following way:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-2",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-2",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\nHigh sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\).\n\n\nHigh specificity: \\(Y=0 \\implies \\hat{Y} = 0\\).\n\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\n\nHigh specificity: \\(\\hat{Y}=1 \\implies Y=1\\).\n\nTo provide precise definitions, we name the four entries of the confusion matrix:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-3",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-3",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-4",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-4",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)).\nThis quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)).\nThis quantity is also called the true negative rate (TNR)."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-5",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-5",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nThere is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)).\nThis quantity is referred to as positive predictive value (PPV) and also as precision.\nNote that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-6",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-6",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nThe table includes a column that shows the definition if we think of the proportions as probabilities.\n\n\n\n\n\n\n\n\n\n\n\nMeasure of\nName 1\nName 2\nDefinition\nProbability representation\n\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\)"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-7",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-7",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nspecificity | PPV | Precision | \\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\) | \\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\)|.\nThe caret function confusionMatrix computes all these metrics for us once we define which category is the “positive” (Y=1).\nThe function expects factors as input, and the first level is considered the positive outcome or \\(Y=1\\).\nIn our example, Female is the first level because it comes before Male alphabetically."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-8",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-8",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nIf you type this into R, you will see several metrics including accuracy, sensitivity, specificity, and PPV.\nYou can access these directly, for example, like this:\n\n\ncm$overall[\"Accuracy\"] \n\nAccuracy \n   0.804 \n\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")] \n\nSensitivity Specificity  Prevalence \n      0.403       0.921       0.227 \n\n\n\nWe can see that the high overall accuracy is possible despite relatively low sensitivity.\nAs we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-9",
    "href": "slides/ml/evaluation-metrics.html#sensitivity-and-specificity-9",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nBecause prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity).\nThis is an example of why it is important to examine sensitivity and specificity and not just accuracy.\nBefore applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nAlthough we usually recommend studying both specificity and sensitivity, often it is useful to have a one-number summary, for example, for optimization purposes.\nOne metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy.\nBecause specificity and sensitivity are rates, it is more appropriate to compute the harmonic average.\nIn fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-1",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-1",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +  \n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]\n\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-2",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-2",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nwhen defining \\(F_1\\).\nRemember that, depending on the context, some types of errors are more costly than others.\nFor instance, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition.\nIn a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-3",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-3",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nThe \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently.\nTo do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:\n\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +  \n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-4",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-4",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nThe F_meas function in the caret package computes this summary with beta defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\n\ncutoff &lt;- seq(61, 70) \nF_1 &lt;- sapply(cutoff, function(x){ \n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels(test_set$sex)) \n  F_meas(data = y_hat, reference = factor(train_set$sex)) \n}) \n\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-5",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-5",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-6",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-6",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nWe see that it is maximized at \\(F_1\\) value of:\n\n\nmax(F_1) \n\n[1] 0.647\n\n\n\nThis maximum is achieved when we use the following cutoff:\n\n\nbest_cutoff &lt;- cutoff[which.max(F_1)] \nbest_cutoff \n\n[1] 66\n\n\n\nA cutoff of 66 makes more sense than 64.\nFurthermore, it balances the specificity and sensitivity of our confusion matrix:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-7",
    "href": "slides/ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score-7",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \nsensitivity(data = y_hat, reference = test_set$sex) \n\n[1] 0.63\n\nspecificity(data = y_hat, reference = test_set$sex) \n\n[1] 0.833\n\n\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "href": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nA machine learning algorithm with very high TPR and TNR may not be useful in practice when prevalence is close to either 0 or 1.\nTo see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease.\nThe doctor shares data with about 1/2 cases and 1/2 controls and some predictors.\nYou then develop an algorithm with TPR=0.99 and TNR = 0.99."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-1",
    "href": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-1",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nYou are excited to explain to the doctor that this means that if a patient has the disease, the algorithm is very likely to predict correctly.\nThe doctor is not impressed and explains that your TNR is too low for this algorithm to be used in practice.\nThis is because this is a rare disease with a prevalence in the general population of 0.5%.\nThe doctor reminds you of Bayes formula:\n\n\\[ \\mbox{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)} \\implies \\text{Precision} = \\text{TPR} \\times \\frac{\\text{Prevalence}}{\\text{TPR}\\times \\text{Prevalence} + \\text{FPR}\\times(1-\\text{Prevalence})} \\approx 0.33  \\]"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-2",
    "href": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-2",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nHere is plot of precision as a function of prevalence with TPR and TNR are 95%:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-3",
    "href": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-3",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-4",
    "href": "slides/ml/evaluation-metrics.html#prevalence-matters-in-practice-4",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nAlthough your algorithm has a precision of about 95% on the data you train on, with prevalence of 50%, if applied to the general population, the algorithm’s precision would be just 33%.\nThe doctor can’t use an algorithm with 33% of people receiving a positive test actually not having the disease.\nNote that even if your algorithm had perfect sensitivity, the precision would still be around 33%.\nSo you need to greatly decrease your FPR for the algorithm to be useful in practice."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nWhen comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\).\nThe second method clearly outperformed the first.\nHowever, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability.\nBe aware that guessing Male with higher probability would give us higher accuracy due to the bias in the sample:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-1",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-1",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\np &lt;- 0.9 \nn &lt;- length(test_index) \ny_hat &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(p, 1 - p)) |&gt;  \n  factor(levels = levels(test_set$sex)) \nmean(y_hat == test_set$sex) \n\n[1] 0.739\n\n\n\nBut, as described above, this would come at the cost of lower sensitivity.\nThe curves we describe in this section will help us see this.\nRemember that for each of these parameters, we can get a different sensitivity and specificity."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-2",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-2",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nFor this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.\nA widely used plot that does this is the receiver operating characteristic (ROC) curve.\nIf you are wondering where this name comes from, you can consult the ROC Wikipedia page1.\nThe ROC curve plots sensitivity, represented as the TPR, versus 1 - specificity represented as the false positive rate (FPR).\n\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-3",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-3",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nHere we compute the TPR and FPR needed for different probabilities of guessing male:\n\n\nprobs &lt;- seq(0, 1, length.out = 10) \nguessing &lt;- sapply(probs, function(p){ \n  y_hat &lt;-  \n    sample(c(\"Male\", \"Female\"), nrow(test_set), TRUE, c(p, 1 - p)) |&gt;  \n    factor(levels = c(\"Female\", \"Male\")) \n  c(FPR = 1 - specificity(y_hat, test_set$sex), \n    TPR = sensitivity(y_hat, test_set$sex)) \n}) \n\n\nWe can use similar code to compute these values for our our second approach."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-4",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-4",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nBy plotting both curves together, we are able to compare sensitivity for different values of specificity:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-5",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-5",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-6",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-6",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nWe see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method.\nKeep in mind that ROC curves for guessing always fall on the identity line.\nAlso, note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-7",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-7",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence.\nIn cases in which prevalence matters, we may instead make a precision-recall plot.\nThe idea is similar, but we instead plot precision against recall:"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-8",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-8",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-9",
    "href": "slides/ml/evaluation-metrics.html#roc-and-precision-recall-curves-9",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nFrom the plot on the left, we immediately see that the precision of guessing is not high.\nThis is because the prevalence is low.\nFrom the plot on the right, we also see that if we change \\(Y=1\\) to mean Male instead of Female, the precision increases.\nNote that the ROC curve would remain the same."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nUp to now we have described evaluation metrics that apply exclusively to categorical data.\nSpecifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification.\nHowever, these metrics are not useful for continuous outcomes.\nIn this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error-1",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error-1",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nThe most commonly used loss function is the squared loss function.\nIf \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply: \\((\\hat{y} - y)^2\\).\nBecause we often model \\(y\\) as the outcome of a random process, theoretically, it does not make sense to compare algorithms based on \\((\\hat{y} - y)^2\\) as the minimum can change from sample to sample.\nFor this reason, we minimize mean squared error (MSE):\n\n\\[\n\\text{MSE} \\equiv \\mbox{E}\\{(\\hat{Y} - Y)^2 \\}\n\\]"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error-2",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error-2",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nConsider that if the outcomes are binary, the MSE is equivalent to one minus expected accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise.\nDifferent algorithms will result in different predictions \\(\\hat{Y}\\), and therefore different MSE.\nIn general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.\nHowever, note that the MSE is a theoretical quantity.\nHow do we estimate this?"
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error-3",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error-3",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nBecause in practice we have tests set with many, say \\(N\\), independent observations, a commonly used observable estimate of the MSE is:\n\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\n\nwith the \\(\\hat{y}_i\\) generated completely independently from the the \\(y_i\\)."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error-4",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error-4",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nHowever, the estimate \\(\\hat{\\text{MSE}}\\) is a random variable.\nIn fact, \\(\\text{MSE}\\) and \\(\\hat{\\text{MSE}}\\) are often referred to as the true error and apparent error, respectively.\nDue to the complexity of some machine learning, it is difficult to derive the statistical properties of how well the apparent error estimates the true error.\nIn ?@sec-cross-validation, we introduce cross-validation an approach to estimating the MSE."
  },
  {
    "objectID": "slides/ml/evaluation-metrics.html#mean-squared-error-5",
    "href": "slides/ml/evaluation-metrics.html#mean-squared-error-5",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nWe end this chapter by pointing out that there are loss functions other than the squared loss.\nFor example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors.\n\\((\\hat{Y}_i - Y_i)^2\\).\nHowever, in this book we focus on minimizing square loss since it is the most widely used."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#machine-learning",
    "href": "slides/ml/36-intro-ml.html#machine-learning",
    "title": "Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nMachine learning has achieved remarkable successes in a variety of applications.\nThese range from the postal service’s use of machine learning for reading handwritten zip codes to the development of voice recognition systems."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#machine-learning-1",
    "href": "slides/ml/36-intro-ml.html#machine-learning-1",
    "title": "Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nOther significant advances include movie recommendation systems, spam and malware detection, housing price prediction algorithms, and the ongoing development of autonomous vehicles.\nThe field of Artificial Intelligence (AI) has been evolving for several decades."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#machine-learning-2",
    "href": "slides/ml/36-intro-ml.html#machine-learning-2",
    "title": "Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nTraditional AI systems, including some chess-playing machines, often relied on decision-making based on preset rules and knowledge representation.\nHowever, with the advent of data availability, machine learning has gained prominence.\nIt focuses on decision-making through algorithms trained with data.\nIn recent years, the terms AI and Machine Learning have been used interchangeably in many contexts, though they have distinct meanings."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#machine-learning-3",
    "href": "slides/ml/36-intro-ml.html#machine-learning-3",
    "title": "Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nMachine learning has achieved remarkable successes, ranging from the postal service’s handwritten zip code readers to voice recognition systems like Apple’s Siri.\nThese advances also include movie recommendation systems, spam and malware detection, housing price prediction algorithms, and the development of driverless cars."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#terminology",
    "href": "slides/ml/36-intro-ml.html#terminology",
    "title": "Introduction",
    "section": "Terminology",
    "text": "Terminology\n\nOutcome - what we want to predict\nFeatures - what we use to predict the outcome.\nAlgorithms that take feature values as input and returns a prediction for the outcome.\nWe train an algorithm using a dataset for which we do know the outcome, and then apply algorithm when we don’t know the outcome."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#terminology-1",
    "href": "slides/ml/36-intro-ml.html#terminology-1",
    "title": "Introduction",
    "section": "Terminology",
    "text": "Terminology\n\nPrediction problems can be divided into categorical and continuous outcomes."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#categorical",
    "href": "slides/ml/36-intro-ml.html#categorical",
    "title": "Introduction",
    "section": "Categorical",
    "text": "Categorical\n\nThe number of classes can vary greatly across applications.\nWe denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\).\nHowever, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#categorical-1",
    "href": "slides/ml/36-intro-ml.html#categorical-1",
    "title": "Introduction",
    "section": "Categorical",
    "text": "Categorical\n\nWe denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\).\nHowever, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#continuous",
    "href": "slides/ml/36-intro-ml.html#continuous",
    "title": "Introduction",
    "section": "Continuous",
    "text": "Continuous\nExamples of outcomes include:\n\nstock prices\nrealestate prices\ntemperature next week\nstudent perforamnce"
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#notation",
    "href": "slides/ml/36-intro-ml.html#notation",
    "title": "Introduction",
    "section": "Notation",
    "text": "Notation\n\nWe use \\(y_i\\) to denote the i-th outcome\n\\(x_{i,1}, \\dots, x_{i,p}\\) the corresponding features.\nAlso referred to as predictors or covariates.\nWe use matrix notation \\(\\mathbf{x}_i = (x_{i,1}, \\dots, x_{i,p})^\\top\\) to denote the vector of predictors."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#notation-1",
    "href": "slides/ml/36-intro-ml.html#notation-1",
    "title": "Introduction",
    "section": "Notation",
    "text": "Notation\n\nBecause, we often use statistical models to motivate algorithms we also use capital letters:\n\n\\[\nY \\mbox{ and } \\mathbf{X} = (X_{1}, \\dots, X_{p})\n\\]\n\nNote we drop the index \\(i\\) because it represents the random variable that generates observations.\nWe use lower case, for example \\(\\mathbf{X} = \\mathbf{x}\\), to denote observed values."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#notation-2",
    "href": "slides/ml/36-intro-ml.html#notation-2",
    "title": "Introduction",
    "section": "Notation",
    "text": "Notation\n\nThe machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features:\n\n\\[\n\\hat{y} = f(x_1,x_2,\\dots,x_p)\n\\]\n\nWe will learn several approaches to building these algorithms."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#the-machine-learning-challenge",
    "href": "slides/ml/36-intro-ml.html#the-machine-learning-challenge",
    "title": "Introduction",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nThe general setup is as follows.\nWe have a series of features and an unknown outcome we want to predict:\n\n\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature p\n\n\n\n\n?\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(\\dots\\)\n\\(X_p\\)"
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-1",
    "href": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-1",
    "title": "Introduction",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nTo build a model that provides a prediction for any set of observed values \\(X_1=x_1, X_2=x_2, \\dots X_p=x_p\\), we collect data for which we know the outcome:\n\n\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature 5\n\n\n\n\n\\(y_{1}\\)\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n\\(x_{1,3}\\)\n\\(\\dots\\)\n\\(x_{1,p}\\)\n\n\n\\(y_{2}\\)\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n\\(x_{2,3}\\)\n\\(\\dots\\)\n\\(x_{2,p}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(y_n\\)\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n\\(x_{n,3}\\)\n\\(\\dots\\)\n\\(x_{n,p}\\)"
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-2",
    "href": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-2",
    "title": "Introduction",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nWhen the output is continuous, we refer to the ML task as prediction.\nWe use the term actual outcome \\(y\\) to denote what we end up observing.\nWe want the prediction \\(\\hat{y}\\) to match the actual outcome \\(y\\) as best as possible.\nWe define error as the difference between the prediction and the actual outcome \\(y - \\hat{y}\\)."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-3",
    "href": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-3",
    "title": "Introduction",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nWhen the outcome is categorical, we refer to the machine learning task as classification\nThe main output of the model will be a decision rule which prescribes which of the \\(K\\) classes we should predict."
  },
  {
    "objectID": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-4",
    "href": "slides/ml/36-intro-ml.html#the-machine-learning-challenge-4",
    "title": "Introduction",
    "section": "The machine learning challenge",
    "text": "The machine learning challenge\n\nMost models provide functions for each class \\(k\\), \\(f_k(x_1, x_2, \\dots, x_p)\\), that are used to make this decision such as\n\n\\[\n\\mbox{When } f_k(x_1, x_2, \\dots, x_p) &gt; C, \\mbox{ predict category } k\n\\]\n\nHere predictions will be either right or wrong."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#evaluation-metrics",
    "href": "slides/ml/37-evaluation-metrics.html#evaluation-metrics",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Here we describe ways in which machine learning algorithms are evaluated.\nWe need to quantify what we mean when we say an algorithm performs better.\nWe demonstrate with a boring and simple example: how to predict sex using height."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-1",
    "href": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-1",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nWe introduce the caret package, which provides useful functions to facilitate machine learning in R.\nWe describe caret it in more detail later"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-2",
    "href": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-2",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nFor our first example, we use the height data provided by the dslabs package.\n\n\nlibrary(caret) \nlibrary(dslabs) \n\n\nWe start by defining the outcome and predictors.\n\n\ny &lt;- heights$sex \nx &lt;- heights$height"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-3",
    "href": "slides/ml/37-evaluation-metrics.html#evaluation-metrics-3",
    "title": "Evaluation Metrics",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nA machine learning algorithm is evaluated on how it performs in the real world with a new datasets.\nHowever, when developing an algorithm, we usually have a dataset for which we know the outcomes.\nTo mimic the ultimate evaluation process, we split the data into two parts and act as if we don’t know the outcome for one of these.\nWe stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#training-and-test-sets",
    "href": "slides/ml/37-evaluation-metrics.html#training-and-test-sets",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nset.seed(2007) \ntest_index &lt;- createDataPartition(y, times = 1, p = 0.25, list = FALSE) \n\n\nWe can use the result of the createDataPartition function call to define the training and test sets as follows:\n\n\ntest_set &lt;- heights[test_index, ] \ntrain_set &lt;- heights[-test_index, ]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#training-and-test-sets-1",
    "href": "slides/ml/37-evaluation-metrics.html#training-and-test-sets-1",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nset.seed(2007) \ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) \n\n\nWe can use the result of the createDataPartition function call to define the training and test sets as follows:\n\n\ntest_set &lt;- heights[test_index, ] \ntrain_set &lt;- heights[-test_index, ]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#training-and-test-sets-2",
    "href": "slides/ml/37-evaluation-metrics.html#training-and-test-sets-2",
    "title": "Evaluation Metrics",
    "section": "Training and test sets",
    "text": "Training and test sets\n\nWe develop an algorithm using only the training set.\nOnce we are done developing the algorithm, we will freeze it and evaluate it using the test set.\nThe simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set.\nThis metric is usually referred to as overall accuracy."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) |&gt; \n  factor(levels = levels(test_set$sex)) \n\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\n\nmean(y_hat == test_set$sex) \n\n[1] 0.479"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-1",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-1",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nCan we do better?\nExploratory data analysis suggests we can because, on average, males are slightly taller than females:\n\n\nlibrary(tidyverse) \nheights |&gt; group_by(sex) |&gt; summarize(avg = mean(height), sd = sd(height)) \n\n# A tibble: 2 × 3\n  sex      avg    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Female  64.9  3.76\n2 Male    69.3  3.61\n\n\n\nHow do we make use of this insight?"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-2",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-2",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nLet’s try another simple approach: predict Male if height is within two standard deviations from the average male.\n\n\ny_hat &lt;- factor(ifelse(x &gt; 62, \"Male\", \"Female\"), levels(test_set$sex)) \n\n\nThe accuracy goes up from 0.50 to about 0.80:\n\n\nmean(y == y_hat) \n\n[1] 0.793\n\n\n\nBut can we do even better?"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-3",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-3",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\n\ncutoff &lt;- seq(61, 70) \naccuracy &lt;- sapply(cutoff, function(x){ \n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels = levels(test_set$sex)) \n  mean(y_hat == train_set$sex) \n})"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-4",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-4",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-5",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-5",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe see that the maximum value is:\n\n\nmax(accuracy) \n\n[1] 0.834\n\n\n\nwhich is much higher than 0.5.\nThe cutoff resulting in this accuracy is:\n\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)] \nbest_cutoff \n\n[1] 64"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-6",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-6",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \ny_hat &lt;- factor(y_hat) \nmean(y_hat == test_set$sex) \n\n[1] 0.806\n\n\n\nThe estimate of accuracy is biased due to slight over-training.\nBut ultimately we tested on a dataset that we did not train on."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-7",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-7",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \ny_hat &lt;- factor(y_hat) \nmean(y_hat == test_set$sex) \n\n[1] 0.804"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#overall-accuracy-8",
    "href": "slides/ml/37-evaluation-metrics.html#overall-accuracy-8",
    "title": "Evaluation Metrics",
    "section": "Overall accuracy",
    "text": "Overall accuracy\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing.\nAnd by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix",
    "href": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex) \ncm$table \n\n          Reference\nPrediction Female Male\n    Female     24   15\n    Male       36  188\n\n\n\nIf we study this table closely, it reveals a problem."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-1",
    "href": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-1",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nIf we compute the accuracy separately we get:\n\n\ncm$byClass[c(\"Sensitivity\", \"Specificity\")] \n\nSensitivity Specificity \n      0.400       0.926"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-2",
    "href": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-2",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThis is because the prevalence of males is high.\nThese heights were collected from three data sciences courses, two of which had higher male enrollment:\n\n\ncm$byClass[\"Prevalence\"] \n\nPrevalence \n     0.228 \n\n\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men.\nThis type of bias can actually be a big problem in practice.\nIf your training data is biased in some way, you are likely to develop algorithms that are biased as well."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-3",
    "href": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-3",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThe fact that we used a test set does not matter because it is also derived from the original biased dataset.\nThis is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nA general improvement to using overall accuracy is to study sensitivity and specificity separately."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-4",
    "href": "slides/ml/37-evaluation-metrics.html#the-confusion-matrix-4",
    "title": "Evaluation Metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\n\nThe fact that we used a test set does not matter because it is also derived from the original biased dataset.\nThis is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix.\nA general improvement to using overall accuracy is to study sensitivity and specificity separately."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nNeed binary outcome.\nSensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{y}=1\\) when \\(y=1\\).\nBecause an algorithm that calls everything positive has perfect sensitivity, this metric on its own is not enough to judge an algorithm.\nSpecificity, is the ability of an algorithm to not predict a positive \\(\\hat{y}=0\\) when the actual outcome is not a positive \\(y=0\\)."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-1",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-1",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nWe can summarize in the following way:\nHigh sensitivity: \\(y=1 \\implies \\hat{y}=1\\).\nHigh specificity: \\(y=0 \\implies \\hat{y} = 0\\).\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\nHigh specificity: \\(\\hat{y}=1 \\implies y=1\\)."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-2",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-2",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nTo provide precise definitions, we name the four entries of the confusion matrix:\n\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-3",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-3",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\).\nThis quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\).\nThis quantity is also called the true negative rate (TNR)."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-4",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-4",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nThere is another way of quantifying specificity which is \\(TP/(TP+FP)\\)\nThis quantity is referred to as positive predictive value (PPV) and also as precision.\nNote that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-5",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-5",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\n\n\n\n\n\nMeasure of\nName_1\nName_2\nDefinition\nProbability representation\n\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\)\n\n\nspecificity\nPPV\nPrecision\n\\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\)"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-6",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-6",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nThe caret function confusionMatrix computes all these metrics:\n\n\ncm$overall[\"Accuracy\"] \n\nAccuracy \n   0.806 \n\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")] \n\nSensitivity Specificity  Prevalence \n      0.400       0.926       0.228"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-7",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-7",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nBecause prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity).\nThis is an example of why it is important to examine sensitivity and specificity and not just accuracy.\nBefore applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-8",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-8",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nBecause prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity).\nThis is an example of why it is important to examine sensitivity and specificity and not just accuracy.\nBefore applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-9",
    "href": "slides/ml/37-evaluation-metrics.html#sensitivity-and-specificity-9",
    "title": "Evaluation Metrics",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\nBecause prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity).\nThis is an example of why it is important to examine sensitivity and specificity and not just accuracy.\nBefore applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +  \n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-1",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-1",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-2",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-2",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nThe \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently.\n\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +  \n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-3",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-3",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nThe F_meas function in the caret package computes this summary with beta defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\n\ncutoff &lt;- seq(61, 70) \nF_1 &lt;- sapply(cutoff, function(x){ \n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels(test_set$sex)) \n  F_meas(data = y_hat, reference = factor(train_set$sex)) \n})"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-4",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-4",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-5",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-5",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nWe see that it is maximized at \\(F_1\\) value of:\n\n\nmax(F_1) \n\n[1] 0.614\n\n\n\nThis maximum is achieved when we use the following cutoff:\n\n\nbest_cutoff &lt;- cutoff[which.max(F_1)] \nbest_cutoff \n\n[1] 66\n\n\n\nA cutoff of 66 makes more sense than 64."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-6",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-6",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\nFurthermore, it balances the specificity and sensitivity of our confusion matrix:\n\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \nsensitivity(data = y_hat, reference = test_set$sex) \n\n[1] 0.633\n\nspecificity(data = y_hat, reference = test_set$sex) \n\n[1] 0.857\n\n\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-7",
    "href": "slides/ml/37-evaluation-metrics.html#balanced-accuracy-and-f_1-score-7",
    "title": "Evaluation Metrics",
    "section": "Balanced accuracy and \\(F_1\\) score",
    "text": "Balanced accuracy and \\(F_1\\) score\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt;  \n  factor(levels = levels(test_set$sex)) \nsensitivity(data = y_hat, reference = test_set$sex) \n\n[1] 0.63\n\nspecificity(data = y_hat, reference = test_set$sex) \n\n[1] 0.833\n\n\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice",
    "href": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nA machine learning algorithm with very high TPR and TNR may not be useful in practice when prevalence is close to either 0 or 1.\nTo see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease.\nThe doctor shares data with about 1/2 cases and 1/2 controls and some predictors.\nYou then develop an algorithm with TPR=0.99 and TNR = 0.99."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-1",
    "href": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-1",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nYou are excited to explain to the doctor that this means that if a patient has the disease, the algorithm is very likely to predict correctly.\nThe doctor is not impressed and explains that your TNR is too low for this algorithm to be used in practice.\nThis is because this is a rare disease with a prevalence in the general population of 0.5%.\nThe doctor reminds you of Bayes formula:\n\n\\[ \\mbox{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)} \\implies \\text{Precision} = \\text{TPR} \\times \\frac{\\text{Prevalence}}{\\text{TPR}\\times \\text{Prevalence} + \\text{FPR}\\times(1-\\text{Prevalence})} \\approx 0.33  \\]"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-2",
    "href": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-2",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nHere is plot of precision as a function of prevalence with TPR and TNR are 95%:"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-3",
    "href": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-3",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-4",
    "href": "slides/ml/37-evaluation-metrics.html#prevalence-matters-in-practice-4",
    "title": "Evaluation Metrics",
    "section": "Prevalence matters in practice",
    "text": "Prevalence matters in practice\n\nAlthough your algorithm has a precision of about 95% on the data you train on, with prevalence of 50%, if applied to the general population, the algorithm’s precision would be just 33%.\nThe doctor can’t use an algorithm with 33% of people receiving a positive test actually not having the disease.\nNote that even if your algorithm had perfect sensitivity, the precision would still be around 33%.\nSo you need to greatly decrease your FPR for the algorithm to be useful in practice."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-1",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-1",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nThe packages pROC and plotROC are useful for generating these plots."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-2",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-2",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-3",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-3",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-4",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-4",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nFrom the plot on the left, we immediately see that the precision of guessing is not high.\nThis is because the prevalence is low.\nFrom the plot on the right, we also see that if we change \\(Y=1\\) to mean Male instead of Female, the precision increases.\nNote that the ROC curve would remain the same."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-5",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-5",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-6",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-6",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nWe see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method.\nKeep in mind that ROC curves for guessing always fall on the identity line.\nAlso, note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-7",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-7",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence.\nIn cases in which prevalence matters, we may instead make a precision-recall plot.\nThe idea is similar, but we instead plot precision against recall:"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-8",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-8",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-9",
    "href": "slides/ml/37-evaluation-metrics.html#roc-and-precision-recall-curves-9",
    "title": "Evaluation Metrics",
    "section": "ROC and precision-recall curves",
    "text": "ROC and precision-recall curves\n\nFrom the plot on the left, we immediately see that the precision of guessing is not high.\nThis is because the prevalence is low.\nFrom the plot on the right, we also see that if we change \\(Y=1\\) to mean Male instead of Female, the precision increases.\nNote that the ROC curve would remain the same."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nUp to now we have described evaluation metrics that apply exclusively to categorical data.\nSpecifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification.\nHowever, these metrics are not useful for continuous outcomes.\nIn this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error-1",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error-1",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nMost commont metric to minimize is mean squared error (MSE):\n\n\\[\n\\text{MSE} \\equiv \\mbox{E}\\{(\\hat{Y} - Y)^2 \\}\n\\]\n\nHow do we estimate this?"
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error-2",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error-2",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nBecause in practice we have tests set with many, say \\(N\\), independent observations, a commonly used observable estimate of the MSE is:\n\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\n\nwith the \\(\\hat{y}_i\\) generated completely independently from the the \\(y_i\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn practice, we often report the root mean squared error (RMSE), which is simply \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error-3",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error-3",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nThe estimate \\(\\hat{\\text{MSE}}\\) is a random variable.\n\\(\\text{MSE}\\) and \\(\\hat{\\text{MSE}}\\) are often referred to as the true error and apparent error, respectively.\nIt is difficult to derive the statistical properties of how well the apparent error estimates the true error.\nWe later introduce cross-validation an approach to estimating the MSE."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error-4",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error-4",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nThere are loss functions other than the squared loss.\nFor example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors.\n\\((\\hat{Y}_i - Y_i)^2\\).\nHowever, in this book we focus on minimizing square loss since it is the most widely used."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#mean-squared-error-5",
    "href": "slides/ml/37-evaluation-metrics.html#mean-squared-error-5",
    "title": "Evaluation Metrics",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nWe end this chapter by pointing out that there are loss functions other than the squared loss.\nFor example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors.\n\\((\\hat{Y}_i - Y_i)^2\\).\nHowever, in this book we focus on minimizing square loss since it is the most widely used."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations",
    "title": "Conditionals",
    "section": "Conditional probabilities and expectations",
    "text": "Conditional probabilities and expectations\n\nIn machine learning applications, we rarely can predict outcomes perfectly.\nThe most common reason for not being able to build perfect algorithms is that it is impossible.\nTo see this, consider that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations-1",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations-1",
    "title": "Conditionals",
    "section": "Conditional probabilities and expectations",
    "text": "Conditional probabilities and expectations\n\nBecause our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions).\nTherefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases.\nIt therefor makes sense to consider conditional probabilities:\n\n\\[\n\\mbox{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K\n\\]"
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations-2",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-and-expectations-2",
    "title": "Conditionals",
    "section": "Conditional probabilities and expectations",
    "text": "Conditional probabilities and expectations\n\nHowever, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions.\nTo achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in ?@sec-conditional-expectation.\nObservations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class.\nWe will write this idea out mathematically for the case of categorical data."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nWe will also use the following notation for the conditional probability of being class \\(k\\):\n\n\\[\np_k(\\mathbf{x}) = \\mbox{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K\n\\]\n\nNotice that the \\(p_k(\\mathbf{x})\\) have to add up to 1 for each \\(\\mathbf{x}\\), so once we know \\(K-1\\), we know all."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-1",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-1",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nWhen the outcome is binary, we only need to know 1, so we drop the \\(k\\) and use the notation:\n\n\\[p(\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\]\n\n\n\n\n\n\nNote\n\n\n\nDo not be confused by the fact that we use \\(p\\) for two different things: the conditional probability \\(p(\\mathbf{x})\\) and the number of predictors \\(p\\)."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-2",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-2",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nThese probabilities guide the construction of an algorithm that makes the best prediction:\n\n\\[\\hat{Y} = \\max_k p_k(\\mathbf{x})\\]\n\nIn machine learning, we refer to this as Bayes’ Rule.\nBut this is a theoretical rule since, in practice, we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\)."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-3",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-3",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nEstimating these conditional probabilities can be thought of as the main challenge of machine learning.\nThe better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor \\(\\hat{Y}\\)."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-4",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-4",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nHow well we predict depends on two things:\n\nhow close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and\nhow close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\).\n\nWe can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-5",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-5",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nThe first restriction does imply that we have limits as to how well even the best possible algorithm can perform.\nYou should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others, our success is restricted by the randomness of the process, such as with movie recommendations.\nKeep in mind that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-6",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-6",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nAs discussed in ?@sec-evaluation-metrics, sensitivity and specificity may differ in importance.\nBut even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish.\nFor instance, we can simply change the cutoffs used to predict one outcome or the other."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-probabilities-7",
    "href": "slides/ml/38-conditionals.html#conditional-probabilities-7",
    "title": "Conditionals",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nIn the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations",
    "href": "slides/ml/38-conditionals.html#conditional-expectations",
    "title": "Conditionals",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nFor binary data, you can think of the probability \\(\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\).\n\n\\[\n\\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}).\n\\]\n\nAs a result, we often only use the expectation to denote both the conditional probability and conditional expectation."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations-1",
    "href": "slides/ml/38-conditionals.html#conditional-expectations-1",
    "title": "Conditionals",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nWhy do we care about the conditional expectation in machine learning?\nThis is because the expected value has an attractive mathematical property: it minimizes the MSE.\n\n\\[\n\\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]"
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations-2",
    "href": "slides/ml/38-conditionals.html#conditional-expectations-2",
    "title": "Conditionals",
    "section": "Conditional expectations",
    "text": "Conditional expectations\n\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\n\\[\nf(\\mathbf{x}) \\equiv \\mbox{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\n\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)^\\top\\).\nThis is easier said than done, since this function can take any shape and \\(p\\) can be very large."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function",
    "href": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function",
    "title": "Conditionals",
    "section": "Conditional expectations minimizes squared loss function",
    "text": "Conditional expectations minimizes squared loss function\n\nWhy do we care about the conditional expectation in machine learning?\nThis is because the expected value has an attractive mathematical property: it minimizes the MSE.\n\n\\[\n\\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]"
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function-1",
    "href": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function-1",
    "title": "Conditionals",
    "section": "Conditional expectations minimizes squared loss function",
    "text": "Conditional expectations minimizes squared loss function\n\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\n\\[\nf(\\mathbf{x}) \\equiv \\mbox{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\n\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)^\\top\\).\nThis is easier said than done, since this function can take any shape and \\(p\\) can be very large."
  },
  {
    "objectID": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function-2",
    "href": "slides/ml/38-conditionals.html#conditional-expectations-minimizes-squared-loss-function-2",
    "title": "Conditionals",
    "section": "Conditional expectations minimizes squared loss function",
    "text": "Conditional expectations minimizes squared loss function\n\nThe expectation \\(\\mbox{E}\\{ Y  \\mid  X=x \\}\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything.\nIt gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\).\nFor example, in our digit reader example \\(p = 784\\)!.\nThe main way in which competing machine learning algorithms differ is in their approach to estimating this conditional expectation."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#smoothing",
    "href": "slides/ml/39-smoothing.html#smoothing",
    "title": "Smoothing",
    "section": "Smoothing",
    "text": "Smoothing\n\nBefore continuing learning about machine learning algorithms, we introduce the important concept of smoothing.\nSmoothing is a very powerful technique used all across data analysis.\nOther names given to this technique are curve fitting and low pass filtering.\nIt is designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#smoothing-1",
    "href": "slides/ml/39-smoothing.html#smoothing-1",
    "title": "Smoothing",
    "section": "Smoothing",
    "text": "Smoothing\n\nThe smoothing name comes from the fact that to accomplish this feat, we assume that the trend is smooth, as in a smooth surface.\nIn contrast, the noise, or deviation from the trend, is unpredictably wobbly:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#smoothing-2",
    "href": "slides/ml/39-smoothing.html#smoothing-2",
    "title": "Smoothing",
    "section": "Smoothing",
    "text": "Smoothing"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#smoothing-3",
    "href": "slides/ml/39-smoothing.html#smoothing-3",
    "title": "Smoothing",
    "section": "Smoothing",
    "text": "Smoothing\n\nPart of what we explain in this section are the assumptions that permit us to extract the trend from the noise."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nTo motivate the need for smoothing and make the connection with machine learning, we will construct a simplified version of the MNIST dataset with just two classes for the outcome and two predictors.\nSpecifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)).\nWe also selected a random sample of 1,000 digits, 500 in the training set and 500 in the test set."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-1",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-1",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nWe provide this dataset in the mnist_27 object in the dslabs package.\nFor the training data, we have \\(n=500\\) observed outcomes \\(y_1,\\dots,y_n\\), with \\(Y\\) defined as \\(1\\) if the digit is 7 and 0 if it’s 2, and \\(n=500\\) features \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), with each feature a two-dimensional point \\(\\mathbf{x}_i = (x_{i,1}, x_{i,2})^\\top\\).\nHere is a plot of the \\(x_2\\)s versus the \\(x_1\\)s with color determining if \\(y\\) is 1 (blue) or 0 (red):\n\n\nlibrary(caret) \nlibrary(dslabs) \nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-2",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-2",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-3",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-3",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nWe can immediately see some patterns.\nFor example, if \\(x_1\\) (the upper left panel) is very large, then the digit is probably a 7.\nAlso, for smaller values of \\(x_1\\), the 2s appear to be in the mid range values of \\(x_2\\).\nTo illustrate how to interpret \\(x_1\\) and \\(x_2\\), we include four example images."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-4",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-4",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nOn the left are the original images of the two digits with the largest and smallest values for \\(x_1\\) and on the right we have the images corresponding to the largest and smallest values of \\(x_2\\):"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-5",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-5",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-6",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-6",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nWe can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.\nWe haven’t really learned any algorithms yet, so let’s try building an algorithm using multivariable regression.\nThe model is simply:\n\n\\[\n\\begin{aligned}\np(\\mathbf{x}) &= \\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2)\\\\\n&= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-7",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-7",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nWe fit can fit this model using least squares and obtain an estimate \\(\\hat{p}(\\mathbf{x})\\) by using the least square estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\).\nWe define a decision rule by predicting \\(\\hat{y}=1\\) if \\(\\hat{p}(\\mathbf{x})&gt;0.5\\) and 0 otherwise.\n\n\nWe get an accuracy of 0.775, well above 50%.\nNot bad for our first try."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-8",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-8",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nBut can we do better?\nBecause we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(\\mathbf{x})\\).\nKeep in mind that in practice we don’t have access to the true conditional distribution.\nWe include it in this educational example because it permits the comparison of \\(\\hat{p}(\\mathbf{x})\\) to the true \\(p(\\mathbf{x})\\)."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-9",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-9",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nThis comparison teaches us the limitations of different algorithms.\nWe have stored the true \\(p(\\mathbf{x})\\) in the mnist_27 and can plot it as an image.\nWe draw a curve that separates pairs \\((\\mathbf{x})\\) for which \\(p(\\mathbf{x}) &gt; 0.5\\) and pairs for which \\(p(\\mathbf{x}) &lt; 0.5\\):"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-10",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-10",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-11",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-11",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nTo start understanding the limitations of regression, first note that with regression \\(\\hat{p}(\\mathbf{x})\\) has to be a plane, and as a result the boundary defined by the decision rule is given by:\n\\(\\hat{p}(\\mathbf{x}) = 0.5\\):\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies \\\\\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5  \\implies \\\\\nx_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2  -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-12",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-12",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nThis implies that for the boundary, \\(x_2\\) is a linear function of \\(x_1\\), which suggests that our regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\)."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-13",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-13",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nVisual representation of \\(\\hat{p}(\\mathbf{x})\\):"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-14",
    "href": "slides/ml/39-smoothing.html#example-is-it-a-2-or-a-7-14",
    "title": "Smoothing",
    "section": "Example: Is it a 2 or a 7?",
    "text": "Example: Is it a 2 or a 7?\n\nWe need something more flexible: a method that permits estimates with shapes other than a plane.\nSmoothing techniques permit this flexibility.\nWe will start by describing nearest neighbor and kernel approaches.\nTo understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nTo explain these concepts, we will focus first on a problem with just one predictor.\nSpecifically, we try to estimate the time trend in the 2008 US popular vote poll margin (the difference between Obama and McCain).\nLater we will learn about methods, such as k-nearest neighbors, that can be used to smooth with higher dimensions.\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + geom_point()"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-1",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-1",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-2",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-2",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nFor the purposes of the popular vote example, do not think of it as a forecasting problem.\nInstead, we are simply interested in learning the shape of the trend after the election is over.\nWe assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\)."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-3",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-3",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nA mathematical model for the observed poll margin \\(Y_i\\) is:\n\n\\[\nY_i = f(x_i) + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-4",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-4",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nTo think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\).\nIf we knew the conditional expectation \\(f(x) = \\mbox{E}(Y \\mid X=x)\\), we would use it.\nBut since we don’t know this conditional expectation, we have to estimate it.\nLet’s use regression, since it is the only method we have learned up to now."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-5",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-5",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-6",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-6",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nThe fitted regression line does not appear to describe the trend very well.\nFor example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls.\nHowever, the regression line does not capture this potential trend.\nTo see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing",
    "href": "slides/ml/39-smoothing.html#bin-smoothing",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nThe general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant.\nWe can make this assumption when we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of \\(x\\).\nAn example of this idea for the poll_2008 data is to assume that public opinion remained approximately the same within a week’s time.\nWith this assumption in place, we have several data points with the same expected value."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-1",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-1",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nIf we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\).\nThis assumption implies that:\n\n\\[\nE[Y_i | X_i = x_i ] \\approx \\mu \\mbox{   if   }  |x_i - x_0| \\leq 3.5\n\\]\n\nIn smoothing, we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-2",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-2",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nLater we will see that we try to optimize this parameter.\nThis assumption implies that a good estimate for \\(f(x_0)\\) is the average of the \\(Y_i\\) values in the window.\nIf we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is:\n\n\\[\n\\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0}  Y_i\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-3",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-3",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nWe make this calculation with each value of \\(x\\) as the center.\nIn the poll example, for each day, we would compute the average of the values within a week with that day in the center.\nHere are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\).\nThe blue segment represents the resulting average."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-4",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-4",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-5",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-5",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nBy computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\).\nBelow we show the procedure happening as we move from the -155 up to 0."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-6",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-6",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nAt each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-7",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-7",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing\n\nThe final code and resulting estimate look like this:\n\n\nspan &lt;- 7  \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"box\", bandwidth = span)) \npolls_2008 |&gt; mutate(fit = fit$y) |&gt; \n  ggplot(aes(x = day)) + \n  geom_point(aes(y = margin), size = 3, alpha = .5, color = \"grey\") +  \n  geom_line(aes(y = fit), color = \"red\")"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-8",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-8",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#bin-smoothing-9",
    "href": "slides/ml/39-smoothing.html#bin-smoothing-9",
    "title": "Smoothing",
    "section": "Bin smoothing",
    "text": "Bin smoothing"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels",
    "href": "slides/ml/39-smoothing.html#kernels",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nThe final result from the bin smoother is quite wiggly.\nOne reason for this is that each time the window moves, two points change.\nWe can attenuate this somewhat by taking weighted averages that give the center point more weight than far away points, with the two points at the edges receiving very little weight."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-1",
    "href": "slides/ml/39-smoothing.html#kernels-1",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nYou can think of the bin smoother approach as a weighted average:\n\n\\[\n\\hat{f}(x_0) = \\sum_{i=1}^N w_0(x_i) Y_i\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-2",
    "href": "slides/ml/39-smoothing.html#kernels-2",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nin which each point receives a weight of either \\(0\\) or \\(1/N_0\\), with \\(N_0\\) the number of points in the week.\nIn the code above, we used the argument kernel=\"box\" in our call to the function ksmooth.\nThis is because the weight function looks like a box.\nThe ksmooth function provides a “smoother” option which uses the normal density to assign weights."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-3",
    "href": "slides/ml/39-smoothing.html#kernels-3",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-4",
    "href": "slides/ml/39-smoothing.html#kernels-4",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nThe final code and resulting plot for the normal kernel look like this:\n\n\nspan &lt;- 7 \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"normal\", bandwidth = span)) \npolls_2008 |&gt; mutate(smooth = fit$y) |&gt; \n  ggplot(aes(day, margin)) + \n  geom_point(size = 3, alpha = .5, color = \"grey\") +  \n  geom_line(aes(day, smooth), color = \"red\")"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-5",
    "href": "slides/ml/39-smoothing.html#kernels-5",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-6",
    "href": "slides/ml/39-smoothing.html#kernels-6",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nNotice that this version looks smoother.\nThere are several functions in R that implement bin smoothers.\nOne example is ksmooth, shown above.\nIn practice, however, we typically prefer methods that use slightly more complex models than fitting a constant.\nThe final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example)."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold.\nAs a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\).\nHere we describe how local weighted regression (loess) permits us to consider larger window sizes.\nTo do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-1",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-1",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nTo see why this makes sense, consider the curved edges gardeners make using straight-edged spades:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-2",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-2",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear.\nWe can consider larger window sizes with the linear assumption than with a constant.\nInstead of the one-week window, we consider a larger one in which the trend is approximately linear.\nWe start with a three-week window and later consider and evaluate other options:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-3",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-3",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\n\nFor every point \\(x_0\\), loess defines a window and fits a line within that window.\nHere is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-4",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-4",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-5",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-5",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\).\nBelow we show the procedure happening as we move from the -155 up to 0:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-6",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-6",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-7",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-7",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\n\ntotal_days &lt;- diff(range(polls_2008$day)) \nspan &lt;- 21/total_days \nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008) \npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt; \n  ggplot(aes(day, margin)) + \n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-8",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-8",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-9",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-9",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nDifferent spans give us different estimates.\nWe can see how different window sizes lead to different estimates:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-10",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-10",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-11",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-11",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nHere are the final estimates:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-12",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-12",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nThere are three other differences between loess and the typical bin smoother.\n\n\nRather than keeping the bin size the same, loess keeps the number of points used in the local fit the same.\n\n\nThis number is controlled via the span argument, which expects a proportion.\nFor example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5 * N closest points to \\(x\\) for the fit."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-13",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-13",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\nWhen fitting a line locally, loess uses a weighted approach.\n\n\nBasically, instead of using least squares, we minimize a weighted version:\n\n\\[\n\\sum_{i=1}^N w_0(x_i) \\left[Y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2\n\\]\n\nHowever, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-14",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-14",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\\[\nW(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } W(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\n\nTo define the weights, we denote \\(2h\\) as the window size and define:\n\n\\[\nw_0(x_i) = W\\left(\\frac{x_i - x_0}{h}\\right)\n\\]\n\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-15",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-15",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-loess-16",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-loess-16",
    "title": "Smoothing",
    "section": "Local weighted regression (loess)",
    "text": "Local weighted regression (loess)\n\n3.\nloess has the option of fitting the local model robustly.\nAn iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration.\nTo use this option, we use the argument family=\"symmetric\"."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#fitting-parabolas",
    "href": "slides/ml/39-smoothing.html#fitting-parabolas",
    "title": "Smoothing",
    "section": "Fitting parabolas",
    "text": "Fitting parabolas\n\nTaylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola.\nThe theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines.\nThis means we can make our windows even larger and fit parabolas instead of lines.\n\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{   if   }  |x_i - x_0| \\leq h\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#fitting-parabolas-1",
    "href": "slides/ml/39-smoothing.html#fitting-parabolas-1",
    "title": "Smoothing",
    "section": "Fitting parabolas",
    "text": "Fitting parabolas\n\nYou may have noticed that when we showed the code for using loess, we set degree = 1.\nThis tells loess to fit polynomials of degree 1, a fancy name for lines.\nIf you read the help page for loess, you will see that the argument degree defaults to 2.\nBy default, loess fits parabolas not lines.\nHere is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):\n\n\ntotal_days &lt;- diff(range(polls_2008$day)) \nspan &lt;- 28/total_days \nfit_1 &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008) \nfit_2 &lt;- loess(margin ~ day, span = span, data = polls_2008) \npolls_2008 |&gt; mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) |&gt; \n  ggplot(aes(day, margin)) + \n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth_1), color = \"red\", lty = 2) + \n  geom_line(aes(day, smooth_2), color = \"orange\", lty = 1)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#fitting-parabolas-2",
    "href": "slides/ml/39-smoothing.html#fitting-parabolas-2",
    "title": "Smoothing",
    "section": "Fitting parabolas",
    "text": "Fitting parabolas"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#fitting-parabolas-3",
    "href": "slides/ml/39-smoothing.html#fitting-parabolas-3",
    "title": "Smoothing",
    "section": "Fitting parabolas",
    "text": "Fitting parabolas\n\nThe degree = 2 gives us more wiggly results.\nIn general, we actually prefer degree = 1 as it is less prone to this kind of noise."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters",
    "href": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters",
    "title": "Smoothing",
    "section": "Beware of default smoothing parameters",
    "text": "Beware of default smoothing parameters\n\nggplot uses loess in its geom_smooth function:\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + \n  geom_point() +  \n  geom_smooth(method = loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-1",
    "href": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-1",
    "title": "Smoothing",
    "section": "Beware of default smoothing parameters",
    "text": "Beware of default smoothing parameters"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-2",
    "href": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-2",
    "title": "Smoothing",
    "section": "Beware of default smoothing parameters",
    "text": "Beware of default smoothing parameters\n\nBut be careful with default parameters as they are rarely optimal.\nHowever, you can conveniently change them:\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + \n  geom_point() +  \n  geom_smooth(method = loess, method.args = list(span = 0.15, degree = 1))"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-3",
    "href": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-3",
    "title": "Smoothing",
    "section": "Beware of default smoothing parameters",
    "text": "Beware of default smoothing parameters"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-4",
    "href": "slides/ml/39-smoothing.html#beware-of-default-smoothing-parameters-4",
    "title": "Smoothing",
    "section": "Beware of default smoothing parameters",
    "text": "Beware of default smoothing parameters"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning",
    "href": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning",
    "title": "Smoothing",
    "section": "Connecting smoothing to machine learning",
    "text": "Connecting smoothing to machine learning\n\nTo see how smoothing relates to machine learning with a concrete example, consider again our two or seven example.\nIf we define the outcome \\(Y = 1\\) for digits that are seven and \\(Y=0\\) for digits that are 2, then we are interested in estimating the conditional probability:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2).\n\\]"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning-1",
    "href": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning-1",
    "title": "Smoothing",
    "section": "Connecting smoothing to machine learning",
    "text": "Connecting smoothing to machine learning\n\nIn this example, the 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(\\mathbf{x})\\) are not that close to 0 or 1.\nWe therefore need to estimate \\(p(\\mathbf{x})\\).\nSmoothing is an alternative to accomplishing this.\nWe saw that linear regression was not flexible enough to capture the non-linear nature of \\(p(\\mathbf{x})\\), thus smoothing approaches provide an improvement."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning-2",
    "href": "slides/ml/39-smoothing.html#connecting-smoothing-to-machine-learning-2",
    "title": "Smoothing",
    "section": "Connecting smoothing to machine learning",
    "text": "Connecting smoothing to machine learning\n\nWe later describe a popular machine learning algorithm, k-nearest neighbors, which is based on the concept of smoothing."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html#problem",
    "href": "slides/ml/37-evaluation-metrics.html#problem",
    "title": "Evaluation Metrics",
    "section": "Problem",
    "text": "Problem\n\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#signal-plus-noise-model-7",
    "href": "slides/ml/39-smoothing.html#signal-plus-noise-model-7",
    "title": "Smoothing",
    "section": "Signal plus noise model",
    "text": "Signal plus noise model\n\nWe therefore need an alternative, more flexible approach."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#kernels-7",
    "href": "slides/ml/39-smoothing.html#kernels-7",
    "title": "Smoothing",
    "section": "Kernels",
    "text": "Kernels\n\nMethods such as loess, which we explain next, improve on this."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold.\nAs a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\).\nHere we describe how local weighted regression (loess) permits us to consider larger window sizes.\nTo do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-1",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-1",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nTo see why this makes sense, consider the curved edges gardeners make using straight-edged spades:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-2",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-2",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear.\nWe can consider larger window sizes with the linear assumption than with a constant.\nInstead of the one-week window, we consider a larger one in which the trend is approximately linear.\nWe start with a three-week window and later consider and evaluate other options:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-3",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-3",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\n\nFor every point \\(x_0\\), loess defines a window and fits a line within that window.\nHere is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-4",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-4",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-5",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-5",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\)."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-6",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-6",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-7",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-7",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\n\ntotal_days &lt;- diff(range(polls_2008$day)) \nspan &lt;- 21/total_days \nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008) \npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt; \n  ggplot(aes(day, margin)) + \n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-8",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-8",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-9",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-9",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nDifferent spans give us different estimates.\nWe can see how different window sizes lead to different estimates:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-10",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-10",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-11",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-11",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\nHere are the final estimates:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-12",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-12",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-13",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-13",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\n3.\nloess has the option of fitting the local model robustly.\nAn iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration.\nTo use this option, we use the argument family=\"symmetric\"."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-14",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-14",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\\[\nW(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } W(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\n\nTo define the weights, we denote \\(2h\\) as the window size and define:\n\n\\[\nw_0(x_i) = W\\left(\\frac{x_i - x_0}{h}\\right)\n\\]\n\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-15",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-15",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#local-weighted-regression-16",
    "href": "slides/ml/39-smoothing.html#local-weighted-regression-16",
    "title": "Smoothing",
    "section": "Local weighted regression",
    "text": "Local weighted regression\n\n3.\nloess has the option of fitting the local model robustly.\nAn iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration.\nTo use this option, we use the argument family=\"symmetric\"."
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default",
    "href": "slides/ml/39-smoothing.html#beware-of-default",
    "title": "Smoothing",
    "section": "Beware of default",
    "text": "Beware of default\n\nggplot uses loess in its geom_smooth function:\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + \n  geom_point() +  \n  geom_smooth(method = loess)"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-1",
    "href": "slides/ml/39-smoothing.html#beware-of-default-1",
    "title": "Smoothing",
    "section": "Beware of default",
    "text": "Beware of default"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-2",
    "href": "slides/ml/39-smoothing.html#beware-of-default-2",
    "title": "Smoothing",
    "section": "Beware of default",
    "text": "Beware of default\n\nBut be careful with default parameters as they are rarely optimal.\nHowever, you can conveniently change them:\n\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + \n  geom_point() +  \n  geom_smooth(method = loess, method.args = list(span = 0.15, degree = 1))"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-3",
    "href": "slides/ml/39-smoothing.html#beware-of-default-3",
    "title": "Smoothing",
    "section": "Beware of default",
    "text": "Beware of default"
  },
  {
    "objectID": "slides/ml/39-smoothing.html#beware-of-default-4",
    "href": "slides/ml/39-smoothing.html#beware-of-default-4",
    "title": "Smoothing",
    "section": "Beware of default",
    "text": "Beware of default"
  },
  {
    "objectID": "slides/ml/36-intro-ml.html",
    "href": "slides/ml/36-intro-ml.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning has achieved remarkable successes in a variety of applications.\nThese range from the postal service’s use of machine learning for reading handwritten zip codes to the development of voice recognition systems."
  },
  {
    "objectID": "slides/highdim/34-distance.html",
    "href": "slides/highdim/34-distance.html",
    "title": "Distance",
    "section": "",
    "text": "Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance.\nMany machine learning techniques rely on defining distances between observations.\nClustering algorithms search of observations that are similar.\nBut what does this mean mathematically?"
  },
  {
    "objectID": "psets/pset-10-ml.html",
    "href": "psets/pset-10-ml.html",
    "title": "Problem set 10",
    "section": "",
    "text": "The data for this problem set is provided by this link: https://github.com/datasciencelabs/2024/raw/refs/heads/main/data/pset-10-mnist.rds\nRead this object into R. For example, you can use:\n\nfn &lt;- tempfile()\ndownload.file(\"https://github.com/datasciencelabs/2024/raw/refs/heads/main/data/pset-10-mnist.rds\", fn)\ndat &lt;- readRDS(fn)\nfile.remove(fn)\n\nThe object is a list with two components dat$train and dat$test. Use the data in dat$train to develop a machine learning algorithms to predict the labels for the images in the dat$test$images component.\nSave the your predicted labels in an object called digit_predictions. This should be a vector of integers with length nrow(dat$test$images). It is important that the digit_predictions is ordered to match the rows of dat$test$images.\nSave the object to a file called digit_predictions.rds using:\n\nsaveRDS(digit_predictions, file = \"digit_predictions.rds\")\n\nYou will submit:\n\nThe file digit_predictions.rds\nA quarto file that reproduces your analysis and provides brief explanations for your choices.\n\nIf your code reproduces the result, your grade will be your accuracy rounded up the closest integer. So,for example, if your accuracy is .993 your grade will be 100%.\nYou will have two opportunities to submit your predictions and see your accuracy before your submitting your final predictions."
  },
  {
    "objectID": "slides/ml/37-evaluation-metrics.html",
    "href": "slides/ml/37-evaluation-metrics.html",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Here we describe ways in which machine learning algorithms are evaluated.\nWe need to quantify what we mean when we say an algorithm performs better.\nWe demonstrate with a boring and simple example: how to predict sex using height."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#resampling-methods-math",
    "href": "slides/ml/41-resampling-methods.html#resampling-methods-math",
    "title": "Resampling Methods",
    "section": "Resampling methods math",
    "text": "Resampling methods math\n\nOur goal is to find the \\(\\lambda\\) that minimizes:\n\n\\[\n\\text{MSE}(\\lambda) = \\mbox{E}\\{[\\widehat{Y}(\\lambda) - Y]^2 \\}\n\\]\n\nA intuitive first attempt is the apparent error defined by:\n\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{N}\\sum_{i = 1}^N \\left\\{\\widehat{y}_i(\\lambda) - y_i\\right\\}^2\n\\]"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mathematical-description-of-resampling-methods",
    "href": "slides/ml/41-resampling-methods.html#mathematical-description-of-resampling-methods",
    "title": "Resampling Methods",
    "section": "Mathematical description of resampling methods",
    "text": "Mathematical description of resampling methods\n\nImagine a world in which repeat data collection.\nTake a large of number samples \\(B\\) define:\n\n\\[\n\\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left\\{\\widehat{y}_i^b(\\lambda) - y_i^b\\right\\}^2\n\\]\n\nLaw of large numbers says this is close to \\(MSE(\\lambda)\\)."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#resampling-methods-math-1",
    "href": "slides/ml/41-resampling-methods.html#resampling-methods-math-1",
    "title": "Resampling Methods",
    "section": "Resampling methods math",
    "text": "Resampling methods math\n\nBut this is just one realization of a random variable.\n\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{N}\\sum_{i = 1}^N \\left\\{\\widehat{y}_i(\\lambda) - y_i\\right\\}^2\n\\]\n\nCan we find a better estimate?"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#cross-validation",
    "href": "slides/ml/41-resampling-methods.html#cross-validation",
    "title": "Resampling Methods",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#cross-validation-1",
    "href": "slides/ml/41-resampling-methods.html#cross-validation-1",
    "title": "Resampling Methods",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#cross-validation-2",
    "href": "slides/ml/41-resampling-methods.html#cross-validation-2",
    "title": "Resampling Methods",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#k-fold-cross-validation",
    "href": "slides/ml/41-resampling-methods.html#k-fold-cross-validation",
    "title": "Resampling Methods",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\nRemember we are going to imitate this:\n\n\\[\n\\mbox{MSE}(\\lambda) \\approx\\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\widehat{y}_i^b(\\lambda) - y_i^b\\right)^2  \n\\]\n\nWe want to generate a dataset that can be thought of as independent random sample, and do this \\(B\\) times.\nThe K in K-fold cross validation, represents the number of time \\(B\\)."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-1",
    "href": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-1",
    "title": "Resampling Methods",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\nFor each sample we simply pick \\(M = N/B\\) observations at random and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b = 1\\).\nWe call this the validation set.\nNow we can fit the model in the training set, then compute the apparent error on the independent set:\n\n\\[\n\\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i = 1}^M \\left(\\widehat{y}_i^b(\\lambda) - y_i^b\\right)^2  \n\\]"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-2",
    "href": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-2",
    "title": "Resampling Methods",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\nIn K-fold cross validation, we randomly split the observations into \\(B\\) non-overlapping sets:"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-3",
    "href": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-3",
    "title": "Resampling Methods",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\nNow we repeat the calculation above for each of these sets \\(b = 1,\\dots,B\\) and obtain:\n\n\\[\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_B(\\lambda)\\]\n\nThen, for our final estimate, we compute the average:\n\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-4",
    "href": "slides/ml/41-resampling-methods.html#k-fold-cross-validation-4",
    "title": "Resampling Methods",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\nA final step is to select the \\(\\lambda\\) that minimizes the \\(\\hat{\\mbox{MSE}}(\\lambda)\\).\n\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#how-many-folds",
    "href": "slides/ml/41-resampling-methods.html#how-many-folds",
    "title": "Resampling Methods",
    "section": "How many folds?",
    "text": "How many folds?\n\nHow do we pick the cross validation fold?\nLarge values of \\(B\\) are preferable because the training data better imitates the original dataset.\nHowever, larger values of \\(B\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation.\nFor this reason, the choices of \\(B = 5\\) and \\(B = 10\\) are popular."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#how-many-folds-1",
    "href": "slides/ml/41-resampling-methods.html#how-many-folds-1",
    "title": "Resampling Methods",
    "section": "How many folds?",
    "text": "How many folds?\n\nOne way we can improve the variance of our final estimate is to take more samples.\nTo do this, we would no longer require the training set to be partitioned into non-overlapping sets.\nInstead, we would just pick \\(B\\) sets of some size at random."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm\n\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-1",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-1",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-2",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-2",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-3",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-3",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-4",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-4",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm\n\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-5",
    "href": "slides/ml/41-resampling-methods.html#estimate-mse-of-our-optimized-algorithm-5",
    "title": "Resampling Methods",
    "section": "Estimate MSE of our optimized algorithm",
    "text": "Estimate MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#boostrap-resampling",
    "href": "slides/ml/41-resampling-methods.html#boostrap-resampling",
    "title": "Resampling Methods",
    "section": "Boostrap resampling",
    "text": "Boostrap resampling\n\nTypically, cross-validation involves partitioning the original dataset into a training set to train the model and a testing set to evaluate it.\nWith the bootstrap approach you can create multiple different training datasets via bootstrapping.\nThis method is sometimes called bootstrap aggregating or bagging.\nIn bootstrap resampling, we create a large number of bootstrap samples from the original training dataset."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#boostrap-resampling-1",
    "href": "slides/ml/41-resampling-methods.html#boostrap-resampling-1",
    "title": "Resampling Methods",
    "section": "Boostrap resampling",
    "text": "Boostrap resampling\n\nEach bootstrap sample is created by randomly selecting observations with replacement, usually the same size as the original training dataset.\nFor each bootstrap sample, we fit the model and compute the MSE estimate on the observations not selected in the random sampling, referred to as the out-of-bag observations.\nThese out-of-bag observations serve a similar role to a validation set in standard cross-validation.\nWe then average the MSEs obtained in the out-of-bag observations."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#boostrap-resampling-2",
    "href": "slides/ml/41-resampling-methods.html#boostrap-resampling-2",
    "title": "Resampling Methods",
    "section": "Boostrap resampling",
    "text": "Boostrap resampling\n\nThis approach is actually the default approach in the caret package.\nWe describe how to implement resampling methods with the caret package next."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates",
    "href": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates",
    "title": "Resampling Methods",
    "section": "Comparison of MSE estimates",
    "text": "Comparison of MSE estimates"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates-1",
    "href": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates-1",
    "title": "Resampling Methods",
    "section": "Comparison of MSE estimates",
    "text": "Comparison of MSE estimates\n\nThe blue curve is the result of using 100 bootstrap samples to estimate MSE.\nThe variability is reduced even further, but at the cost of a 10 fold increase in computation time."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates-2",
    "href": "slides/ml/41-resampling-methods.html#comparison-of-mse-estimates-2",
    "title": "Resampling Methods",
    "section": "Comparison of MSE estimates",
    "text": "Comparison of MSE estimates"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#resampling-methods-math-2",
    "href": "slides/ml/41-resampling-methods.html#resampling-methods-math-2",
    "title": "Resampling Methods",
    "section": "Resampling methods math",
    "text": "Resampling methods math\n\nWe can’t do this in practice.\nBut we can try to immitate it."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm\n\nWe have described how to use cross validation to optimize parameters.\nHowever, we now have to take into account the fact that the optimization occurred on the training data and we therefore need an estimate of our final algorithm based on data that was not used to optimize the choice.\nHere is where we use the test set we separated early on."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-1",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-1",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-2",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-2",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm\n\nWe can actually do cross validation again:"
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-3",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-3",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm\n\nand obtain a final estimate of our expected loss.\nHowever, note that last cross validation iteration means that our entire compute time gets multiplied by \\(K\\).\nYou will soon learn that fitting each algorithm takes time because we are performing many complex computations.\nAs a result, we are always looking for ways to reduce this time.\nFor the final evaluation, we often just use the one test set."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-4",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-4",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm\n\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters."
  },
  {
    "objectID": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-5",
    "href": "slides/ml/41-resampling-methods.html#mse-of-our-optimized-algorithm-5",
    "title": "Resampling Methods",
    "section": "MSE of our optimized algorithm",
    "text": "MSE of our optimized algorithm"
  },
  {
    "objectID": "slides/ml/42-caret.html#the-caret-package",
    "href": "slides/ml/42-caret.html#the-caret-package",
    "title": "The caret package",
    "section": "The caret package",
    "text": "The caret package\n\nThere dozens of machine learning algorithms.\nMany of these algorithms are implemented in R.\nHowever, they are distributed via different packages, developed by different authors, and often use different syntax.\nThe caret package tries to consolidate these differences and provide consistency."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-caret-package-1",
    "href": "slides/ml/42-caret.html#the-caret-package-1",
    "title": "The caret package",
    "section": "The caret package",
    "text": "The caret package\n\nIt currently includes over 200 different methods which are summarized in the caret package manual."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-caret-package-2",
    "href": "slides/ml/42-caret.html#the-caret-package-2",
    "title": "The caret package",
    "section": "The caret package",
    "text": "The caret package\n\nWe use the 2 or 7 example to illustrate.\nThen we apply it to the larger MNIST dataset."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-train-function",
    "href": "slides/ml/42-caret.html#the-train-function",
    "title": "The caret package",
    "section": "The train function",
    "text": "The train function\n\nFunctions such as lm, glm, qda, lda, knn3, rpart and randomForrest use different syntax, have different argument names, and produce objects of different types.\nThe caret train function lets us train different algorithms using similar syntax."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-train-function-1",
    "href": "slides/ml/42-caret.html#the-train-function-1",
    "title": "The caret package",
    "section": "The train function",
    "text": "The train function\n\nFor example, we can type the following to train three different models:\n\n\nlibrary(dslabs)\nlibrary(caret) \ntrain_glm &lt;- train(y ~ ., method = \"glm\", data = mnist_27$train) \ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = mnist_27$train) \ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)"
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function",
    "href": "slides/ml/42-caret.html#the-predict-function",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nThe predict function is very useful for machine learning applications.\nHere is an example with regression:\n\n\nfit &lt;- lm(y ~ ., data = mnist_27$train) \np_hat &lt;- predict(fit, newdata = mnist_27$test)"
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-1",
    "href": "slides/ml/42-caret.html#the-predict-function-1",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nIn this case, the function is simply computing.\n\n\\[\n\\widehat{p}(\\mathbf{x}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2\n\\]\n\nfor the x_1 and x_2 in the test set mnist_27$test."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-2",
    "href": "slides/ml/42-caret.html#the-predict-function-2",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nWith these estimates in place, we can make our predictions and compute our accuracy:\n\n\ny_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))"
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-3",
    "href": "slides/ml/42-caret.html#the-predict-function-3",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\npredict does not always return objects of the same type\nit depends on what type of object it is applied to.\nTo learn about the specifics, you need to look at the help file specific for the type of fit object that is being used."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-4",
    "href": "slides/ml/42-caret.html#the-predict-function-4",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\npredict is actually a special type of function in R called a generic function.\nGeneric functions call other functions depending on what kind of object it receives.\nSo if predict receives an object coming out of the lm function, it will call predict.lm.\nIf it receives an object coming out of glm, it calls predict.glm.\nIf from knn3, it calls predict.knn3, and so on."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-5",
    "href": "slides/ml/42-caret.html#the-predict-function-5",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nThese functions are similar but not exactly.\n\n\n?predict.glm \n?predict.qda \n?predict.knn3 \n\n\nThere are many other versions of predict and many machine learning algorithms define their own predict function."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-6",
    "href": "slides/ml/42-caret.html#the-predict-function-6",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nAs with train, caret unifies the use of predict with the function predict.train.\nThis function takes the output of train and produces prediction of categories or estimates of \\(p(\\mathbf{x})\\)."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-7",
    "href": "slides/ml/42-caret.html#the-predict-function-7",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nThe code looks the same for all methods:\n\n\ny_hat_glm &lt;- predict(train_glm, mnist_27$test, type = \"raw\") \ny_hat_qda &lt;- predict(train_qda, mnist_27$test, type = \"raw\") \ny_hat_knn &lt;- predict(train_knn, mnist_27$test, type = \"raw\") \n\n\nThis permits us to quickly compare the algorithms."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-8",
    "href": "slides/ml/42-caret.html#the-predict-function-8",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nFor example, we can compare the accuracy like this:\n\n\nfits &lt;- list(glm = y_hat_glm, qda = y_hat_qda, knn = y_hat_knn) \nsapply(fits, function(fit){\n  confusionMatrix(fit, mnist_27$test$y)$overall[[\"Accuracy\"]]\n})\n\n  glm   qda   knn \n0.775 0.815 0.835"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling",
    "href": "slides/ml/42-caret.html#resampling",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nWhen an algorithm includes a tuning parameter, train automatically uses a resampling method to estimate MSE and decide among a few default candidate values.\nTo find out what parameter or parameters are optimized, you can read the caret manual."
  },
  {
    "objectID": "slides/ml/42-caret.html#the-predict-function-9",
    "href": "slides/ml/42-caret.html#the-predict-function-9",
    "title": "The caret package",
    "section": "The predict function",
    "text": "The predict function\n\nOr study the output of:\n\n\nmodelLookup(\"knn\") \n\n\nTo obtain all the details of how caret implements kNN you can use:\n\n\ngetModelInfo(\"knn\")"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-1",
    "href": "slides/ml/42-caret.html#resampling-1",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nIf we run it with default values:\n\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train) \n\n\nyou can quickly see the results of the cross validation using the ggplot function."
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-2",
    "href": "slides/ml/42-caret.html#resampling-2",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nggplot(train_knn, highlight = TRUE) \n\n\nThe argument highlight highlights the max."
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-3",
    "href": "slides/ml/42-caret.html#resampling-3",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nBy default, the resampling is performed by taking 25 bootstrap samples, each comprised of 25% of the observations.\nWe change this using the trControl argument. More on this later.\nFor the kNN method, the default is to try \\(k=5,7,9\\).\nWe change this using the tuneGrid argument."
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-4",
    "href": "slides/ml/42-caret.html#resampling-4",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nLet’s try seq(5, 101, 2).\nSince we are fitting \\(49 \\times 25 = 1225\\) kNN models, running this code will take several seconds.\n\n\nset.seed(2003)\ntrain_knn &lt;- train(y ~ ., method = \"knn\",  \n                   data = mnist_27$train, \n                   tuneGrid = data.frame(k = seq(5, 101, 2)))"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-5",
    "href": "slides/ml/42-caret.html#resampling-5",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nggplot(train_knn, highlight = TRUE)"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-6",
    "href": "slides/ml/42-caret.html#resampling-6",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nTo access the parameter that maximized the accuracy, you can use this:\n\n\ntrain_knn$bestTune \n\n    k\n26 55\n\n\n\nand the best performing model like this:\n\n\ntrain_knn$finalModel \n\n55-nearest neighbor model\nTraining set outcome distribution:\n\n  2   7 \n401 399"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-7",
    "href": "slides/ml/42-caret.html#resampling-7",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nThe function predict will use this best performing model.\nHere is the accuracy of the best model when applied to the test set, which we have not yet used because the cross validation was done on the training set:\n\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"), \n                mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n   0.825"
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-8",
    "href": "slides/ml/42-caret.html#resampling-8",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nBootstrapping is not always the best approach to resampling.\nIf we want to change our resampling method, we can use the trainControl function.\nFor example, the code below runs 10-fold cross validation."
  },
  {
    "objectID": "slides/ml/42-caret.html#resampling-9",
    "href": "slides/ml/42-caret.html#resampling-9",
    "title": "The caret package",
    "section": "Resampling",
    "text": "Resampling\n\nWe accomplish this using the following code:\n\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9) \ntrain_knn_cv &lt;- train(y ~ ., method = \"knn\",  \n                   data = mnist_27$train, \n                   tuneGrid = data.frame(k = seq(1, 71, 2)), \n                   trControl = control)"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing",
    "href": "slides/ml/42-caret.html#preprocessing",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nNow let’s move on to the MNIST digits.\n\n\nlibrary(dslabs) \nmnist &lt;- read_mnist()"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-1",
    "href": "slides/ml/42-caret.html#preprocessing-1",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nThe dataset includes two components:\n\n\nnames(mnist) \n\n[1] \"train\" \"test\""
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-2",
    "href": "slides/ml/42-caret.html#preprocessing-2",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nEach of these components includes a matrix with features in the columns:\n\n\ndim(mnist$train$images) \n\n[1] 60000   784\n\n\n\nand vector with the classes as integers:\n\n\nclass(mnist$train$labels) \n\n[1] \"integer\"\n\ntable(mnist$train$labels) \n\n\n   0    1    2    3    4    5    6    7    8    9 \n5923 6742 5958 6131 5842 5421 5918 6265 5851 5949"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-3",
    "href": "slides/ml/42-caret.html#preprocessing-3",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nBecause we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset.\nWe will sample 10,000 random rows from the training set and 1,000 random rows from the test set:\n\n\nset.seed(1990) \nindex &lt;- sample(nrow(mnist$train$images), 10000) \nx &lt;- mnist$train$images[index,] \ny &lt;- factor(mnist$train$labels[index]) \nindex &lt;- sample(nrow(mnist$test$images), 1000) \nx_test &lt;- mnist$test$images[index,] \ny_test &lt;- factor(mnist$test$labels[index])"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-4",
    "href": "slides/ml/42-caret.html#preprocessing-4",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nWhen fitting models to large datasets, we recommend using matrices instead of data frames, as matrix operations tend to be faster.\nIf the matrices lack column names, you can assign names based on their position:\n\n\ncolnames(x) &lt;- 1:ncol(mnist$train$images) \ncolnames(x_test) &lt;- colnames(x)"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-5",
    "href": "slides/ml/42-caret.html#preprocessing-5",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nWe often transform predictors before running the machine algorithm.\nWe also remove predictors that are clearly not useful.\nWe call these steps preprocessing.\nExamples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation."
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-6",
    "href": "slides/ml/42-caret.html#preprocessing-6",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-7",
    "href": "slides/ml/42-caret.html#preprocessing-7",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nThe caret packages includes a function that recommends features to be removed due to near zero variance:\n\n\nnzv &lt;- nearZeroVar(x)"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-8",
    "href": "slides/ml/42-caret.html#preprocessing-8",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nWe can see the columns recommended for removal are the near the edges:\n\n\nimage(matrix(1:784 %in% nzv, 28, 28))"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-9",
    "href": "slides/ml/42-caret.html#preprocessing-9",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nSo we end up removing\n\n\nlength(nzv) \n\n[1] 532\n\n\npredictors."
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-10",
    "href": "slides/ml/42-caret.html#preprocessing-10",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nThe caret package features the preProcess function, which allows users to establish a predefined set of preprocessing operations based on a training set.\nThis function is designed to apply these operations to new datasets without recalculating anything on the test set, ensuring that all preprocessing steps are consistent and derived solely from the training data."
  },
  {
    "objectID": "slides/ml/42-caret.html#knn",
    "href": "slides/ml/42-caret.html#knn",
    "title": "The caret package",
    "section": "kNN",
    "text": "kNN\n\nThe first step is to optimize for \\(k\\).\n\n\ntrain_knn &lt;- train(x, y, method = \"knn\",  \n                   preProcess = \"nzv\", \n                   trControl = trainControl(\"cv\", number = 20, p = 0.95), \n                   tuneGrid = data.frame(k = seq(1, 7, 2)))"
  },
  {
    "objectID": "slides/ml/42-caret.html#knn-1",
    "href": "slides/ml/42-caret.html#knn-1",
    "title": "The caret package",
    "section": "kNN",
    "text": "kNN\n\nOnce we optimize our algorithm, the predict function defaults to using the best performing algorithm fit with the entire training data:\n\n\ny_hat_knn &lt;- predict(train_knn, x_test, type = \"raw\")"
  },
  {
    "objectID": "slides/ml/42-caret.html#knn-2",
    "href": "slides/ml/42-caret.html#knn-2",
    "title": "The caret package",
    "section": "kNN",
    "text": "kNN\n\nWe achieve relatively high accuracy:\n\n\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"] \n\nAccuracy \n   0.952"
  },
  {
    "objectID": "slides/ml/40-knn.html#motivation",
    "href": "slides/ml/40-knn.html#motivation",
    "title": "k-nearest neighbors (knn)",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/ml/40-knn.html#motivation-1",
    "href": "slides/ml/40-knn.html#motivation-1",
    "title": "k-nearest neighbors (knn)",
    "section": "Motivation",
    "text": "Motivation\n\nWe are interested in estimating the conditional probability function:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid X_1 = x_1, \\dots, X_{784} = x_{784}).\n\\]"
  },
  {
    "objectID": "slides/ml/40-knn.html#simpler-example",
    "href": "slides/ml/40-knn.html#simpler-example",
    "title": "k-nearest neighbors (knn)",
    "section": "Simpler example",
    "text": "Simpler example"
  },
  {
    "objectID": "slides/ml/40-knn.html#conditional-probability-function",
    "href": "slides/ml/40-knn.html#conditional-probability-function",
    "title": "k-nearest neighbors (knn)",
    "section": "Conditional Probability Function",
    "text": "Conditional Probability Function"
  },
  {
    "objectID": "slides/ml/40-knn.html#motivation-2",
    "href": "slides/ml/40-knn.html#motivation-2",
    "title": "k-nearest neighbors (knn)",
    "section": "Motivation",
    "text": "Motivation\n\nWe are interested in estimating:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2).\n\\]"
  },
  {
    "objectID": "slides/ml/40-knn.html#training-set",
    "href": "slides/ml/40-knn.html#training-set",
    "title": "k-nearest neighbors (knn)",
    "section": "Training set",
    "text": "Training set\n\nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point(alpha=.75)"
  },
  {
    "objectID": "slides/ml/40-knn.html#test-set",
    "href": "slides/ml/40-knn.html#test-set",
    "title": "k-nearest neighbors (knn)",
    "section": "Test set",
    "text": "Test set\n\nmnist_27$test |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point(alpha = .75)"
  },
  {
    "objectID": "slides/ml/40-knn.html#motivation-3",
    "href": "slides/ml/40-knn.html#motivation-3",
    "title": "k-nearest neighbors (knn)",
    "section": "Motivation",
    "text": "Motivation\n\nWith kNN we estimate \\(p(\\mathbf{x})\\) using smoothing.\nWe define the distance between all observations based on the features.\nFor any \\(\\mathbf{x}_0\\), we estimate \\(p(\\mathbf{x})\\) by identifying the \\(k\\) nearest points to \\(mathbf{x}_0\\) and taking an average of the \\(y\\)s associated with these points.\nWe refer to the set of points used to compute the average as the neighborhood.\nThis gives us \\(\\widehat{p}(\\mathbf{x}_0)\\)."
  },
  {
    "objectID": "slides/ml/40-knn.html#motivation-4",
    "href": "slides/ml/40-knn.html#motivation-4",
    "title": "k-nearest neighbors (knn)",
    "section": "Motivation",
    "text": "Motivation\n\nAs with bin smoothers, we can control the flexibility of our estimate through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and wiggly estimates.\nTo implement the algorithm, we can use the knn3 function from the caret package.\nLooking at the help file for this package, we see that we can call it in one of two ways.\nWe will use the first way in which we specify a formula and a data frame."
  },
  {
    "objectID": "slides/ml/40-knn.html#lets-try-it",
    "href": "slides/ml/40-knn.html#lets-try-it",
    "title": "k-nearest neighbors (knn)",
    "section": "Let’s try it",
    "text": "Let’s try it\n\nlibrary(dslabs) \nlibrary(caret) \nknn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)"
  },
  {
    "objectID": "slides/ml/40-knn.html#knn",
    "href": "slides/ml/40-knn.html#knn",
    "title": "k-nearest neighbors (knn)",
    "section": "kNN",
    "text": "kNN\nThis gives us \\(\\widehat{p}(\\mathbf{x})\\):\n\np_hat_knn &lt;- predict(knn_fit, mnist_27$test)[,2]\n\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\") \nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n   0.815 \n\n\n\nWe see that kNN, with the default parameter, already beats regression."
  },
  {
    "objectID": "slides/ml/40-knn.html#over-training",
    "href": "slides/ml/40-knn.html#over-training",
    "title": "k-nearest neighbors (knn)",
    "section": "Over-training",
    "text": "Over-training\nCompare\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = \"class\") \nconfusionMatrix(y_hat_knn, mnist_27$train$y)$overall[\"Accuracy\"] \n\nAccuracy \n    0.86 \n\n\nto\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\") \nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n   0.815"
  },
  {
    "objectID": "slides/ml/40-knn.html#over-training-1",
    "href": "slides/ml/40-knn.html#over-training-1",
    "title": "k-nearest neighbors (knn)",
    "section": "Over-training",
    "text": "Over-training\nCompare this:\n\nknn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1) \ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = \"class\") \nconfusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[\"Accuracy\"]] \n\n[1] 0.994\n\n\nto this:\n\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = \"class\") \nconfusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n    0.81 \n\n\n\nWe can see the over-fitting problem by plotting the decision rule boundaries produced by \\(p(\\mathbf{x})\\):"
  },
  {
    "objectID": "slides/ml/40-knn.html#over-training-2",
    "href": "slides/ml/40-knn.html#over-training-2",
    "title": "k-nearest neighbors (knn)",
    "section": "Over-training",
    "text": "Over-training"
  },
  {
    "objectID": "slides/ml/40-knn.html#over-smoothing",
    "href": "slides/ml/40-knn.html#over-smoothing",
    "title": "k-nearest neighbors (knn)",
    "section": "Over-smoothing",
    "text": "Over-smoothing\nLet’s try a much bigger neighborhood:\n\nknn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401) \ny_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = \"class\") \nconfusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n    0.76"
  },
  {
    "objectID": "slides/ml/40-knn.html#over-smoothing-1",
    "href": "slides/ml/40-knn.html#over-smoothing-1",
    "title": "k-nearest neighbors (knn)",
    "section": "Over-smoothing",
    "text": "Over-smoothing"
  },
  {
    "objectID": "slides/ml/40-knn.html#parameter-tuning",
    "href": "slides/ml/40-knn.html#parameter-tuning",
    "title": "k-nearest neighbors (knn)",
    "section": "Parameter tuning",
    "text": "Parameter tuning"
  },
  {
    "objectID": "slides/ml/40-knn.html#parameter-tuning-1",
    "href": "slides/ml/40-knn.html#parameter-tuning-1",
    "title": "k-nearest neighbors (knn)",
    "section": "Parameter tuning",
    "text": "Parameter tuning"
  },
  {
    "objectID": "slides/ml/40-knn.html#knn-1",
    "href": "slides/ml/40-knn.html#knn-1",
    "title": "k-nearest neighbors (knn)",
    "section": "kNN",
    "text": "kNN\n\nTo see why this is the case, we plot \\(\\widehat{p}(\\mathbf{x})\\) and compare it to the true conditional probability \\(p(\\mathbf{x})\\):"
  },
  {
    "objectID": "slides/ml/42-caret.html#preprocessing-11",
    "href": "slides/ml/42-caret.html#preprocessing-11",
    "title": "The caret package",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nBelow is an example demonstrating how to remove predictors with near-zero variance and then center the remaining predictors:\n\n\npp &lt;- preProcess(x, method = c(\"nzv\", \"center\")) \ncentered_subsetted_x_test &lt;- predict(pp, newdata = x_test) \ndim(centered_subsetted_x_test) \n\n[1] 1000  252\n\n\n\nAdditionally, the train function in caret includes a preProcess argument that allows users to specify which preprocessing steps to apply automatically during model training."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#examples-of-algorithms",
    "href": "slides/ml/43-algorithms.html#examples-of-algorithms",
    "title": "Algorithms",
    "section": "Examples of algorithms",
    "text": "Examples of algorithms\n\nThere are hundreds of machine learning algorithms.\nHere we provide a few examples spanning rather different approaches.\nWe will use the mnist_27 dataset from dslabs.\n\n\nlibrary(tidyverse) \nlibrary(caret) \nlibrary(dslabs)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression",
    "href": "slides/ml/43-algorithms.html#logistic-regression",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nWe previously used linear regression to predict classes by fitting the model:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =  \n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\nWe used least squares after assigning numeric values of 0 and 1 to \\(y\\), and used lm as if the data were continuous."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression-1",
    "href": "slides/ml/43-algorithms.html#logistic-regression-1",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nAn obvious problem with this approach is that \\(\\widehat{p}(\\mathbf{x})\\) can be negative and larger than 1:\n\n\nfit_lm &lt;- lm(y ~ x_1 + x_2, \n             data = mutate(mnist_27$train,y = ifelse(y == 7, 1, 0))) \nrange(fit_lm$fitted) \n\n[1] -0.22  1.92"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression-2",
    "href": "slides/ml/43-algorithms.html#logistic-regression-2",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nTo avoid this, we can apply an approach more appropriate for binary data:\n\n\\[\n\\log \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression-3",
    "href": "slides/ml/43-algorithms.html#logistic-regression-3",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\nWe can use the glm function to fit the model:\n\nfit_glm &lt;- glm(y ~ x_1 + x_2, data = mnist_27$train, family = \"binomial\") \np_hat_glm &lt;- predict(fit_glm, mnist_27$test, type = \"response\") \ny_hat_glm &lt;- factor(ifelse(p_hat_glm &gt; 0.5, 7, 2)) \nconfusionMatrix(y_hat_glm, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n   0.775 \n\n\n\nWe see that logistic regression performs similarly to regression."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression-4",
    "href": "slides/ml/43-algorithms.html#logistic-regression-4",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nThis is not surprising given that the estimate of \\(\\widehat{p}(\\mathbf{x})\\) looks similar:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#k-nearest-neighbors",
    "href": "slides/ml/43-algorithms.html#k-nearest-neighbors",
    "title": "Algorithms",
    "section": "k-nearest neighbors",
    "text": "k-nearest neighbors"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#generative-models",
    "href": "slides/ml/43-algorithms.html#generative-models",
    "title": "Algorithms",
    "section": "Generative models",
    "text": "Generative models\n\nWith binary outcomes the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid \\mathbf{X}=\\mathbf{x})  \n\\]\n\nWe have described approaches to estimating \\(p(\\mathbf{x})\\)."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#generative-models-1",
    "href": "slides/ml/43-algorithms.html#generative-models-1",
    "title": "Algorithms",
    "section": "Generative models",
    "text": "Generative models\n\nIn all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors.\nThese are referred to as discriminative approaches.\nHowever, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful.\nMethods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models\nWe model how \\(Y\\) and \\(\\mathbf{X}\\) are generated."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#naive-bayes",
    "href": "slides/ml/43-algorithms.html#naive-bayes",
    "title": "Algorithms",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nRecall that Bayes rule tells us that we can rewrite \\(p(\\mathbf{x})\\) as follows:\n\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\mbox{Pr}(Y = 1)}\n{ f_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\mbox{Pr}(Y = 0)  + f_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\mbox{Pr}(Y = 1) }\n\\]\n\nwith \\(f_{\\mathbf{X}|Y = 1}\\) and \\(f_{\\mathbf{X}|Y = 0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y = 1\\) and \\(Y = 0\\)."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#controlling-prevalence",
    "href": "slides/ml/43-algorithms.html#controlling-prevalence",
    "title": "Algorithms",
    "section": "Controlling prevalence",
    "text": "Controlling prevalence\n\nOne useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence.\nUsing our sample, we estimate \\(f_{X|Y = 1}\\), \\(f_{X|Y = 0}\\) and \\(\\pi\\).\nIf we use hats to denote the estimates, we can write \\(\\widehat{p}(x)\\) as:\n\n\\[\n\\widehat{p}(x)= \\frac{\\widehat{f}_{X|Y = 1}(x) \\widehat{\\pi}}\n{ \\widehat{f}_{X|Y = 0}(x)(1-\\widehat{\\pi}) + \\widehat{f}_{X|Y = 1}(x)\\widehat{\\pi} }\n\\]\n\nWe can change prevalence by pluggin in other values instead of \\(\\widehat{\\pi}\\)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#naive-bayes-1",
    "href": "slides/ml/43-algorithms.html#naive-bayes-1",
    "title": "Algorithms",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nIf we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule.\nHowever, this is a big if.\nWhen \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution, Naive Bayes will be practically impossible to implement.\nHowever, with a small number of predictors and many categories generative models can be quite powerful."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nQuadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y = 1}(x)\\) and \\(p_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\) are multivariate normal.\nThe simple example we described in the previous section is actually QDA.\nLet’s now look at a slightly more complicated case: the 2 or 7 example.\nIn this example, we have two predictors so we assume each one is bivariate normal."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-1",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-1",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nWe can estiamte the paramters from the data:\n\n\nparams &lt;- mnist_27$train |&gt;  \n  group_by(y) |&gt;  \n  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2),  \n            sd_1= sd(x_1), sd_2 = sd(x_2),  \n            r = cor(x_1, x_2)) \nparams \n\n# A tibble: 2 × 6\n  y     avg_1 avg_2   sd_1   sd_2     r\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2     0.136 0.287 0.0670 0.0600 0.415\n2 7     0.238 0.290 0.0745 0.104  0.468"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-2",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-2",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nWith these estimates in place, all we need are the prevalence \\(\\pi\\) to compute:\n\n\\[\n\\widehat{p}(\\mathbf{x})= \\frac{\\widehat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\widehat{\\pi}}\n{ \\widehat{f}_{\\mathbf{X}|Y = 0}(x)(1-\\widehat{\\pi}) + \\widehat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\widehat{\\pi} }\n\\]"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-3",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-3",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\nHere is the fitted model:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-4",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-4",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nWe can fit QDA using the qda function the MASS package:\n\n\ntrain_qda &lt;- MASS::qda(y ~ ., data = mnist_27$train) \ny_hat &lt;- predict(train_qda, mnist_27$test)$class \n\n\nWe see that we obtain relatively good accuracy:\n\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]  \n\nAccuracy \n   0.815"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-5",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-5",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nThe conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-6",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-6",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nOne reason QDA does not work as well as the kernel methods is because the assumption of normality does not quite hold:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-7",
    "href": "slides/ml/43-algorithms.html#quadratic-discriminant-analysis-7",
    "title": "Algorithms",
    "section": "Quadratic discriminant analysis",
    "text": "Quadratic discriminant analysis\n\nQDA can work well here, but it becomes harder to use as the number of predictors increases.\nHere we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations.\nNotice that if we have 10 predictors, we have 45 correlations for each class.\nThe formula is \\(K\\times p(p-1)/2\\), which gets big fast.\nOnce the number of parameters approaches the size of our data, the method becomes impractical due to overfitting."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#linear-discriminant-analysis",
    "href": "slides/ml/43-algorithms.html#linear-discriminant-analysis",
    "title": "Algorithms",
    "section": "Linear discriminant analysis",
    "text": "Linear discriminant analysis\n\nA relatively simple solution to QDA’s problem of having too many parameters is to assume that the correlation structure is the same for all classes.\nThis reduces the number of parameters we need to estimate."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#linear-discriminant-analysis-1",
    "href": "slides/ml/43-algorithms.html#linear-discriminant-analysis-1",
    "title": "Algorithms",
    "section": "Linear discriminant analysis",
    "text": "Linear discriminant analysis\nThe estimated distribution look like this:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#linear-discriminant-analysis-2",
    "href": "slides/ml/43-algorithms.html#linear-discriminant-analysis-2",
    "title": "Algorithms",
    "section": "Linear discriminant analysis",
    "text": "Linear discriminant analysis\n\nWe can LDA using the MASS lda function:\n\n\nThis added constraint lowers the number of parameters, the rigidity lowers our accuracy to:\n\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"] \n\nAccuracy \n   0.775"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#linear-discriminant-analysis-3",
    "href": "slides/ml/43-algorithms.html#linear-discriminant-analysis-3",
    "title": "Algorithms",
    "section": "Linear discriminant analysis",
    "text": "Linear discriminant analysis\nIn fact we can show that the boundary is a line:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#connection-to-distance",
    "href": "slides/ml/43-algorithms.html#connection-to-distance",
    "title": "Algorithms",
    "section": "Connection to distance",
    "text": "Connection to distance\n\nThe normal density is:\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\}\n\\]\n\nIf we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get:\n\n\\[\n\\frac{(x-\\mu)^2}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#classification-and-regression-trees-cart",
    "href": "slides/ml/43-algorithms.html#classification-and-regression-trees-cart",
    "title": "Algorithms",
    "section": "Classification and regression trees (CART)",
    "text": "Classification and regression trees (CART)\n\nAnything based on distance or distributions will face the course of dimensionality\nIn high dimensions the nearest neighbors will actually define a large region.\nThis makes it hard to estimate local non-linearities.\nRegression trees use a completely different approach: directly partition the prediction space."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation",
    "href": "slides/ml/43-algorithms.html#cart-motivation",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nTo motivate this section, we will use a new dataset:\n\n\nnames(olive) \n\n [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\" \"stearic\"    \n [6] \"oleic\"       \"linoleic\"    \"linolenic\"   \"arachidic\"   \"eicosenoic\""
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-1",
    "href": "slides/ml/43-algorithms.html#cart-motivation-1",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nWe try to predict the region using the fatty acid composition:\n\n\ntable(olive$region) \n\n\nNorthern Italy       Sardinia Southern Italy \n           151             98            323 \n\n\n\nWe remove the area column because we won’t use it as a predictor.\n\n\nolive &lt;- select(olive, -area)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-2",
    "href": "slides/ml/43-algorithms.html#cart-motivation-2",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nUsing kNN, we can achieve the following accuracy:\n\n\nlibrary(caret) \nfit &lt;- train(region ~ .,  method = \"knn\",  \n             tuneGrid = data.frame(k = seq(1, 15, 2)),  \n             data = olive) \nfit$results[1,2]\n\n[1] 0.969"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-3",
    "href": "slides/ml/43-algorithms.html#cart-motivation-3",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nHowever, a bit of data exploration reveals that we should be able to do even better:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-4",
    "href": "slides/ml/43-algorithms.html#cart-motivation-4",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nWe should be able to predict perfectly:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-5",
    "href": "slides/ml/43-algorithms.html#cart-motivation-5",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees",
    "href": "slides/ml/43-algorithms.html#regression-trees",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nWhen using trees, and the outcome is continuous, we call the approach a regression tree.\nTo introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms.\nAs with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mbox{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-1",
    "href": "slides/ml/43-algorithms.html#regression-trees-1",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-2",
    "href": "slides/ml/43-algorithms.html#regression-trees-2",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\nThis fits the model:\n\nlibrary(rpart) \nfit &lt;- rpart(margin ~ ., data = polls_2008) \n\nThere are rules to decide when to stop."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-3",
    "href": "slides/ml/43-algorithms.html#regression-trees-3",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nplot(fit, margin = 0.1) \ntext(fit, cex = 0.75)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-4",
    "href": "slides/ml/43-algorithms.html#regression-trees-4",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-5",
    "href": "slides/ml/43-algorithms.html#regression-trees-5",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nIf we let it go to the end we get:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-6",
    "href": "slides/ml/43-algorithms.html#regression-trees-6",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nPicking the parameters that controls when to stop:\n\n\nlibrary(caret) \ntrain_rpart &lt;- train(margin ~ .,  \n                     method = \"rpart\", \n                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)), \n                     data = polls_2008)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-7",
    "href": "slides/ml/43-algorithms.html#regression-trees-7",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-8",
    "href": "slides/ml/43-algorithms.html#regression-trees-8",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nWe can also prune:\n\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, \n             control = rpart.control(cp = 0)) \npruned_fit &lt;- prune(fit, cp = 0.01)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#regression-trees-9",
    "href": "slides/ml/43-algorithms.html#regression-trees-9",
    "title": "Algorithms",
    "section": "Regression trees",
    "text": "Regression trees\n\nWe can also prune:\n\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0)) \npruned_fit &lt;- prune(fit, cp = 0.01)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#classification-decision-trees",
    "href": "slides/ml/43-algorithms.html#classification-decision-trees",
    "title": "Algorithms",
    "section": "Classification (decision) trees",
    "text": "Classification (decision) trees\n\nApply it to 2 or 7 data:\n\n\ntrain_rpart &lt;- train(y ~ ., \n                     method = \"rpart\", \n                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)), \n                     data = mnist_27$train) \ny_hat &lt;- predict(train_rpart, mnist_27$test) \nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n    0.81"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#classification-decision-trees-1",
    "href": "slides/ml/43-algorithms.html#classification-decision-trees-1",
    "title": "Algorithms",
    "section": "Classification (decision) trees",
    "text": "Classification (decision) trees\nHere is the estimate of \\(p(\\mathbf{x})\\):"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests",
    "href": "slides/ml/43-algorithms.html#random-forests",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\n\nApply it to the polls data:\n\n\nlibrary(randomForest) \nfit &lt;- randomForest(margin ~ ., data = polls_2008)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-1",
    "href": "slides/ml/43-algorithms.html#random-forests-1",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nHow many trees?\n\nrafalib::mypar() \nplot(fit)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-2",
    "href": "slides/ml/43-algorithms.html#random-forests-2",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\n\nThe resulting estimate is obtained with:\n\n\ny_hat &lt;-  predict(fit, newdata = polls_2008)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-3",
    "href": "slides/ml/43-algorithms.html#random-forests-3",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nFinal estimate:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-4",
    "href": "slides/ml/43-algorithms.html#random-forests-4",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-5",
    "href": "slides/ml/43-algorithms.html#random-forests-5",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nApply it to 2 or 7 data:\n\nlibrary(randomForest) \ntrain_rf &lt;- randomForest(y ~ ., data = mnist_27$train) \nconfusionMatrix(predict(train_rf, mnist_27$test), \n                mnist_27$test$y)$overall[\"Accuracy\"]\n\nAccuracy \n    0.82"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-6",
    "href": "slides/ml/43-algorithms.html#random-forests-6",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nHere is the estimate of \\(p(\\mathbf{x})\\):"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-7",
    "href": "slides/ml/43-algorithms.html#random-forests-7",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nWe increase smoothness with the nodesize parameter:\n\ntrain_rf_31 &lt;- randomForest(y ~ ., data = mnist_27$train, nodesize = 31)"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-8",
    "href": "slides/ml/43-algorithms.html#random-forests-8",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nThis provides a better estimate:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#random-forests-9",
    "href": "slides/ml/43-algorithms.html#random-forests-9",
    "title": "Algorithms",
    "section": "Random forests",
    "text": "Random forests\nThis provides a better estimate:"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#logistic-regression-5",
    "href": "slides/ml/43-algorithms.html#logistic-regression-5",
    "title": "Algorithms",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nJust like regression, the decision rule is a line:\n\n\\[\ng^{-1}(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2) = 0.5 \\implies\\\\\n\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2 = g(0.5) = 0 \\implies \\\\\nx_2 = -\\widehat{\\beta}_0/\\widehat{\\beta}_2 -\\widehat{\\beta}_1/\\widehat{\\beta}_2 x_1\n\\]"
  },
  {
    "objectID": "slides/ml/43-algorithms.html#connection-to-distance-1",
    "href": "slides/ml/43-algorithms.html#connection-to-distance-1",
    "title": "Algorithms",
    "section": "Connection to distance",
    "text": "Connection to distance\n\nNote this ithe negative of a distance squared scaled by the standard deviation.\n\n\\[\n\\frac{(x-\\mu)^2}{\\sigma^2}\n\\]\n\nFor higher dimensions, the same is true except the scaling is more complex and involves correlations."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart",
    "href": "slides/ml/43-algorithms.html#cart",
    "title": "Algorithms",
    "section": "CART",
    "text": "CART\n\nAnything based on distance or distributions will face the course of dimensionality\nIn high dimensions the nearest neighbors will actually define a large region.\nThis makes it hard to estimate local non-linearities.\nRegression trees use a completely different approach: directly partition the prediction space."
  },
  {
    "objectID": "slides/ml/43-algorithms.html#cart-motivation-6",
    "href": "slides/ml/43-algorithms.html#cart-motivation-6",
    "title": "Algorithms",
    "section": "CART motivation",
    "text": "CART motivation\n\nDecision trees like this are often used in practice."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice",
    "href": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice",
    "title": "ML In Practice",
    "section": "Machine learning in practice",
    "text": "Machine learning in practice\n\nNow that we have learned several methods and explored them with simple examples, we will try them out on a real example: the MNIST digits.\nWe can load this data using the following dslabs package:\n\n\nlibrary(dslabs) \nmnist &lt;- read_mnist()"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice-1",
    "href": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice-1",
    "title": "ML In Practice",
    "section": "Machine learning in practice",
    "text": "Machine learning in practice\n\nWe will sample 10,000 random rows from the training set and 1,000 random rows from the test set:\n\n\nset.seed(1990) \n\nindex &lt;- sample(nrow(mnist$train$images), 10000) \nx &lt;- mnist$train$images[index,] \ny &lt;- factor(mnist$train$labels[index]) \n\nindex &lt;- sample(nrow(mnist$test$images), 1000) \nx_test &lt;- mnist$test$images[index,] \ny_test &lt;- factor(mnist$test$labels[index]) \n\n\nNote we make the outcomes factors."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice-2",
    "href": "slides/ml/44-ml-in-practice.html#machine-learning-in-practice-2",
    "title": "ML In Practice",
    "section": "Machine learning in practice",
    "text": "Machine learning in practice\n\nAssing names to columns for caret.\n\n\ncolnames(x) &lt;- 1:ncol(mnist$train$images) \ncolnames(x_test) &lt;- colnames(x)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#parallelization",
    "href": "slides/ml/44-ml-in-practice.html#parallelization",
    "title": "ML In Practice",
    "section": "Parallelization",
    "text": "Parallelization\n\nDuring cross-validation or bootstrapping, the process of fitting models to different samples or using varying parameters can be performed independently.\nImagine you are fitting 100 models; if you had access to 100 computers, you could theoretically speed up the process by a factor of 100 by fitting each model on a separate computer and then aggregating the results.\nMost modern computers, are equipped with multiple processors that allow for such parallel execution."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#parallelization-1",
    "href": "slides/ml/44-ml-in-practice.html#parallelization-1",
    "title": "ML In Practice",
    "section": "Parallelization",
    "text": "Parallelization\n\nThe caret package is set up to run in parallel but you have to let R know that you are want to parallelize your work.\nTo do this we can use the doParallel package:\n\n\nlibrary(doParallel) \nnc &lt;- detectCores() - 1 # it is convention to leave 1 core for OS \ncl &lt;- makeCluster(nc) \nregisterDoParallel(cl)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#parallelization-2",
    "href": "slides/ml/44-ml-in-practice.html#parallelization-2",
    "title": "ML In Practice",
    "section": "Parallelization",
    "text": "Parallelization\n\nIf you do use parallelization, make sure to let R know you are done with the following lines of code:\n\n\nstopCluster(cl) \nstopImplicitCluster()"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors",
    "href": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors",
    "title": "ML In Practice",
    "section": "k-nearest neighbors",
    "text": "k-nearest neighbors\n\nWe therefore us cross-validation to estimate our MSE.\nThe first step is to optimize for \\(k\\).\n\n\ntrain_knn &lt;- train(x, y, method = \"knn\",  \n                   preProcess = \"nzv\", \n                   trControl = trainControl(\"cv\", \n                                            number = 20, \n                                            p = 0.95), \n                   tuneGrid = data.frame(k = seq(1, 7, 2)))"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors-1",
    "href": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors-1",
    "title": "ML In Practice",
    "section": "k-nearest neighbors",
    "text": "k-nearest neighbors\n\nOnce we optimize our algorithm, the predict function defaults to using the best performing algorithm fit with the entire training data:\n\n\ny_hat_knn &lt;- predict(train_knn, x_test, type = \"raw\") \n\n\nWe achieve relatively high accuracy:\n\n\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"] \n\nAccuracy \n   0.952"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors-2",
    "href": "slides/ml/44-ml-in-practice.html#k-nearest-neighbors-2",
    "title": "Ml In Practice",
    "section": "k-nearest neighbors",
    "text": "k-nearest neighbors\n\nOnce we optimize our algorithm, the predict function defaults to using the best performing algorithm fit with the entire training data:\n\n\ny_hat_knn &lt;- predict(train_knn, x_test, type = \"raw\") \n\n\nWe achieve relatively high accuracy:\n\n\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"] \n\nAccuracy \n   0.952"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nAn alternative to removing low variance columns directly is to use dimension reduction on the feature matrix before applying the algorithms.\n\n\npca &lt;- prcomp(x) \n\n\nWe can actually explain, say, 75% of the variability in the predictors with a small number of dimensions:\n\n\np &lt;- which(cumsum(pca$sdev^2)/sum(pca$sdev^2) &gt;= 0.75)[1]"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-1",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-1",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nWe can then re-run our algorithm on these 33 features:\n\n\nfit_knn_pca &lt;- knn3(pca$x[,1:p], y, k = train_knn$bestTune)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-2",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-2",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nWhen predicting, it is important that we not use the test set when finding the PCs nor any summary of the data, as this could result in overtraining.\nWe therefrom compute the averages needed for centering and the rotation on the training set:\n\n\nnewdata &lt;-  sweep(x_test, 2, colMeans(x)) %*% pca$rotation[,1:p] \ny_hat_knn_pca &lt;- predict(fit_knn_pca, newdata, type = \"class\")"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-3",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-3",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nWe obtain similar accuracy, while using only 33 dimensions:\n\n\nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"] \n\nAccuracy \n   0.965"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-4",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-4",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nIn this example, we used the \\(k\\) optimized for the raw data, not the principal components.\nNote that to obtain an unbiased MSE estimate we have to recompute the PCA for each cross-validation sample and apply to the validation set."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-5",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-5",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nBecause the train function includes PCA as one of the available preprocessing operations we can achieve this with this modification of the code above:\n\n\ntrain_knn_pca &lt;- train(x, y, method = \"knn\",  \n                       preProcess = c(\"nzv\", \"pca\"), \n                       trControl = trainControl(\"cv\", \n                                                number = 20, \n                                                p = 0.95, \n                                                preProcOptions = \n                                                  list(pcaComp = p)), \n                       tuneGrid = data.frame(k = seq(1, 7, 2))) \ny_hat_knn_pca &lt;- predict(train_knn_pca, x_test, type = \"raw\") \nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"] \n\nAccuracy \n   0.969"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#random-forest",
    "href": "slides/ml/44-ml-in-practice.html#random-forest",
    "title": "ML In Practice",
    "section": "Random Forest",
    "text": "Random Forest\n\nWith the random forest algorithm several parameters can be optimized, but the main one is mtry, the number of predictors that are randomly selected for each tree."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#random-forest-1",
    "href": "slides/ml/44-ml-in-practice.html#random-forest-1",
    "title": "ML In Practice",
    "section": "Random Forest",
    "text": "Random Forest\n\nThis is also the only tuning parameter that the caret function train permits when using the default implementation from the randomForest package.\n\n\nlibrary(randomForest) \ntrain_rf &lt;- train(x, y, method = \"rf\",  \n                  preProcess = \"nzv\", \n                  tuneGrid = data.frame(mtry = seq(5, 15))) \ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\")"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#random-forest-2",
    "href": "slides/ml/44-ml-in-practice.html#random-forest-2",
    "title": "ML In Practice",
    "section": "Random Forest",
    "text": "Random Forest\n\nNow that we have optimized our algorithm, we are ready to fit our final model:\n\n\ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\") \n\n\nAs with kNN, we also achieve high accuracy:\n\n\nconfusionMatrix(y_hat_rf, y_test)$overall[\"Accuracy\"] \n\nAccuracy \n   0.952 \n\n\n\nBy optimizing some of the other algorithm parameters, we can achieve even higher accuracy."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time",
    "href": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time",
    "title": "Ml In Practice",
    "section": "Testing and improving computation time",
    "text": "Testing and improving computation time\n\nThe default method for estimating accuracy used by the train function is to test prediction on 25 bootstrap samples.\nThis can result in long compute times.\nWe can use the system.time function to estimate how long it takes to run the algorithm once:\n\n\nnzv &lt;- nearZeroVar(x) \nsystem.time({fit_rf &lt;- randomForest(x[, -nzv], y,  mtry = 9)}) \n\n   user  system elapsed \n 60.499   1.033  61.946"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-1",
    "href": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-1",
    "title": "Ml In Practice",
    "section": "Testing and improving computation time",
    "text": "Testing and improving computation time\n\nOne way to reduce run time is to use k-fold cross validation with a smaller number of test sets.\nA popular choice is leaving out 5 test sets with 20% of the data.\nTo use this we set the trControl argument in train to trainControl(method = \"cv\", number = 5, p = .8)."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-2",
    "href": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-2",
    "title": "Ml In Practice",
    "section": "Testing and improving computation time",
    "text": "Testing and improving computation time\n\nFor random forest, we can also speed up the training step by running less trees per fit.\nAfter running the algorithm once, we can use the plot function to see how the error rate changes as the number of trees grows.\nHere we can see that error rate stabilizes after about 200 trees:\n\n\nplot(fit_rf)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-3",
    "href": "slides/ml/44-ml-in-practice.html#testing-and-improving-computation-time-3",
    "title": "Ml In Practice",
    "section": "Testing and improving computation time",
    "text": "Testing and improving computation time\n\nWe can use this finding to speed up the cross validation procedure.\nSpecifically, because the default is 500, by adding the argument ntree = 200 to the call to train\nThe procedure will finish 2.5 times faster."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#variable-importance",
    "href": "slides/ml/44-ml-in-practice.html#variable-importance",
    "title": "ML In Practice",
    "section": "Variable importance",
    "text": "Variable importance\n\nThe following function computes the importance of each feature:\n\n\nimp &lt;- importance(fit_rf)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#variable-importance-1",
    "href": "slides/ml/44-ml-in-practice.html#variable-importance-1",
    "title": "ML In Practice",
    "section": "Variable importance",
    "text": "Variable importance\n\nWe can see which features are being used most by plotting an image:\n\n\nmat &lt;- rep(0, ncol(x)) \nmat[-nzv] &lt;- imp \nimage(matrix(mat, 28, 28))"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#diagnostics",
    "href": "slides/ml/44-ml-in-practice.html#diagnostics",
    "title": "ML In Practice",
    "section": "Diagnostics",
    "text": "Diagnostics"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#ensembles",
    "href": "slides/ml/44-ml-in-practice.html#ensembles",
    "title": "ML In Practice",
    "section": "Ensembles",
    "text": "Ensembles\n\nThe idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.\nIn machine learning, one can usually greatly improve the final results by combining the results of different algorithms.\nHere is a simple example where we compute new class probabilities by taking the average of random forest and kNN."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#ensembles-1",
    "href": "slides/ml/44-ml-in-practice.html#ensembles-1",
    "title": "ML In Practice",
    "section": "Ensembles",
    "text": "Ensembles\n\nWe can see that the accuracy improves:\n\n\np_rf &lt;- predict(fit_rf, x_test[,-nzv], type = \"prob\")   \np_rf &lt;- p_rf / rowSums(p_rf) \np_knn_pca  &lt;- predict(train_knn_pca, x_test, type = \"prob\") \np &lt;- (p_rf + p_knn_pca)/2 \ny_pred &lt;- factor(apply(p, 1, which.max) - 1) \nconfusionMatrix(y_pred, y_test)$overall[\"Accuracy\"] \n\nAccuracy \n    0.97"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#ensembles-2",
    "href": "slides/ml/44-ml-in-practice.html#ensembles-2",
    "title": "ML In Practice",
    "section": "Ensembles",
    "text": "Ensembles\n\nWe have just built an ensemble with just two algorithms.\nBy combing more similarly performing, but uncorrelated, algorithms we can improve accuracy further."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing",
    "href": "slides/ml/44-ml-in-practice.html#testing",
    "title": "ML In Practice",
    "section": "Testing",
    "text": "Testing\n\nThe default method for estimating accuracy used by the train function is to test prediction on 25 bootstrap samples.\nThis can result in long compute times.\nWe can use the system.time function to estimate how long it takes to run the algorithm once:\n\n\nnzv &lt;- nearZeroVar(x) \nsystem.time({fit_rf &lt;- randomForest(x[, -nzv], y,  mtry = 9)}) \n\n   user  system elapsed \n 60.499   1.033  61.946"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-1",
    "href": "slides/ml/44-ml-in-practice.html#testing-1",
    "title": "ML In Practice",
    "section": "Testing",
    "text": "Testing\n\nOne way to reduce run time is to use k-fold cross validation with a smaller number of test sets.\nA popular choice is leaving out 5 test sets with 20% of the data.\nTo use this we set the trControl argument in train to:\n\ntrControl = trainControl(method = \"cv\", number = 5, p = .8)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-2",
    "href": "slides/ml/44-ml-in-practice.html#testing-2",
    "title": "ML In Practice",
    "section": "Testing",
    "text": "Testing\n\nFor random forest, we can also speed up the training step by running less trees per fit.\nAfter running the algorithm once, we can use the plot function to see how the error rate changes as the number of trees grows."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-3",
    "href": "slides/ml/44-ml-in-practice.html#testing-3",
    "title": "ML In Practice",
    "section": "Testing",
    "text": "Testing\n\nWe can see that error rate stabilizes after200 trees:\n\n\nplot(fit_rf)"
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#testing-4",
    "href": "slides/ml/44-ml-in-practice.html#testing-4",
    "title": "ML In Practice",
    "section": "Testing",
    "text": "Testing\n\nWe can use this finding to speed up the cross validation procedure.\nSpecifically, because the default is 500, by adding the argument ntree = 200 to the call to train\nThe procedure will finish 2.5 times faster."
  },
  {
    "objectID": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-6",
    "href": "slides/ml/44-ml-in-practice.html#dimension-reduction-with-pca-6",
    "title": "ML In Practice",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nA limitation of this approach is that we don’t get to optimize the number of PCs used in the analysis.\nTo do this we need to write our own method."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#association-is-not-causation",
    "href": "slides/other-topics/association-not-causation.html#association-is-not-causation",
    "title": "Association Not Causation",
    "section": "Association is not causation",
    "text": "Association is not causation\n\nAssociation is not causation is perhaps the most important lesson one can learn in a statistics class.\nCorrelation is not causation is another way to say this.\nThroughout the statistics part of the book, we have described tools useful for quantifying associations between variables.\nHowever, we must be careful not to over-interpret these associations."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#association-is-not-causation-1",
    "href": "slides/other-topics/association-not-causation.html#association-is-not-causation-1",
    "title": "Association Not Causation",
    "section": "Association is not causation",
    "text": "Association is not causation\n\nThere are many reasons that a variable \\(X\\) can be correlated with a variable \\(Y\\), without having any direct effect on \\(Y\\).\nBelow we examine four common ways that can lead to misinterpreting data."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nThe following comical example underscores the concept that correlation is not causation.\nIt shows a very strong correlation between divorce rates and margarine consumption."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-1",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-1",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-2",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-2",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nDoes this mean that margarine causes divorces?\nOr do divorces cause people to eat more margarine?\nOf course.\nthe answer to both these questions is no.\nThis is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\n\nhttp://tylervigen.com/spurious-correlations."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-3",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-3",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\n\nThe cases presented on the website are all instances of what is generally called data dredging, data fishing, or data snooping.\nIt’s basically a form of what in the US they call cherry picking.\nAn example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-4",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-4",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables.\nWe will save the results of our simulation into a tibble:\n\n\nlibrary(tidyverse) \nN &lt;- 25 \ng &lt;- 1000000 \nsim_data &lt;- tibble(group = rep(1:g, each = N),  \n                   x = rnorm(N*g),  \n                   y = rnorm(N*g)) \n\n\nThe first column denotes group."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-5",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-5",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nWe created groups.\nFor each group, we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns.\nBecause we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look at the max:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-6",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-6",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nres &lt;- sim_data |&gt;  \n  group_by(group) |&gt;  \n  summarize(r = cor(x, y)) |&gt;  \n  arrange(desc(r)) \nres \n\n# A tibble: 1,000,000 × 2\n    group     r\n    &lt;int&gt; &lt;dbl&gt;\n 1 617324 0.799\n 2 592959 0.768\n 3  13734 0.762\n 4 819701 0.761\n 5  20051 0.749\n 6 603558 0.746\n 7 271954 0.745\n 8 867367 0.744\n 9  71242 0.742\n10 837045 0.740\n# ℹ 999,990 more rows\n\n\n\nWe see a maximum correlation of 0.799.\nIf you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\n\n\nsim_data |&gt; filter(group == res$group[which.max(res$r)]) |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() +  \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-7",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-7",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-8",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-8",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nRemember that the correlation summary is a random variable.\nHere is the distribution generated by the Monte Carlo simulation:\n\n\nres |&gt; ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \"black\")"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-9",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-9",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-10",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-10",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nIt’s simply a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\n\n\nlibrary(broom) \nsim_data |&gt;  \n  filter(group == res$group[which.max(res$r)]) |&gt; \n  summarize(tidy(lm(y ~ x))) |&gt;  \n  filter(term == \"x\") \n\n# A tibble: 1 × 5\n  term  estimate std.error statistic    p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 x        0.587    0.0920      6.38 0.00000163"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-11",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-11",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nThis particular form of data dredging is referred to as p-hacking.\nP-hacking is a topic of much discussion because it poses a problem in scientific publications.\nSince publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results.\nIn epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures, and report only the one exposure that resulted in a small p-value."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#spurious-correlation-12",
    "href": "slides/other-topics/association-not-causation.html#spurious-correlation-12",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nFurthermore, they might try fitting several different models to account for confounding and choose the one that yields the smallest p-value.\nIn experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported.\nThis does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking.\nIn advanced statistics courses, you can learn methods to adjust for these multiple comparisons."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers",
    "href": "slides/other-topics/association-not-causation.html#outliers",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nSuppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements.\nHowever, imagine we make a mistake and forget to standardize entry 23.\nWe can simulate such data using:\n\n\nset.seed(1985) \nx &lt;- rnorm(100,100,1) \ny &lt;- rnorm(100,84,1) \nx[-23] &lt;- scale(x[-23]) \ny[-23] &lt;- scale(y[-23]) \n\n\nThe data look like this:\n\n\nqplot(x, y)"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers-1",
    "href": "slides/other-topics/association-not-causation.html#outliers-1",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers-2",
    "href": "slides/other-topics/association-not-causation.html#outliers-2",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nNot surprisingly, the correlation is very high:\n\n\ncor(x,y) \n\n[1] 0.988\n\n\n\nBut this is driven by the one outlier.\nIf we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\n\ncor(x[-23], y[-23]) \n\n[1] -0.0442\n\n\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers.\nIt is called Spearman correlation."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers-3",
    "href": "slides/other-topics/association-not-causation.html#outliers-3",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nThe idea is simple: compute the correlation on the ranks of the values.\nHere is a plot of the ranks plotted against each other:\n\n\nqplot(rank(x), rank(y))"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers-4",
    "href": "slides/other-topics/association-not-causation.html#outliers-4",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#outliers-5",
    "href": "slides/other-topics/association-not-causation.html#outliers-5",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nThe outlier is no longer associated with a very large value, and the correlation decreases significantly:\n\n\ncor(rank(x), rank(y)) \n\n[1] 0.00251\n\n\n\nSpearman correlation can also be calculated like this:\n\n\ncor(x, y, method = \"spearman\") \n\n[1] 0.00251\n\n\n\nThere are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J.\nHuber & Elvezio M.\nRonchetti."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nAnother way association is confused with causation is when the cause and effect are reversed.\nAn example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored.\nIn this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated1.\n\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-1",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-1",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nConsider this quote from the article:\n\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found.\n\n\nRegardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades…"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-2",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-2",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nEven more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can easily construct an example of cause and effect reversal using the father and son height data.\nIf we fit the model:\n\n\\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\]"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-3",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-3",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nto the father and son height data, where \\(X_i\\) is the father height and \\(y_i\\) is the son height, we do get a statistically significant result.\nWe use the galton_heights dataset defined in ?@sec-regression:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-4",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-4",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\ngalton_heights |&gt; summarize(tidy(lm(father ~ son))) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   40.9      4.40        9.29 5.47e-17\n2 son            0.407    0.0636      6.40 1.36e- 9\n\n\n\nThe model fits the data very well."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-5",
    "href": "slides/other-topics/association-not-causation.html#reversing-cause-and-effect-5",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nIf we examine the mathematical formulation of the model above, it could easily be misinterpreted so as to suggest that the son being tall caused the father to be tall.\nHowever, based on our understanding of genetics and biology, we know it’s the other way around.\nThe model is technically correct.\nThe estimates and p-values were obtained correctly as well.\nWhat is wrong here is the interpretation."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#confounders",
    "href": "slides/other-topics/association-not-causation.html#confounders",
    "title": "Association Not Causation",
    "section": "Confounders",
    "text": "Confounders\n\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) cause changes in both \\(X\\) and \\(Y\\).\nEarlier, when studying baseball data, we saw how Home Runs were a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs.\nIn some cases, we can use linear models to account for confounders."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#confounders-1",
    "href": "slides/other-topics/association-not-causation.html#confounders-1",
    "title": "Association Not Causation",
    "section": "Confounders",
    "text": "Confounders\n\nHowever, this is not always the case.\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect.\nHere, we present a widely used example related to college admissions."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\nAdmission data from six U.C.\nBerkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women.\nPJ Bickel, EA Hammel, and JW O’Connell.\nScience (1975).\nWe can load the data and compute a statistical test, which clearly rejects the hypothesis that gender and admission are independent:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-1",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-1",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt;  \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),  \n            not_admitted = sum(applicants) - sum(total_admitted)) |&gt; \n  select(-gender)  \nchisq.test(two_by_two)$p.value \n\n[1] 1.06e-21\n\n\n\nBut closer inspection shows a paradoxical result.\nHere are the percent admissions by major:\n\n\nadmissions |&gt; select(major, gender, admitted) |&gt; \n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt; \n  mutate(women_minus_men = women - men) \n\n# A tibble: 6 × 4\n  major   men women women_minus_men\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 A        62    82              20\n2 B        63    68               5\n3 C        37    34              -3\n4 D        33    35               2\n5 E        28    24              -4\n6 F         6     7               1\n\n\n\nFour out of the six majors favor women."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-2",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-2",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\nMore importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear.\nWhat’s going on?\nThis actually can happen if an uncounted confounder is driving most of the variability."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-3",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-3",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\nSo let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major.\nA gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than for \\(x=0\\).\nHowever, \\(Z\\) is an important confounder to consider.\nClearly, \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\).\nBut is major selectivity \\(Z\\) associated with gender \\(X\\)?"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-4",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-4",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\n\n\nadmissions |&gt;  \n  group_by(major) |&gt;  \n  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants), \n            percent_women_applicants = sum(applicants * (gender==\"women\")) / \n                                             sum(applicants) * 100) |&gt; \n  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) + \n  geom_text()"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-5",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-5",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-6",
    "href": "slides/other-topics/association-not-causation.html#example-uc-berkeley-admissions-6",
    "title": "Association Not Causation",
    "section": "Example: UC Berkeley admissions",
    "text": "Example: UC Berkeley admissions\n\nThere seems to be association.\nThe plot suggests that women were much more likely to apply to the two “hard” majors, indicating a confounding between gender and major’s selectivity.\nCompare, for example, major B and major E.\nMajor E is much harder to enter than major B, and over 60% of applicants for major E were women, while less than 30% of the applicants for major B were women."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#confounding-explained-graphically",
    "href": "slides/other-topics/association-not-causation.html#confounding-explained-graphically",
    "title": "Association Not Causation",
    "section": "Confounding explained graphically",
    "text": "Confounding explained graphically\n\nThe following plot shows the number of applicants that were admitted and those that were not by major and gender:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#confounding-explained-graphically-1",
    "href": "slides/other-topics/association-not-causation.html#confounding-explained-graphically-1",
    "title": "Association Not Causation",
    "section": "Confounding explained graphically",
    "text": "Confounding explained graphically"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#confounding-explained-graphically-2",
    "href": "slides/other-topics/association-not-causation.html#confounding-explained-graphically-2",
    "title": "Association Not Causation",
    "section": "Confounding explained graphically",
    "text": "Confounding explained graphically\n\nIt also breaks down the acceptances by major.\nThis breakdown allows us to see that the majority of accepted men came from two majors, A and B.\nIt also reveals that few women applied to these majors."
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#average-after-stratifying",
    "href": "slides/other-topics/association-not-causation.html#average-after-stratifying",
    "title": "Association Not Causation",
    "section": "Average after stratifying",
    "text": "Average after stratifying\n\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\n\n\nadmissions |&gt;  \n  ggplot(aes(major, admitted, col = gender, size = applicants)) + \n  geom_point()"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#average-after-stratifying-1",
    "href": "slides/other-topics/association-not-causation.html#average-after-stratifying-1",
    "title": "Association Not Causation",
    "section": "Average after stratifying",
    "text": "Average after stratifying"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#average-after-stratifying-2",
    "href": "slides/other-topics/association-not-causation.html#average-after-stratifying-2",
    "title": "Association Not Causation",
    "section": "Average after stratifying",
    "text": "Average after stratifying\n\nNow we see that major by major, there is not much difference.\nThe size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\n\nadmissions |&gt;  group_by(gender) |&gt; summarize(average = mean(admitted)) \n\n# A tibble: 2 × 2\n  gender average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 men       38.2\n2 women     41.7"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#simpsons-paradox",
    "href": "slides/other-topics/association-not-causation.html#simpsons-paradox",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nThe case we have just covered is an example of Simpson’s paradox.\nIt is called a paradox because we see the sign of the correlation flip when comparing the entire publication to specific strata.\nAs an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\), and we observe realizations of these.\nHere is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#simpsons-paradox-1",
    "href": "slides/other-topics/association-not-causation.html#simpsons-paradox-1",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#simpsons-paradox-2",
    "href": "slides/other-topics/association-not-causation.html#simpsons-paradox-2",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated.\nHowever, once we stratify by \\(Z\\) (shown in different colors below), another pattern emerges:"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#simpsons-paradox-3",
    "href": "slides/other-topics/association-not-causation.html#simpsons-paradox-3",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox"
  },
  {
    "objectID": "slides/other-topics/association-not-causation.html#simpsons-paradox-4",
    "href": "slides/other-topics/association-not-causation.html#simpsons-paradox-4",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\).\nIf we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated, as seen in the plot above."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#association-is-not-causation",
    "href": "slides/other-topics/45-association-not-causation.html#association-is-not-causation",
    "title": "Association Not Causation",
    "section": "Association is not causation",
    "text": "Association is not causation\n\nAssociation is not causation is perhaps the most important lesson one can learn in a statistics class.\nCorrelation is not causation is another way to say this.\nThroughout the statistics part of the book, we have described tools useful for quantifying associations between variables.\nHowever, we must be careful not to over-interpret these associations."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#association-is-not-causation-1",
    "href": "slides/other-topics/45-association-not-causation.html#association-is-not-causation-1",
    "title": "Association Not Causation",
    "section": "Association is not causation",
    "text": "Association is not causation\n\nThere are many reasons that a variable \\(X\\) can be correlated with a variable \\(Y\\), without having any direct effect on \\(Y\\).\nToday we describe three: spurious correlation, reverse causation, and confounders."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-1",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-1",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\nMore here: http://tylervigen.com/spurious-correlations\n\nReferred to as data dredging, data fishing, or data snooping.\nIt’s basically a form of what in the US they call cherry picking."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-2",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-2",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nlibrary(tidyverse) \nN &lt;- 25 \ng &lt;- 1000000 \nsim_data &lt;- tibble(group = rep(1:g, each = N),  \n                   x = rnorm(N*g),  \n                   y = rnorm(N*g))"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-3",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-3",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nres &lt;- sim_data |&gt;  \n  group_by(group) |&gt;  \n  summarize(r = cor(x, y)) |&gt;  \n  arrange(desc(r)) \nres \n\n# A tibble: 1,000,000 × 2\n    group     r\n    &lt;int&gt; &lt;dbl&gt;\n 1 508834 0.779\n 2 187260 0.773\n 3 788683 0.768\n 4 919009 0.765\n 5 284126 0.760\n 6 375763 0.760\n 7 767885 0.753\n 8 605587 0.747\n 9 959725 0.745\n10 351369 0.742\n# ℹ 999,990 more rows\n\n\n\nWe see a maximum correlation of 0.779."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-4",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-4",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nsim_data |&gt; filter(group == res$group[which.max(res$r)]) |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() +  \n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-5",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-5",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-6",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-6",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nSample correlation is a random variable:"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-7",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-7",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nIt’s simply a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204, the largest one will be close to 1."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-8",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-8",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nlibrary(broom) \nsim_data |&gt;  \n  filter(group == res$group[which.max(res$r)]) |&gt; \n  summarize(tidy(lm(y ~ x))) |&gt;  \n  filter(term == \"x\") \n\n# A tibble: 1 × 5\n  term  estimate std.error statistic    p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 x        0.622     0.104      5.96 0.00000449"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-9",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-9",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nThis particular form of data dredging is referred to as p-hacking.\nP-hacking is a topic of much discussion because it poses a problem in scientific publications.\nSince publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-10",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-10",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nIn epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures, and report only the one exposure that resulted in a small p-value."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-11",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-11",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nFurthermore, they might try fitting several different models to account for confounding and choose the one that yields the smallest p-value."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-12",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-12",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nIn experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#spurious-correlation-13",
    "href": "slides/other-topics/45-association-not-causation.html#spurious-correlation-13",
    "title": "Association Not Causation",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nThis does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking.\nIn advanced statistics courses, you can learn methods to adjust for these multiple comparisons."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers",
    "href": "slides/other-topics/45-association-not-causation.html#outliers",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nset.seed(1985) \nx &lt;- rnorm(100,100,1) \ny &lt;- rnorm(100,84,1) \nx[-23] &lt;- scale(x[-23]) \ny[-23] &lt;- scale(y[-23])"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-1",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-1",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-2",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-2",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\ncor(x,y) \n\n[1] 0.988\n\n\nThis high correlation is driven by the one outlier."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-3",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-3",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nIf we remove this outlier, the correlation is greatly reduced to almost 0:\n\n\ncor(x[-23], y[-23]) \n\n[1] -0.0442"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-4",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-4",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers.\nIt is called Spearman correlation."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-5",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-5",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#outliers-6",
    "href": "slides/other-topics/45-association-not-causation.html#outliers-6",
    "title": "Association Not Causation",
    "section": "Outliers",
    "text": "Outliers\n\nThe outlier is no longer associated with a very large value, and the correlation decreases significantly:\n\n\ncor(rank(x), rank(y)) \n\n[1] 0.00251\n\n\n\nSpearman correlation can also be calculated like this:\n\n\ncor(x, y, method = \"spearman\") \n\n[1] 0.00251"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect",
    "href": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nAnother way association is confused with causation is when the cause and effect are reversed.\nAn example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored.\nIn this case, the tutoring is not causing the low test scores, but the other way around."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-1",
    "href": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-1",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\nQuote from NY Times:\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades…"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-2",
    "href": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-2",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nA more likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-3",
    "href": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-3",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nIf we fit the model:\n\n\\[\nX_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\n\\]\n\nwhere \\(X_i\\) is the father height and \\(y_i\\) is the son height, we do get a statistically significant result.\n\n\ngalton_heights |&gt; summarize(tidy(lm(father ~ son))) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   40.9      4.40        9.29 5.47e-17\n2 son            0.407    0.0636      6.40 1.36e- 9"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-4",
    "href": "slides/other-topics/45-association-not-causation.html#reversing-cause-and-effect-4",
    "title": "Association Not Causation",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\n\nThe model fits the data very well.\nThe model is technically correct.\nThe estimates and p-values were obtained correctly as well.\nWhat is wrong here is the interpretation."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#confounders",
    "href": "slides/other-topics/45-association-not-causation.html#confounders",
    "title": "Association Not Causation",
    "section": "Confounders",
    "text": "Confounders\n\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) cause changes in both \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#confounders-1",
    "href": "slides/other-topics/45-association-not-causation.html#confounders-1",
    "title": "Association Not Causation",
    "section": "Confounders",
    "text": "Confounders\n\nEarlier, when studying baseball data, we saw how Home Runs were a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs.\nIn some cases, we can use linear models to account for confounders.\nHowever, this is not always the case."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#confounders-2",
    "href": "slides/other-topics/45-association-not-causation.html#confounders-2",
    "title": "Association Not Causation",
    "section": "Confounders",
    "text": "Confounders\n\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect.\nHere, we present a widely used example related to college admissions."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions",
    "href": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions",
    "title": "Association Not Causation",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt;  \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),  \n            not_admitted = sum(applicants) - sum(total_admitted)) \ntwo_by_two |&gt; \n  mutate(percent = total_admitted/(total_admitted + not_admitted)*100)\n\n# A tibble: 2 × 4\n  gender total_admitted not_admitted percent\n  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n1 men              1198         1493    44.5\n2 women             557         1278    30.4\n\n\n\ntwo_by_two &lt;- select(two_by_two, -gender)\nchisq.test(two_by_two)$p.value \n\n[1] 1.06e-21"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-1",
    "href": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-1",
    "title": "Association Not Causation",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions\n\nCloser inspection shows a paradoxical result:\n\n\nadmissions |&gt; select(major, gender, admitted) |&gt; \n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt; \n  mutate(women_minus_men = women - men) \n\n# A tibble: 6 × 4\n  major   men women women_minus_men\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 A        62    82              20\n2 B        63    68               5\n3 C        37    34              -3\n4 D        33    35               2\n5 E        28    24              -4\n6 F         6     7               1"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-2",
    "href": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-2",
    "title": "Association Not Causation",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions\n\nWhat’s going on?\nThis actually can happen if an uncounted confounder is driving most of the variability."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-3",
    "href": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-3",
    "title": "Association Not Causation",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-4",
    "href": "slides/other-topics/45-association-not-causation.html#uc-berkeley-admissions-4",
    "title": "Association Not Causation",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#confounding-explained-graphically",
    "href": "slides/other-topics/45-association-not-causation.html#confounding-explained-graphically",
    "title": "Association Not Causation",
    "section": "Confounding explained graphically",
    "text": "Confounding explained graphically"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#average-after-stratifying",
    "href": "slides/other-topics/45-association-not-causation.html#average-after-stratifying",
    "title": "Association Not Causation",
    "section": "Average after stratifying",
    "text": "Average after stratifying"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#average-after-stratifying-1",
    "href": "slides/other-topics/45-association-not-causation.html#average-after-stratifying-1",
    "title": "Association Not Causation",
    "section": "Average after stratifying",
    "text": "Average after stratifying\n\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\n\nadmissions |&gt;  group_by(gender) |&gt; \n  summarize(average = mean(admitted)) \n\n# A tibble: 2 × 2\n  gender average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 men       38.2\n2 women     41.7"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nThe case we have just covered is an example of Simpson’s paradox.\nIt is called a paradox because we see the sign of the correlation flip when comparing the entire publication to specific strata."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-1",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-1",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nSimulated \\(X\\), \\(Y\\), and \\(Z\\):"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-2",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-2",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated.\nHowever, once we stratify by \\(Z\\) (shown in different colors below), another pattern emerges."
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-3",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-3",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-4",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-4",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#confounding-explained",
    "href": "slides/other-topics/45-association-not-causation.html#confounding-explained",
    "title": "Association Not Causation",
    "section": "Confounding explained",
    "text": "Confounding explained"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-5",
    "href": "slides/other-topics/45-association-not-causation.html#simpsons-paradox-5",
    "title": "Association Not Causation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\).\nIf we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated, as seen in the plot above."
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#what-is-shiny",
    "href": "slides/other-topics/46-shiny.html#what-is-shiny",
    "title": "Introduction to Shiny",
    "section": "What is Shiny?",
    "text": "What is Shiny?\n\nShiny is an R package for building interactive web applications.\nCombines the computational power of R with the interactivity of modern web technologies.\nNo web development experience required!"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#why-use-shiny",
    "href": "slides/other-topics/46-shiny.html#why-use-shiny",
    "title": "Introduction to Shiny",
    "section": "Why Use Shiny?",
    "text": "Why Use Shiny?\n\nCreate interactive dashboards for data visualization.\nShare R analyses with non-programmers.\nIntegrate real-time data updates in your workflows."
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#basic-components",
    "href": "slides/other-topics/46-shiny.html#basic-components",
    "title": "Introduction to Shiny",
    "section": "Basic Components",
    "text": "Basic Components\n\nUI: Defines the layout and appearance of the app.\nServer: Contains the logic and computations of the app.\nApp: Combines the UI and server into a functional app."
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#basic-components-1",
    "href": "slides/other-topics/46-shiny.html#basic-components-1",
    "title": "Introduction to Shiny",
    "section": "Basic Components",
    "text": "Basic Components\nlibrary(shiny)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Hello, Shiny!\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Choose a number:\", 1, 100, 50)\n    ),\n    mainPanel(\n      textOutput(\"result\")\n    )\n  )\n)\n\n# Define Server\nserver &lt;- function(input, output) {\n  output$result &lt;- renderText({\n    paste(\"You selected:\", input$num)\n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#app-overview",
    "href": "slides/other-topics/46-shiny.html#app-overview",
    "title": "Introduction to Shiny",
    "section": "App Overview",
    "text": "App Overview\n\nA scatter plot with customizable inputs.\n\nUI Code\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Plot\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"points\", \"Number of Points:\", 10, 1000, 500),\n      selectInput(\"color\", \"Point Color:\", c(\"Red\", \"Blue\", \"Green\"))\n    ),\n    mainPanel(\n      plotOutput(\"scatterPlot\")\n    )\n  )\n)"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#reactive-programming",
    "href": "slides/other-topics/46-shiny.html#reactive-programming",
    "title": "Introduction to Shiny",
    "section": "Reactive Programming",
    "text": "Reactive Programming\n\nreactive(): Generates a reactive expression.\n\nreactiveVal &lt;- reactive({\n  input$num * 2\n})\n\nobserve(): Executes code in response to changes.\n\nobserve({\n  print(input$slider)\n})\n\nobserveEvent(): Triggers on specific input changes.\n\nobserveEvent(input$button, {\n  showNotification(\"Button clicked!\")\n})"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#examples",
    "href": "slides/other-topics/46-shiny.html#examples",
    "title": "Introduction to Shiny",
    "section": "Examples",
    "text": "Examples\nreactiveVal &lt;- reactive({\n  input$num * 2\n})\nobserve({\n  print(input$slider)\n})\nobserveEvent(input$button, {\n  showNotification(\"Button clicked!\")\n})"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#deployment-options",
    "href": "slides/other-topics/46-shiny.html#deployment-options",
    "title": "Introduction to Shiny",
    "section": "Deployment Options",
    "text": "Deployment Options\n\nShare Shiny apps using:\n\nShiny Server\nRStudio Connect\nshinyapps.io"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#motivation",
    "href": "slides/other-topics/46-shiny.html#motivation",
    "title": "Introduction to Shiny",
    "section": "Motivation",
    "text": "Motivation\nhttps://rconnect.dfci.harvard.edu/covidpr/"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#motivation-1",
    "href": "slides/other-topics/46-shiny.html#motivation-1",
    "title": "Introduction to Shiny",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#motivation-2",
    "href": "slides/other-topics/46-shiny.html#motivation-2",
    "title": "Introduction to Shiny",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/other-topics/46-shiny.html#motivation-3",
    "href": "slides/other-topics/46-shiny.html#motivation-3",
    "title": "Introduction to Shiny",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/other-topics/45-association-not-causation.html",
    "href": "slides/other-topics/45-association-not-causation.html",
    "title": "Association Not Causation",
    "section": "",
    "text": "Association is not causation is perhaps the most important lesson one can learn in a statistics class.\nCorrelation is not causation is another way to say this.\nThroughout the statistics part of the book, we have described tools useful for quantifying associations between variables.\nHowever, we must be careful not to over-interpret these associations."
  }
]