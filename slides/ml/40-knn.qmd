---
title: "k-nearest neighbors (knn)"
keywords: "Machine Learning"
date: "2024-12-10"
format:
  revealjs:
    theme: night
execute:
  echo: true
  fig-align: center
---

```{r setup, include=FALSE}
options(digits = 3)
library(tidyverse)
dslabs::ds_theme_set()
```



## Motivation

## Motivation

- We are interested in estimating the conditional probability function:

$$
p(\mathbf{x}) = \mbox{Pr}(Y = 1 \mid X_1 = x_1, \dots, X_{784} = x_{784}). 
$$ 

## Simpler example

```{r two-or-seven-images-large-x1, echo=FALSE, message=FALSE, warning=FALSE} 
library(tidyverse)
library(dslabs)
if (!exists("mnist")) mnist <- read_mnist("~/Downloads/mnist/") 
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1), which.max(mnist_27$train$x_1))] 
titles <- c("smallest","largest") 
tmp <- lapply(1:2, function(i){ 
    expand.grid(Row = 1:28, Column = 1:28) |>   
      mutate(label = titles[i],   
             value = mnist$train$images[is[i],]) 
}) 
tmp <- Reduce(rbind, tmp) 
p1 <- tmp |> ggplot(aes(Row, Column, fill = value)) +  
  geom_raster(show.legend = FALSE) +  
  scale_y_reverse() + 
  scale_fill_gradient(low = "white", high = "black") + 
  facet_grid(.~label) +  
  geom_vline(xintercept = 14.5) + 
  geom_hline(yintercept = 14.5) + 
  ggtitle("Largest and smallest x_1") 
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2), which.max(mnist_27$train$x_2))] 
titles <- c("smallest","largest") 
tmp <- lapply(1:2, function(i){ 
    expand.grid(Row = 1:28, Column = 1:28) |>   
      mutate(label = titles[i],   
             value = mnist$train$images[is[i],]) 
}) 
tmp <- Reduce(rbind, tmp) 
p2 <- tmp |> ggplot(aes(Row, Column, fill = value)) +  
    geom_raster(show.legend = FALSE) +  
    scale_y_reverse() + 
    scale_fill_gradient(low = "white", high = "black") + 
    facet_grid(.~label) +  
    geom_vline(xintercept = 14.5) + 
    geom_hline(yintercept = 14.5) + 
  ggtitle("Largest and smallest x_2") 
gridExtra::grid.arrange(p1, p2, ncol = 2) 
``` 

## Conditional Probability Function

```{r true-p-better-colors, fig.asp =1, echo=FALSE} 
mnist_27$true_p |>  
  ggplot(aes(x_1, x_2, z = p)) + 
  geom_raster(aes(fill = p), show.legend = FALSE) + 
  scale_fill_gradientn(colors = c("#F8766D", "white", "#00BFC4")) + 
  stat_contour(breaks = c(0.5), color = "black") 
``` 


## Motivation

- We are interested in estimating:


$$
p(\mathbf{x}) = \mbox{Pr}(Y = 1 \mid X_1 = x_1 , X_2 = x_2). 
$$ 

## Training set

```{r two-or-seven-scatter, fig.asp = 0.9} 
mnist_27$train |> ggplot(aes(x_1, x_2, color = y)) + geom_point(alpha=.75) 
``` 

## Test set

```{r two-or-seven-scatter-2, fig.asp = 0.9} 
mnist_27$test |> ggplot(aes(x_1, x_2, color = y)) + geom_point(alpha = .75) 
``` 


## Motivation

- With  kNN we estimate $p(\mathbf{x})$ using smoothing.

- We define the distance between all observations based on the features.

- For any $\mathbf{x}_0$, we estimate $p(\mathbf{x})$ by identifying the $k$ nearest points to $mathbf{x}_0$ and taking an average of the $y$s associated with these points.

- We refer to the set of points used to compute the average as the *neighborhood*.

- This gives us $\widehat{p}(\mathbf{x}_0)$.


## Motivation

- As with bin smoothers, we can control the flexibility of our estimate through the $k$ parameter: larger $k$s result in smoother estimates, while smaller $k$s result in more flexible and wiggly estimates.

- To implement the algorithm, we can use the `knn3` function from the **caret** package.

- Looking at the help file for this package, we see that we can call it in one of two ways.

- We will use the first way in which we specify a *formula* and a data frame.



## Let's try it

```{r, message = FALSE, warning = FALSE} 
library(dslabs) 
library(caret) 
knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 5) 
``` 

## kNN

This gives us $\widehat{p}(\mathbf{x})$:

```{r}
p_hat_knn <- predict(knn_fit, mnist_27$test)[,2]
```



```{r} 
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class") 
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"] 
``` 

- We see that kNN, with the default parameter, already beats regression.

## kNN

- To see why this is the case, we plot $\widehat{p}(\mathbf{x})$ and compare it to the true conditional probability $p(\mathbf{x})$:


```{r knn-fit, echo = FALSE, message = FALSE, warning = FALSE, out.width="100%"} 
## We use this function to plot the estimated conditional probabilities 
plot_cond_prob <- function(p_hat = NULL){ 
  tmp <- mnist_27$true_p 
  if (!is.null(p_hat)) { 
    tmp <- mutate(tmp, p = p_hat) 
  } 
  tmp |> ggplot(aes(x_1, x_2, z = p)) + 
  geom_raster(aes(fill = p), show.legend = FALSE) + 
  scale_fill_gradientn(colors = c("#F8766D", "white", "#00BFC4")) + 
  stat_contour(breaks = c(0.5), color = "black") 
} 

p1 <- plot_cond_prob() + ggtitle("True conditional probability") 
p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) + 
  ggtitle("kNN-5 estimate") 
library(gridExtra) 
grid.arrange(p2, p1, nrow = 1) 
``` 







## Over-training

Compare

```{r} 
y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class") 
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall["Accuracy"] 
```

to

```{r}
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class") 
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"] 
``` 





## Over-training

Compare this:

```{r} 
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1) 
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class") 
confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[["Accuracy"]] 
``` 

to this:

```{r} 
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class") 
confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall["Accuracy"] 
``` 

- We can see the over-fitting problem by plotting  the decision rule boundaries produced by $p(\mathbf{x})$:



## Over-training

```{r knn-1-overfit, echo = FALSE, out.width="100%"} 
tmp <- mnist_27$true_p 
tmp$knn <- predict(knn_fit_1, newdata = mnist_27$true_p)[,2] 
p1 <- tmp |> 
  ggplot() + 
  geom_point(data = mnist_27$train, aes(x_1, x_2, color = y), 
             pch = 21, show.legend = FALSE) + 
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
  stat_contour(aes(x_1, x_2, z = knn), breaks = c(0.5), color = "black") + 
  ggtitle("Train set") 
p2 <- tmp |> 
  ggplot() + 
  geom_point(data = mnist_27$test, aes(x_1, x_2, color = y),  
             pch = 21, show.legend = FALSE) + 
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
  stat_contour(aes(x_1, x_2, z = knn), breaks = c(0.5), color = "black") + 
  ggtitle("Test set") 
grid.arrange(p1, p2, nrow = 1) 
``` 


## Over-smoothing

Let's try a much bigger neighborhood:

```{r} 
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401) 
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class") 
confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall["Accuracy"] 
``` 


## Over-smoothing

```{r mnist-27-glm-est, echo = FALSE, out.width="100%"} 
fit_lm <- lm(y ~ ., data = mutate(mnist_27$train, y=y == "7")) 
p_hat <- predict(fit_lm, newdata = mnist_27$true_p) 
p_hat <- scales::squish(p_hat, c(0, 1)) 
p1 <- plot_cond_prob(p_hat) + 
  ggtitle("Regression") 
p2 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) + 
  ggtitle("kNN-401") 
grid.arrange(p1, p2, nrow = 1) 
``` 



## Parameter tuning

```{r, echo=FALSE,warning = FALSE, message = FALSE} 
ks <- seq(3, 251, 2) 
accuracy <- sapply(ks, function(k){ 
  fit <- knn3(y ~ ., data = mnist_27$train, k = k) 
  y_hat <- predict(fit, mnist_27$train, type = "class") 
  cm_train <- confusionMatrix(y_hat, mnist_27$train$y) 
  train_error <- cm_train$overall[["Accuracy"]] 
  y_hat <- predict(fit, mnist_27$test, type = "class") 
  cm_test <- confusionMatrix(y_hat, mnist_27$test$y) 
  test_error <- cm_test$overall[["Accuracy"]] 
  c(train = train_error, test = test_error) 
}) 
data.frame(k = ks, train = accuracy["train",], test = accuracy["test",]) |> 
  pivot_longer(-k, values_to = "accuracy", names_to = "set") |> 
  mutate(set = factor(set, levels = c("test", "train"))) |> 
  ggplot(aes(k, accuracy, color = set)) +  
  geom_line() + 
  geom_point()  
``` 

