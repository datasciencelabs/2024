{
  "hash": "495d1ab43c42ce98fbab982d918467f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The caret package\"\nkeywords: \"Machine Learning\"\ndate: \"2024-12-09\"\nformat:\n  revealjs:\n    theme: night\nexecute:\n  echo: true\n  fig-align: center\n---\n\n\n\n\n\n\n## The caret package\n\n- There dozens of machine learning algorithms.\n\n- Many of these algorithms are implemented in R.\n\n- However, they are distributed via different packages, developed by different authors, and often use different syntax.\n\n- The __caret__ package tries to consolidate these differences and provide consistency.\n\n## The caret package\n\n- It currently includes over 200 different methods which are summarized in the __caret__ [package manual](https://topepo.github.io/caret/available-models.html).\n\n\n\n\n\n## The caret package\n\n- We use the 2 or 7 example to illustrate.\n\n- Then we apply it to the     larger MNIST dataset.\n\n\n## The `train` function\n\n\n- Functions such as `lm`, `glm`, `qda`, `lda`, `knn3`, `rpart` and `randomForrest` use different syntax, have different argument names, and produce objects of different types.\n\n- The __caret__ `train` function lets us train different algorithms using similar syntax.\n\n## The `train` function\n\n- For example, we can type the following to train three different models:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dslabs)\nlibrary(caret) \ntrain_glm <- train(y ~ ., method = \"glm\", data = mnist_27$train) \ntrain_qda <- train(y ~ ., method = \"qda\", data = mnist_27$train) \ntrain_knn <- train(y ~ ., method = \"knn\", data = mnist_27$train) \n```\n:::\n\n\n\n\n## The `predict` function\n\n- The `predict` function is very useful for machine learning applications.\n\n- Here is an example with regression:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ ., data = mnist_27$train) \np_hat <- predict(fit, newdata = mnist_27$test) \n```\n:::\n\n\n\n## The `predict` function\n\n- In this case, the function is simply computing.\n\n\n$$\n\\widehat{p}(\\mathbf{x}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2 \n$$\n\n- for the `x_1` and `x_2` in the test set `mnist_27$test`.\n\n## The `predict` function\n\n- With these estimates in place, we can make our predictions and compute our accuracy:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat <- factor(ifelse(p_hat > 0.5, 7, 2)) \n```\n:::\n\n\n\n## The `predict` function\n\n-  `predict` does not always return objects of the same type\n\n- it depends on what type of object it is applied to.\n\n- To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used.\n\n\n\n## The `predict` function\n\n- `predict` is actually a special type of function in R called a _generic function_.\n\n- Generic functions call other functions depending on what kind of object it receives.\n\n- So if `predict` receives an object coming out of the `lm` function, it will call `predict.lm`.\n\n- If it receives an object coming out of `glm`, it calls `predict.glm`.\n\n- If from `knn3`, it calls `predict.knn3`, and so on.\n\n\n\n## The `predict` function\n\n- These functions are similar but not exactly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?predict.glm \n?predict.qda \n?predict.knn3 \n```\n:::\n\n\n\n- There are many other versions of `predict` and many machine learning algorithms define their own `predict` function.\n\n## The `predict` function\n\n- As with `train`, **caret** unifies the use of `predict` with the function `predict.train`.\n\n\n- This function takes the output of `train` and produces prediction of categories or estimates of $p(\\mathbf{x})$.\n\n## The `predict` function\n\n- The code looks the same for all methods:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat_glm <- predict(train_glm, mnist_27$test, type = \"raw\") \ny_hat_qda <- predict(train_qda, mnist_27$test, type = \"raw\") \ny_hat_knn <- predict(train_knn, mnist_27$test, type = \"raw\") \n```\n:::\n\n\n\n- This permits us to quickly compare the algorithms.\n\n## The `predict` function\n\n- For example, we can compare the accuracy like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- list(glm = y_hat_glm, qda = y_hat_qda, knn = y_hat_knn) \nsapply(fits, function(fit){\n  confusionMatrix(fit, mnist_27$test$y)$overall[[\"Accuracy\"]]\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  glm   qda   knn \n0.775 0.815 0.835 \n```\n\n\n:::\n:::\n\n\n\n\n## Resampling\n\n- When an algorithm includes a tuning parameter, `train` automatically uses a resampling method to estimate MSE and decide among a few default candidate values.\n\n- To find out what parameter or parameters are optimized, you can read the **caret** [manual](http://topepo.github.io/caret/available-models.html]).\n\n\n\n## The `predict` function\n\n- Or study the output of:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelLookup(\"knn\") \n```\n:::\n\n\n\n- To obtain all the details of how **caret** implements kNN you can use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetModelInfo(\"knn\") \n```\n:::\n\n\n\n## Resampling\n\n- If we run it with default values:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_knn <- train(y ~ ., method = \"knn\", data = mnist_27$train) \n```\n:::\n\n\n\n- you can quickly see the results of the cross validation using the `ggplot` function.\n\n## Resampling\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_knn, highlight = TRUE) \n```\n\n::: {.cell-output-display}\n![](42-caret_files/figure-revealjs/caret-highlight-run-1.png){width=960}\n:::\n:::\n\n\n\nThe argument `highlight` highlights the max.\n\n## Resampling\n\n- By default, the resampling is performed by taking 25 bootstrap samples, each comprised of 25% of the observations.\n\n- We change this using the `trControl` argument. More on this later.\n\n- For the `kNN` method, the default is to try $k=5,7,9$.\n\n- We change this using the `tuneGrid` argument.\n\n\n\n\n## Resampling\n\n- Let's try `seq(5, 101, 2)`.\n\n- Since we are fitting $49 \\times 25 = 1225$ kNN models, running this code will take several seconds.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2003)\ntrain_knn <- train(y ~ ., method = \"knn\",  \n                   data = mnist_27$train, \n                   tuneGrid = data.frame(k = seq(5, 101, 2))) \n```\n:::\n\n\n\n\n## Resampling\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_knn, highlight = TRUE) \n```\n\n::: {.cell-output-display}\n![](42-caret_files/figure-revealjs/train-knn-plot-run-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n---\n\n:::{.callout-note}\n\n- Because resampling methods are random procedures, the same code can result in different results.\n\n- To assure reproducible results you should set the seed, as we did at the start of this chapter.\n\n:::\n\n\n\n## Resampling\n\n- To access the parameter that maximized the accuracy, you can use this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_knn$bestTune \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    k\n26 55\n```\n\n\n:::\n:::\n\n\n\n- and the best performing model like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_knn$finalModel \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n55-nearest neighbor model\nTraining set outcome distribution:\n\n  2   7 \n401 399 \n```\n\n\n:::\n:::\n\n\n\n## Resampling\n\n- The function `predict` will use this best performing model.\n\n- Here is the accuracy of the best model when applied to the test set, which we have not yet used because the cross validation was done on the training set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"), \n                mnist_27$test$y)$overall[\"Accuracy\"] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAccuracy \n   0.825 \n```\n\n\n:::\n:::\n\n\n\n## Resampling\n\n- Bootstrapping is not always the best approach to resampling.\n\n- If we want to change our resampling method, we can use the `trainControl` function.\n\n- For example, the code below runs 10-fold cross validation.\n\n\n\n\n\n## Resampling\n\n- We accomplish this using the following code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol <- trainControl(method = \"cv\", number = 10, p = .9) \ntrain_knn_cv <- train(y ~ ., method = \"knn\",  \n                   data = mnist_27$train, \n                   tuneGrid = data.frame(k = seq(1, 71, 2)), \n                   trControl = control) \n```\n:::\n\n\n\n\n---\n\n:::{.callout-note}\n- The `results` component of the `train` output includes several summary statistics related to the variability of the cross validation estimates:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(train_knn$results) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"k\"          \"Accuracy\"   \"Kappa\"      \"AccuracySD\" \"KappaSD\"   \n```\n\n\n:::\n:::\n\n\n\n- You can learn many more details about the **caret** package, from the [manual](https://topepo.github.io/caret/available-models.html).\n\n:::\n\n## Preprocessing\n\n\n- Now let's move on to the MNIST digits.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dslabs) \nmnist <- read_mnist() \n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Preprocessing\n\n- The dataset includes two components:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(mnist) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"train\" \"test\" \n```\n\n\n:::\n:::\n\n\n\n- Each of these components includes a matrix with features in the columns:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(mnist$train$images) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60000   784\n```\n\n\n:::\n:::\n\n\n\n- and vector with the classes as integers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(mnist$train$labels) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"integer\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(mnist$train$labels) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   0    1    2    3    4    5    6    7    8    9 \n5923 6742 5958 6131 5842 5421 5918 6265 5851 5949 \n```\n\n\n:::\n:::\n\n\n\n## Preprocessing\n\n- Because we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset.\n\n\n- We will sample 10,000 random rows from the training set and 1,000 random rows from the test set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1990) \nindex <- sample(nrow(mnist$train$images), 10000) \nx <- mnist$train$images[index,] \ny <- factor(mnist$train$labels[index]) \nindex <- sample(nrow(mnist$test$images), 1000) \nx_test <- mnist$test$images[index,] \ny_test <- factor(mnist$test$labels[index]) \n```\n:::\n\n\n\n## Preprocessing\n\n- When fitting models to large datasets, we recommend using matrices instead of data frames, as matrix operations tend to be faster.\n\n- If the matrices lack column names, you can assign names based on their position:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(x) <- 1:ncol(mnist$train$images) \ncolnames(x_test) <- colnames(x) \n```\n:::\n\n\n\n## Preprocessing\n\n- We often transform predictors before running the machine algorithm.\n\n- We also remove predictors that are clearly not useful.\n\n- We call these steps *preprocessing*.\n\n- Examples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation.\n\n\n\n\n\n## Preprocessing\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](42-caret_files/figure-revealjs/pixel-sds-run-1.png){width=960}\n:::\n:::\n\n\n\n\n## Preprocessing\n\n\n- The **caret** packages includes a function that recommends features to be removed due to *near zero variance*:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnzv <- nearZeroVar(x) \n```\n:::\n\n\n\n## Preprocessing\n\n- We can see the columns recommended for removal are the near the edges:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage(matrix(1:784 %in% nzv, 28, 28)) \n```\n\n::: {.cell-output-display}\n![](42-caret_files/figure-revealjs/near-zero-image-1.png){width=50%}\n:::\n:::\n\n\n\n## Preprocessing\n\n- So we end up removing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(nzv) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 532\n```\n\n\n:::\n:::\n\n\n\n- predictors.\n\n## Preprocessing\n\n- The **caret** package features the `preProcess` function, which allows users to establish a predefined set of preprocessing operations based on a training set.\n\n- This function is designed to apply these operations to new datasets without recalculating anything on the test set, ensuring that all preprocessing steps are consistent and derived solely from the training data.\n\n\n\n## Preprocessing\n\n- Below is an example demonstrating how to remove predictors with near-zero variance and then center the remaining predictors:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp <- preProcess(x, method = c(\"nzv\", \"center\")) \ncentered_subsetted_x_test <- predict(pp, newdata = x_test) \ndim(centered_subsetted_x_test) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1000  252\n```\n\n\n:::\n:::\n\n\n\n- Additionally, the `train` function in caret includes a `preProcess` argument that allows users to specify which preprocessing steps to apply automatically during model training.\n\n## kNN \n\n- The first step is to optimize for $k$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_knn <- train(x, y, method = \"knn\",  \n                   preProcess = \"nzv\", \n                   trControl = trainControl(\"cv\", number = 20, p = 0.95), \n                   tuneGrid = data.frame(k = seq(1, 7, 2))) \n```\n:::\n\n\n\n## kNN \n\n- Once we optimize our algorithm, the `predict` function defaults to using the best performing algorithm fit with the entire training data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat_knn <- predict(train_knn, x_test, type = \"raw\") \n```\n:::\n\n\n\n## kNN \n\n- We achieve relatively high accuracy:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAccuracy \n   0.952 \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}